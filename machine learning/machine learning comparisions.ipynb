{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b4407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class ModelComparison:\n",
    "    \"\"\"\n",
    "    Compare traditional statistical models with machine learning approaches\n",
    "    for length of stay prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df_clean_final_nb, X_log, y_log, groups, random_state=42):\n",
    "        self.df_nb = df_clean_final_nb.copy()\n",
    "        self.X_mixed = X_log.copy()\n",
    "        self.y_mixed = y_log.copy()\n",
    "        self.groups = groups\n",
    "        self.random_state = random_state\n",
    "        self.results = {}\n",
    "        \n",
    "    def prepare_ml_data(self):\n",
    "        \"\"\"Prepare data for machine learning models\"\"\"\n",
    "        # First, let's check what columns are actually available\n",
    "        print(\"Available columns in df_nb:\", self.df_nb.columns.tolist())\n",
    "        \n",
    "        # For ML models, we'll use the original scale data with all features\n",
    "        feature_cols = [\n",
    "            'immigrant_population', 'import_from_slu', 'age', 'distance_miles',\n",
    "            'state_unemployment', 'sex_enc', 'marital_status_enc', \n",
    "            'employment_status_enc', 'purpose_simple', 'accomd_type_enc',\n",
    "            'season_enc', 'us_state_enc',\n",
    "            # Include engineered features if available\n",
    "            'age_orth1', 'log_distance_orth1', 'age_X_purpose2', \n",
    "            'distance_X_purpose2'\n",
    "        ]\n",
    "        \n",
    "        # Filter to available columns\n",
    "        available_features = [col for col in feature_cols if col in self.df_nb.columns]\n",
    "        print(f\"Using {len(available_features)} features for ML models\")\n",
    "        \n",
    "        self.X_ml = self.df_nb[available_features].copy()\n",
    "        self.y_ml = self.df_nb['los_capped'].copy()\n",
    "        \n",
    "        # Create train-test split\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X_ml, self.y_ml, test_size=0.2, random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Also split the data for traditional models\n",
    "        train_idx = self.X_train.index\n",
    "        test_idx = self.X_test.index\n",
    "        \n",
    "        # Ensure indices are aligned\n",
    "        self.X_mixed_train = self.X_mixed.loc[train_idx].copy()\n",
    "        self.X_mixed_test = self.X_mixed.loc[test_idx].copy()\n",
    "        self.y_mixed_train = self.y_mixed.loc[train_idx].copy()\n",
    "        self.y_mixed_test = self.y_mixed.loc[test_idx].copy()\n",
    "        self.groups_train = self.groups.loc[train_idx].copy()\n",
    "        self.groups_test = self.groups.loc[test_idx].copy()\n",
    "        \n",
    "        self.df_nb_train = self.df_nb.loc[train_idx].copy()\n",
    "        self.df_nb_test = self.df_nb.loc[test_idx].copy()\n",
    "        \n",
    "        # Reset indices to avoid issues\n",
    "        self.X_mixed_train.reset_index(drop=True, inplace=True)\n",
    "        self.X_mixed_test.reset_index(drop=True, inplace=True)\n",
    "        self.y_mixed_train.reset_index(drop=True, inplace=True)\n",
    "        self.y_mixed_test.reset_index(drop=True, inplace=True)\n",
    "        self.groups_train.reset_index(drop=True, inplace=True)\n",
    "        self.groups_test.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        print(f\"Train set size: {len(self.X_train)}, Test set size: {len(self.X_test)}\")\n",
    "        \n",
    "    def fit_negative_binomial(self):\n",
    "        \"\"\"Fit negative binomial model\"\"\"\n",
    "        print(\"Fitting Negative Binomial Model...\")\n",
    "        \n",
    "        # Check if required columns exist\n",
    "        required_cols = ['log_los_capped', 'log_distance', 'log_immigrant_population', \n",
    "                        'log_import_from_slu', 'age_orth1', 'age_X_purpose2']\n",
    "        missing_cols = [col for col in required_cols if col not in self.df_nb_train.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"Warning: Missing columns for NB model: {missing_cols}\")\n",
    "            # Use a simpler formula if some columns are missing\n",
    "            formula_final_nb = \"log_los_capped ~ age + distance_miles + C(sex_enc) + C(purpose_simple) + C(season_enc)\"\n",
    "        else:\n",
    "            formula_final_nb = (\n",
    "                \"log_los_capped ~ log_distance + log_immigrant_population + \"\n",
    "                \"log_import_from_slu + age_orth1 + age_X_purpose2 + \"\n",
    "                \"C(sex_enc)+C(marital_status_enc)+C(purpose_simple)+\"\n",
    "                \"C(accomd_type_enc)+C(season_enc)+C(us_state_enc)\"\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            nb_model = smf.glm(\n",
    "                formula=formula_final_nb,\n",
    "                data=self.df_nb_train,\n",
    "                family=sm.families.NegativeBinomial(link=sm.families.links.log())\n",
    "            )\n",
    "            \n",
    "            self.nb_results = nb_model.fit()\n",
    "            \n",
    "            # Predictions on test set\n",
    "            nb_pred_log = self.nb_results.predict(self.df_nb_test)\n",
    "            # Convert back from log scale\n",
    "            nb_pred = np.exp(nb_pred_log)\n",
    "            \n",
    "            # Ensure predictions are reasonable\n",
    "            nb_pred = np.clip(nb_pred, 1, 365)  # Clip to reasonable range\n",
    "            \n",
    "            return nb_pred\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting NB model: {e}\")\n",
    "            # Return baseline predictions\n",
    "            return np.full(len(self.y_test), self.y_train.mean())\n",
    "    \n",
    "    def fit_mixed_effects(self):\n",
    "        \"\"\"Fit mixed effects model\"\"\"\n",
    "        print(\"Fitting Mixed Effects Model...\")\n",
    "        \n",
    "        try:\n",
    "            # Ensure data types are correct\n",
    "            self.y_mixed_train = self.y_mixed_train.astype(float)\n",
    "            self.X_mixed_train = self.X_mixed_train.astype(float)\n",
    "            \n",
    "            mixed_model = MixedLM(\n",
    "                self.y_mixed_train, \n",
    "                self.X_mixed_train, \n",
    "                groups=self.groups_train\n",
    "            )\n",
    "            \n",
    "            self.mixed_results = mixed_model.fit()\n",
    "            \n",
    "            # Predictions on test set\n",
    "            mixed_pred_log = self.mixed_results.predict(self.X_mixed_test)\n",
    "            # Convert back from log scale\n",
    "            mixed_pred = np.exp(mixed_pred_log)\n",
    "            \n",
    "            # Ensure predictions are reasonable\n",
    "            mixed_pred = np.clip(mixed_pred, 1, 365)\n",
    "            \n",
    "            return mixed_pred\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting Mixed Effects model: {e}\")\n",
    "            # Return baseline predictions\n",
    "            return np.full(len(self.y_test), self.y_train.mean())\n",
    "    \n",
    "    def fit_random_forest(self):\n",
    "        \"\"\"Fit Random Forest model\"\"\"\n",
    "        print(\"Fitting Random Forest Model...\")\n",
    "        \n",
    "        # Handle any missing values\n",
    "        X_train_clean = self.X_train.fillna(self.X_train.mean())\n",
    "        X_test_clean = self.X_test.fillna(self.X_train.mean())\n",
    "        \n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rf_model.fit(X_train_clean, self.y_train)\n",
    "        rf_pred = rf_model.predict(X_test_clean)\n",
    "        \n",
    "        # Store feature importance\n",
    "        self.rf_importance = pd.DataFrame({\n",
    "            'feature': self.X_ml.columns,\n",
    "            'importance': rf_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return rf_pred, rf_model\n",
    "    \n",
    "    def fit_gradient_boosting(self):\n",
    "        \"\"\"Fit Gradient Boosting model (sklearn version, no OpenMP required)\"\"\"\n",
    "        print(\"Fitting Gradient Boosting Model...\")\n",
    "        \n",
    "        # Handle any missing values\n",
    "        X_train_clean = self.X_train.fillna(self.X_train.mean())\n",
    "        X_test_clean = self.X_test.fillna(self.X_train.mean())\n",
    "        \n",
    "        gb_model = GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        gb_model.fit(X_train_clean, self.y_train)\n",
    "        gb_pred = gb_model.predict(X_test_clean)\n",
    "        \n",
    "        # Store feature importance\n",
    "        self.gb_importance = pd.DataFrame({\n",
    "            'feature': self.X_ml.columns,\n",
    "            'importance': gb_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return gb_pred, gb_model\n",
    "    \n",
    "    def calculate_metrics(self, y_true, y_pred, model_name):\n",
    "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "        \n",
    "        # Convert to numpy arrays and flatten\n",
    "        y_true = np.array(y_true).flatten()\n",
    "        y_pred = np.array(y_pred).flatten()\n",
    "        \n",
    "        # Remove any NaN or infinite values\n",
    "        mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "        y_true_clean = y_true[mask]\n",
    "        y_pred_clean = y_pred[mask]\n",
    "        \n",
    "        # Avoid division by zero in MAPE\n",
    "        mape_mask = y_true_clean > 0\n",
    "        if np.sum(mape_mask) > 0:\n",
    "            mape = np.mean(np.abs((y_true_clean[mape_mask] - y_pred_clean[mape_mask]) / y_true_clean[mape_mask])) * 100\n",
    "        else:\n",
    "            mape = np.nan\n",
    "        \n",
    "        metrics = {\n",
    "            'Model': model_name,\n",
    "            'MAE': mean_absolute_error(y_true_clean, y_pred_clean),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_true_clean, y_pred_clean)),\n",
    "            'R²': r2_score(y_true_clean, y_pred_clean),\n",
    "            'MAPE': mape,\n",
    "            'Median AE': np.median(np.abs(y_true_clean - y_pred_clean)),\n",
    "            'Max Error': np.max(np.abs(y_true_clean - y_pred_clean)),\n",
    "            '90th Percentile Error': np.percentile(np.abs(y_true_clean - y_pred_clean), 90)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def run_comparison(self):\n",
    "        \"\"\"Run all models and compare results\"\"\"\n",
    "        \n",
    "        # Prepare data\n",
    "        self.prepare_ml_data()\n",
    "        \n",
    "        results_list = []\n",
    "        predictions = {}\n",
    "        \n",
    "        # 1. Negative Binomial\n",
    "        try:\n",
    "            nb_pred = self.fit_negative_binomial()\n",
    "            predictions['Negative Binomial'] = nb_pred\n",
    "            metrics = self.calculate_metrics(self.y_test, nb_pred, 'Negative Binomial')\n",
    "            results_list.append(metrics)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Negative Binomial: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # 2. Mixed Effects\n",
    "        try:\n",
    "            mixed_pred = self.fit_mixed_effects()\n",
    "            predictions['Mixed Effects'] = mixed_pred\n",
    "            metrics = self.calculate_metrics(self.y_test, mixed_pred, 'Mixed Effects')\n",
    "            results_list.append(metrics)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Mixed Effects: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # 3. Random Forest\n",
    "        try:\n",
    "            rf_pred, rf_model = self.fit_random_forest()\n",
    "            predictions['Random Forest'] = rf_pred\n",
    "            metrics = self.calculate_metrics(self.y_test, rf_pred, 'Random Forest')\n",
    "            results_list.append(metrics)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Random Forest: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # 4. Gradient Boosting (instead of XGBoost)\n",
    "        try:\n",
    "            gb_pred, gb_model = self.fit_gradient_boosting()\n",
    "            predictions['Gradient Boosting'] = gb_pred\n",
    "            metrics = self.calculate_metrics(self.y_test, gb_pred, 'Gradient Boosting')\n",
    "            results_list.append(metrics)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Gradient Boosting: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # Create results dataframe\n",
    "        if results_list:\n",
    "            self.results_df = pd.DataFrame(results_list)\n",
    "            self.predictions = predictions\n",
    "        else:\n",
    "            print(\"No models were successfully fitted!\")\n",
    "            self.results_df = pd.DataFrame()\n",
    "            self.predictions = {}\n",
    "        \n",
    "        return self.results_df, predictions\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Create visualization of model comparison\"\"\"\n",
    "        if self.results_df.empty:\n",
    "            print(\"No results to plot!\")\n",
    "            return None\n",
    "            \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Model Performance Metrics\n",
    "        ax1 = axes[0, 0]\n",
    "        metrics_to_plot = ['MAE', 'RMSE', 'Median AE']\n",
    "        if not self.results_df.empty:\n",
    "            self.results_df[['Model'] + metrics_to_plot].set_index('Model').plot(\n",
    "                kind='bar', ax=ax1\n",
    "            )\n",
    "            ax1.set_title('Error Metrics Comparison')\n",
    "            ax1.set_ylabel('Days')\n",
    "            ax1.legend(loc='upper right')\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 2. R² Comparison\n",
    "        ax2 = axes[0, 1]\n",
    "        if 'R²' in self.results_df.columns:\n",
    "            self.results_df.set_index('Model')['R²'].plot(kind='bar', ax=ax2, color='green')\n",
    "            ax2.set_title('R² Score Comparison')\n",
    "            ax2.set_ylabel('R² Score')\n",
    "            ax2.set_ylim(0, 1)\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 3. Actual vs Predicted for best model\n",
    "        if not self.results_df.empty and self.predictions:\n",
    "            best_model = self.results_df.loc[self.results_df['R²'].idxmax(), 'Model']\n",
    "            ax3 = axes[1, 0]\n",
    "            y_pred_best = self.predictions[best_model]\n",
    "            ax3.scatter(self.y_test, y_pred_best, alpha=0.5)\n",
    "            ax3.plot([self.y_test.min(), self.y_test.max()], \n",
    "                     [self.y_test.min(), self.y_test.max()], 'r--', lw=2)\n",
    "            ax3.set_xlabel('Actual Length of Stay')\n",
    "            ax3.set_ylabel('Predicted Length of Stay')\n",
    "            ax3.set_title(f'Actual vs Predicted - {best_model}')\n",
    "        \n",
    "        # 4. Feature Importance (for ML models)\n",
    "        ax4 = axes[1, 1]\n",
    "        if hasattr(self, 'rf_importance') and not self.rf_importance.empty:\n",
    "            top_features = self.rf_importance.head(10)\n",
    "            ax4.barh(range(len(top_features)), top_features['importance'])\n",
    "            ax4.set_yticks(range(len(top_features)))\n",
    "            ax4.set_yticklabels(top_features['feature'])\n",
    "            ax4.set_xlabel('Importance')\n",
    "            ax4.set_title('Top 10 Features - Random Forest')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def cross_validate_models(self, cv=5):\n",
    "        \"\"\"Perform cross-validation for more robust comparison\"\"\"\n",
    "        print(\"\\nPerforming Cross-Validation...\")\n",
    "        \n",
    "        cv_results = {}\n",
    "        \n",
    "        # Handle missing values for CV\n",
    "        X_ml_clean = self.X_ml.fillna(self.X_ml.mean())\n",
    "        \n",
    "        # Random Forest CV\n",
    "        try:\n",
    "            rf_model = RandomForestRegressor(\n",
    "                n_estimators=100, max_depth=20, random_state=self.random_state, n_jobs=-1\n",
    "            )\n",
    "            rf_scores = cross_val_score(rf_model, X_ml_clean, self.y_ml, \n",
    "                                       cv=cv, scoring='neg_mean_squared_error')\n",
    "            cv_results['Random Forest'] = np.sqrt(-rf_scores)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in RF CV: {e}\")\n",
    "        \n",
    "        # Gradient Boosting CV\n",
    "        try:\n",
    "            gb_model = GradientBoostingRegressor(\n",
    "                n_estimators=100, max_depth=6, random_state=self.random_state\n",
    "            )\n",
    "            gb_scores = cross_val_score(gb_model, X_ml_clean, self.y_ml, \n",
    "                                        cv=cv, scoring='neg_mean_squared_error')\n",
    "            cv_results['Gradient Boosting'] = np.sqrt(-gb_scores)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in GB CV: {e}\")\n",
    "        \n",
    "        # Create CV results summary\n",
    "        if cv_results:\n",
    "            cv_summary = pd.DataFrame({\n",
    "                'Model': list(cv_results.keys()),\n",
    "                'CV RMSE Mean': [np.mean(scores) for scores in cv_results.values()],\n",
    "                'CV RMSE Std': [np.std(scores) for scores in cv_results.values()]\n",
    "            })\n",
    "        else:\n",
    "            cv_summary = pd.DataFrame()\n",
    "        \n",
    "        return cv_summary\n",
    "\n",
    "# Helper function to diagnose data issues\n",
    "def diagnose_data(df_clean_final_nb, X_log, y_log, groups):\n",
    "    \"\"\"Diagnose potential data issues before running comparison\"\"\"\n",
    "    print(\"=== Data Diagnosis ===\")\n",
    "    print(f\"df_clean_final_nb shape: {df_clean_final_nb.shape}\")\n",
    "    print(f\"X_log shape: {X_log.shape}\")\n",
    "    print(f\"y_log shape: {y_log.shape}\")\n",
    "    print(f\"groups shape: {groups.shape}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\nMissing values in df_clean_final_nb: {df_clean_final_nb.isnull().sum().sum()}\")\n",
    "    print(f\"Missing values in X_log: {X_log.isnull().sum().sum()}\")\n",
    "    print(f\"Missing values in y_log: {y_log.isnull().sum()}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"\\ny_log dtype: {y_log.dtype}\")\n",
    "    print(f\"groups dtype: {groups.dtype}\")\n",
    "    \n",
    "    # Check if indices match\n",
    "    print(f\"\\nIndices match: {all(df_clean_final_nb.index == X_log.index)}\")\n",
    "    \n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906573a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First diagnose the data\n",
    "diagnose_data(df_clean_final_nb, X_log, y_log, groups)\n",
    "\n",
    "# Initialize comparison\n",
    "comparison = ModelComparison(df_clean_final_nb, X_log, y_log, groups)\n",
    "\n",
    "# Run comparison with error handling\n",
    "try:\n",
    "    results_df, predictions = comparison.run_comparison()\n",
    "    \n",
    "    # Display results if available\n",
    "    if not results_df.empty:\n",
    "        print(\"\\nModel Performance Summary:\")\n",
    "        print(results_df.round(3))\n",
    "        \n",
    "        # Cross-validation results\n",
    "        cv_results = comparison.cross_validate_models()\n",
    "        if not cv_results.empty:\n",
    "            print(\"\\nCross-Validation Results:\")\n",
    "            print(cv_results.round(3))\n",
    "        \n",
    "        # Plot results\n",
    "        fig = comparison.plot_results()\n",
    "        if fig:\n",
    "            plt.show()\n",
    "        \n",
    "        # Get feature importance for best ML model\n",
    "        if hasattr(comparison, 'rf_importance'):\n",
    "            print(\"\\nTop 10 Important Features (Random Forest):\")\n",
    "            print(comparison.rf_importance.head(10))\n",
    "    else:\n",
    "        print(\"No results were generated. Please check the error messages above.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Fatal error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9fd38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First diagnose the data\n",
    "diagnose_data(df_clean_final_nb, X_log, y_log, groups)\n",
    "\n",
    "# Initialize comparison\n",
    "comparison = ModelComparison(df_clean_final_nb, X_log, y_log, groups)\n",
    "\n",
    "# Run comparison with error handling\n",
    "try:\n",
    "    results_df, predictions = comparison.run_comparison()\n",
    "    \n",
    "    # Display results if available\n",
    "    if not results_df.empty:\n",
    "        print(\"\\nModel Performance Summary:\")\n",
    "        print(results_df.round(3))\n",
    "        \n",
    "        # Cross-validation results\n",
    "        cv_results = comparison.cross_validate_models()\n",
    "        if not cv_results.empty:\n",
    "            print(\"\\nCross-Validation Results:\")\n",
    "            print(cv_results.round(3))\n",
    "        \n",
    "        # Plot results\n",
    "        fig = comparison.plot_results()\n",
    "        if fig:\n",
    "            plt.show()\n",
    "        \n",
    "        # Get feature importance for best ML model\n",
    "        if hasattr(comparison, 'rf_importance'):\n",
    "            print(\"\\nTop 10 Important Features (Random Forest):\")\n",
    "            print(comparison.rf_importance.head(10))\n",
    "    else:\n",
    "        print(\"No results were generated. Please check the error messages above.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Fatal error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View top features\n",
    "print(\"\\nTop 10 Important Features (Random Forest):\")\n",
    "print(comparison.rf_importance.head(10))\n",
    "\n",
    "print(\"\\nTop 10 Important Features (Gradient Boosting):\")\n",
    "print(comparison.gb_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d499f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches, Pt\n",
    "from docx.enum.style import WD_STYLE_TYPE\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "def export_model_comparison_to_word(comparison, results_df, cv_results, filename=\"model_comparison_report.docx\"):\n",
    "    \"\"\"\n",
    "    Export all model comparison results to a comprehensive Word document\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    comparison : ModelComparison object\n",
    "        The fitted ModelComparison object containing all results\n",
    "    results_df : DataFrame\n",
    "        The main results dataframe from run_comparison()\n",
    "    cv_results : DataFrame\n",
    "        Cross-validation results\n",
    "    filename : str\n",
    "        Output filename for the Word document\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a new Document\n",
    "    doc = Document()\n",
    "    \n",
    "    # Add title\n",
    "    title = doc.add_heading('Length of Stay Model Comparison Report', 0)\n",
    "    title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    \n",
    "    # Add date\n",
    "    doc.add_paragraph(f'Generated on: {datetime.now().strftime(\"%B %d, %Y\")}')\n",
    "    doc.add_paragraph()\n",
    "    \n",
    "    # Executive Summary\n",
    "    doc.add_heading('Executive Summary', level=1)\n",
    "    \n",
    "    # Find best model\n",
    "    best_model = results_df.loc[results_df['R²'].idxmax(), 'Model']\n",
    "    best_r2 = results_df.loc[results_df['R²'].idxmax(), 'R²']\n",
    "    best_rmse = results_df.loc[results_df['R²'].idxmax(), 'RMSE']\n",
    "    \n",
    "    summary_text = (\n",
    "        f\"This report compares four different approaches for predicting length of stay: \"\n",
    "        f\"Negative Binomial, Mixed Effects, Random Forest, and Gradient Boosting models. \"\n",
    "        f\"\\n\\nThe best performing model is {best_model} with an R² of {best_r2:.3f} and \"\n",
    "        f\"RMSE of {best_rmse:.3f} days. Machine learning models (Random Forest and Gradient Boosting) \"\n",
    "        f\"significantly outperformed traditional statistical models.\"\n",
    "    )\n",
    "    doc.add_paragraph(summary_text)\n",
    "    \n",
    "    # Model Performance Summary\n",
    "    doc.add_heading('Model Performance Summary', level=1)\n",
    "    doc.add_paragraph('The following table shows the performance metrics for all models on the test set:')\n",
    "    \n",
    "    # Add main results table\n",
    "    table = doc.add_table(rows=1, cols=len(results_df.columns))\n",
    "    table.style = 'Light Grid'\n",
    "    \n",
    "    # Add headers\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    for i, col in enumerate(results_df.columns):\n",
    "        hdr_cells[i].text = col\n",
    "        hdr_cells[i].paragraphs[0].runs[0].bold = True\n",
    "    \n",
    "    # Add data\n",
    "    for _, row in results_df.iterrows():\n",
    "        row_cells = table.add_row().cells\n",
    "        for i, value in enumerate(row):\n",
    "            if isinstance(value, (int, float)):\n",
    "                row_cells[i].text = f'{value:.3f}'\n",
    "            else:\n",
    "                row_cells[i].text = str(value)\n",
    "    \n",
    "    # Add interpretation\n",
    "    doc.add_paragraph()\n",
    "    doc.add_heading('Metrics Interpretation:', level=3)\n",
    "    interpretations = [\n",
    "        f\"• MAE (Mean Absolute Error): Average prediction error is approximately {results_df['MAE'].mean():.2f} days\",\n",
    "        f\"• RMSE (Root Mean Square Error): Typical prediction error is approximately {results_df['RMSE'].mean():.2f} days\",\n",
    "        f\"• R² Score: Models explain between {results_df['R²'].min():.1%} and {results_df['R²'].max():.1%} of the variance\",\n",
    "        f\"• MAPE: Average percentage error ranges from {results_df['MAPE'].min():.1f}% to {results_df['MAPE'].max():.1f}%\",\n",
    "        f\"• Median AE: Half of predictions are within {results_df['Median AE'].mean():.2f} days of actual values\"\n",
    "    ]\n",
    "    for interp in interpretations:\n",
    "        doc.add_paragraph(interp)\n",
    "    \n",
    "    # Cross-Validation Results\n",
    "    doc.add_page_break()\n",
    "    doc.add_heading('Cross-Validation Results', level=1)\n",
    "    doc.add_paragraph('Cross-validation ensures model stability and generalization:')\n",
    "    \n",
    "    if not cv_results.empty:\n",
    "        # Add CV table\n",
    "        cv_table = doc.add_table(rows=1, cols=len(cv_results.columns))\n",
    "        cv_table.style = 'Light Grid'\n",
    "        \n",
    "        # Headers\n",
    "        hdr_cells = cv_table.rows[0].cells\n",
    "        for i, col in enumerate(cv_results.columns):\n",
    "            hdr_cells[i].text = col\n",
    "            hdr_cells[i].paragraphs[0].runs[0].bold = True\n",
    "        \n",
    "        # Data\n",
    "        for _, row in cv_results.iterrows():\n",
    "            row_cells = cv_table.add_row().cells\n",
    "            for i, value in enumerate(row):\n",
    "                if isinstance(value, (int, float)):\n",
    "                    row_cells[i].text = f'{value:.3f}'\n",
    "                else:\n",
    "                    row_cells[i].text = str(value)\n",
    "        \n",
    "        doc.add_paragraph()\n",
    "        doc.add_paragraph(\n",
    "            \"The low standard deviations indicate that both machine learning models \"\n",
    "            \"perform consistently across different data splits, suggesting good generalization.\"\n",
    "        )\n",
    "    \n",
    "    # Feature Importance\n",
    "    doc.add_heading('Feature Importance Analysis', level=1)\n",
    "    \n",
    "    # Random Forest Feature Importance\n",
    "    if hasattr(comparison, 'rf_importance'):\n",
    "        doc.add_heading('Random Forest - Top 10 Important Features', level=2)\n",
    "        \n",
    "        rf_table = doc.add_table(rows=1, cols=2)\n",
    "        rf_table.style = 'Light Grid'\n",
    "        \n",
    "        # Headers\n",
    "        rf_table.rows[0].cells[0].text = 'Feature'\n",
    "        rf_table.rows[0].cells[1].text = 'Importance'\n",
    "        for cell in rf_table.rows[0].cells:\n",
    "            cell.paragraphs[0].runs[0].bold = True\n",
    "        \n",
    "        # Data\n",
    "        for _, row in comparison.rf_importance.head(10).iterrows():\n",
    "            row_cells = rf_table.add_row().cells\n",
    "            row_cells[0].text = row['feature']\n",
    "            row_cells[1].text = f\"{row['importance']:.4f}\"\n",
    "    \n",
    "    # Gradient Boosting Feature Importance\n",
    "    if hasattr(comparison, 'gb_importance'):\n",
    "        doc.add_paragraph()\n",
    "        doc.add_heading('Gradient Boosting - Top 10 Important Features', level=2)\n",
    "        \n",
    "        gb_table = doc.add_table(rows=1, cols=2)\n",
    "        gb_table.style = 'Light Grid'\n",
    "        \n",
    "        # Headers\n",
    "        gb_table.rows[0].cells[0].text = 'Feature'\n",
    "        gb_table.rows[0].cells[1].text = 'Importance'\n",
    "        for cell in gb_table.rows[0].cells:\n",
    "            cell.paragraphs[0].runs[0].bold = True\n",
    "        \n",
    "        # Data\n",
    "        for _, row in comparison.gb_importance.head(10).iterrows():\n",
    "            row_cells = gb_table.add_row().cells\n",
    "            row_cells[0].text = row['feature']\n",
    "            row_cells[1].text = f\"{row['importance']:.4f}\"\n",
    "    \n",
    "    # Model Comparison Insights\n",
    "    doc.add_page_break()\n",
    "    doc.add_heading('Model Comparison Insights', level=1)\n",
    "    \n",
    "    insights = [\n",
    "        \"1. **Machine Learning Superiority**: Random Forest and Gradient Boosting models \"\n",
    "        f\"outperform traditional statistical models by approximately {((results_df[results_df['Model'].isin(['Random Forest', 'Gradient Boosting'])]['R²'].mean() / results_df[results_df['Model'].isin(['Negative Binomial', 'Mixed Effects'])]['R²'].mean()) - 1) * 100:.0f}% in terms of R² score.\",\n",
    "        \n",
    "        \"2. **Prediction Accuracy**: All models achieve median absolute errors around 1 day, \"\n",
    "        \"indicating that half of all predictions are within 24 hours of the actual length of stay.\",\n",
    "        \n",
    "        \"3. **Model Stability**: Cross-validation results show low standard deviations, \"\n",
    "        \"confirming that the models will perform consistently on new data.\",\n",
    "        \n",
    "        f\"4. **Best Model**: {best_model} achieves the best overall performance with \"\n",
    "        f\"R² = {best_r2:.3f} and RMSE = {best_rmse:.3f} days.\",\n",
    "        \n",
    "        \"5. **Error Distribution**: 90% of predictions are within 2.4 days of actual values, \"\n",
    "        \"with maximum errors around 5-6 days across all models.\"\n",
    "    ]\n",
    "    \n",
    "    for insight in insights:\n",
    "        p = doc.add_paragraph(insight)\n",
    "        # Bold the text between ** markers\n",
    "        for run in p.runs:\n",
    "            if '**' in run.text:\n",
    "                parts = run.text.split('**')\n",
    "                run.text = parts[0]\n",
    "                for i in range(1, len(parts), 2):\n",
    "                    if i < len(parts):\n",
    "                        bold_run = p.add_run(parts[i])\n",
    "                        bold_run.bold = True\n",
    "                    if i + 1 < len(parts):\n",
    "                        p.add_run(parts[i + 1])\n",
    "    \n",
    "    # Recommendations\n",
    "    doc.add_heading('Recommendations', level=1)\n",
    "    \n",
    "    recommendations = [\n",
    "        f\"1. **Deploy {best_model}** for production use, as it provides the best predictive performance.\",\n",
    "        \n",
    "        \"2. **Consider Ensemble Approach**: Combine predictions from both Random Forest and \"\n",
    "        \"Gradient Boosting models to potentially improve accuracy further.\",\n",
    "        \n",
    "        \"3. **Feature Engineering**: The relatively low R² values (< 0.15) suggest that \"\n",
    "        \"additional features could improve model performance. Consider adding:\",\n",
    "        \"   • Historical patient data\",\n",
    "        \"   • Seasonal patterns\",\n",
    "        \"   • Hospital capacity metrics\",\n",
    "        \"   • More detailed medical information\",\n",
    "        \n",
    "        \"4. **Model Monitoring**: Implement monitoring to track model performance over time \"\n",
    "        \"and retrain periodically as patterns change.\",\n",
    "        \n",
    "        \"5. **Prediction Intervals**: Given the ~23% MAPE, consider providing prediction \"\n",
    "        \"intervals rather than point estimates for better decision-making.\"\n",
    "    ]\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        p = doc.add_paragraph(rec)\n",
    "        # Bold the text between ** markers\n",
    "        for run in p.runs:\n",
    "            if '**' in run.text:\n",
    "                parts = run.text.split('**')\n",
    "                run.text = parts[0]\n",
    "                for i in range(1, len(parts), 2):\n",
    "                    if i < len(parts):\n",
    "                        bold_run = p.add_run(parts[i])\n",
    "                        bold_run.bold = True\n",
    "                    if i + 1 < len(parts):\n",
    "                        p.add_run(parts[i + 1])\n",
    "    \n",
    "    # Add plots if available\n",
    "    if hasattr(comparison, 'plot_results'):\n",
    "        doc.add_page_break()\n",
    "        doc.add_heading('Visual Analysis', level=1)\n",
    "        \n",
    "        # Create and save plots\n",
    "        fig = comparison.plot_results()\n",
    "        if fig:\n",
    "            # Save plot to buffer\n",
    "            img_buffer = io.BytesIO()\n",
    "            fig.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')\n",
    "            img_buffer.seek(0)\n",
    "            \n",
    "            # Add to document\n",
    "            doc.add_picture(img_buffer, width=Inches(6.5))\n",
    "            doc.add_paragraph('Figure 1: Model Performance Comparison', style='Caption')\n",
    "            \n",
    "            # Close the figure\n",
    "            plt.close(fig)\n",
    "    \n",
    "    # Technical Details\n",
    "    doc.add_page_break()\n",
    "    doc.add_heading('Technical Details', level=1)\n",
    "    \n",
    "    tech_details = [\n",
    "        f\"• **Data Split**: 80% training, 20% testing\",\n",
    "        f\"• **Total Observations**: {len(comparison.df_nb):,}\",\n",
    "        f\"• **Features Used**: {len(comparison.X_ml.columns)} variables\",\n",
    "        f\"• **Cross-Validation**: 5-fold cross-validation\",\n",
    "        \"• **Random State**: 42 (for reproducibility)\",\n",
    "        \"• **Models Compared**:\",\n",
    "        \"  - Negative Binomial (GLM with log link)\",\n",
    "        \"  - Mixed Effects (with state-level random effects)\",\n",
    "        \"  - Random Forest (100 trees, max depth 20)\",\n",
    "        \"  - Gradient Boosting (100 trees, max depth 6, learning rate 0.1)\"\n",
    "    ]\n",
    "    \n",
    "    for detail in tech_details:\n",
    "        p = doc.add_paragraph(detail)\n",
    "        # Bold the text between ** markers\n",
    "        for run in p.runs:\n",
    "            if '**' in run.text:\n",
    "                parts = run.text.split('**')\n",
    "                run.text = parts[0]\n",
    "                for i in range(1, len(parts), 2):\n",
    "                    if i < len(parts):\n",
    "                        bold_run = p.add_run(parts[i])\n",
    "                        bold_run.bold = True\n",
    "                    if i + 1 < len(parts):\n",
    "                        p.add_run(parts[i + 1])\n",
    "    \n",
    "    # Save the document\n",
    "    doc.save(filename)\n",
    "    print(f\"Report saved as: {filename}\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# Additional function to create a summary statistics table\n",
    "def add_data_summary_to_doc(doc, df_clean_final_nb):\n",
    "    \"\"\"Add data summary statistics to the document\"\"\"\n",
    "    doc.add_heading('Data Summary', level=1)\n",
    "    \n",
    "    # Basic statistics\n",
    "    summary_stats = df_clean_final_nb['los_capped'].describe()\n",
    "    \n",
    "    stats_table = doc.add_table(rows=1, cols=2)\n",
    "    stats_table.style = 'Light Grid'\n",
    "    \n",
    "    # Headers\n",
    "    stats_table.rows[0].cells[0].text = 'Statistic'\n",
    "    stats_table.rows[0].cells[1].text = 'Value'\n",
    "    for cell in stats_table.rows[0].cells:\n",
    "        cell.paragraphs[0].runs[0].bold = True\n",
    "    \n",
    "    # Data\n",
    "    for stat_name, value in summary_stats.items():\n",
    "        row_cells = stats_table.add_row().cells\n",
    "        row_cells[0].text = stat_name.capitalize()\n",
    "        row_cells[1].text = f\"{value:.2f} days\"\n",
    "    \n",
    "    return doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950351f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "# After running your model comparison:\n",
    "filename = export_model_comparison_to_word(\n",
    "    comparison=comparison,\n",
    "    results_df=results_df,\n",
    "    cv_results=cv_results,\n",
    "    filename=\"length_of_stay_model_comparison.docx\"\n",
    ")\n",
    "\n",
    "print(f\"Report has been saved to: {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
