{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select the input Excel file...\n",
      "Loading data from: /Users/janai/Library/CloudStorage/OneDrive-SharedLibraries-jlconsulting.llc/Projects - Documents/Research/Saint Lucia Tourism Piece/stata raw data.xlsx\n",
      "\n",
      "Missing data summary:\n",
      "los                         75\n",
      "age                          0\n",
      "sex                         26\n",
      "marital_status              12\n",
      "employment_status          425\n",
      "distance_miles               0\n",
      "purpose                    403\n",
      "accomd_type                  0\n",
      "state_percapita_income       0\n",
      "state_unemployment           0\n",
      "travel_date                  0\n",
      "month_travel                 0\n",
      "import_from_slu              0\n",
      "immigrant_population         0\n",
      "us_state                     0\n",
      "sex_enc                      0\n",
      "marital_status_enc           0\n",
      "employment_status_enc        0\n",
      "purpose_enc                  0\n",
      "accomd_type_enc              0\n",
      "us_state_enc                 0\n",
      "los_trunc                 2178\n",
      "dtype: int64\n",
      "\n",
      "Missing data patterns:\n",
      "0    142227\n",
      "1      2910\n",
      "2       103\n",
      "3         1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Length of stay by purpose:\n",
      "              count        mean  median  min      max          std\n",
      "purpose_enc                                                       \n",
      "-1              389    7.748072     6.0  2.0    373.0    19.536597\n",
      " 0             2131    6.845612     4.0  1.0    372.0    21.176125\n",
      " 1              425    6.334118     5.0  1.0    368.0    18.004622\n",
      " 2              175    7.000000     4.0  1.0    124.0    13.963005\n",
      " 3              432    4.282407     4.0  1.0     17.0     2.203955\n",
      " 4                2    7.000000     7.0  7.0      7.0     0.000000\n",
      " 5              407    6.154791     5.0  2.0     92.0     6.614890\n",
      " 6            22066    6.658253     6.0  1.0    372.0     6.694363\n",
      " 7             1212   69.916667     7.0  1.0  73109.0  2099.927812\n",
      " 8             1361  119.255694     7.0  1.0  73055.0  2799.175773\n",
      " 9           102372    8.927089     6.0  1.0  64986.0   264.902993\n",
      " 10              28    9.857143     7.0  2.0     29.0     6.948031\n",
      " 11            1399    7.987848     7.0  1.0    159.0     7.711945\n",
      " 12             377    6.785146     5.0  1.0    157.0    10.564593\n",
      " 13             177    5.762712     5.0  1.0     42.0     4.328837\n",
      " 14              42   12.214286     7.0  3.0    166.0    24.656208\n",
      " 15            3439   13.221867     8.0  1.0    393.0    25.614415\n",
      " 16            6623    6.179828     5.0  1.0    375.0    14.807467\n",
      " 17               6    6.333333     6.0  6.0      7.0     0.516398\n",
      "\n",
      "Detailed summary of length of stay:\n",
      "count    143063.000000\n",
      "mean         10.035565\n",
      "std         402.763642\n",
      "min           1.000000\n",
      "25%           5.000000\n",
      "50%           6.000000\n",
      "75%           7.000000\n",
      "90%           8.000000\n",
      "95%          10.000000\n",
      "99%          28.000000\n",
      "max       73109.000000\n",
      "Name: los_trunc, dtype: float64\n",
      "\n",
      "Number of missing values per observation:\n",
      "missing\n",
      "0    145166\n",
      "1        75\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Remaining observations after dropping missing values: 145166\n",
      "After filtering to 95th percentile, remaining observations: 136252\n",
      "\n",
      "Purpose simple distribution:\n",
      "1.0 (Business): 980\n",
      "2.0 (Events): 22549\n",
      "3.0 (Wedding): 6\n",
      "4.0 (Pleasure): 1104\n",
      "5.0 (Other): 109257\n",
      "\n",
      "Fitting simple negative binomial regression model...\n",
      "Model formula: los_capped ~ immigrant_population + import_from_slu + age + distance_miles + state_percapita_income + state_unemployment + C(sex_enc) + C(marital_status_enc) + C(employment_status_enc) + C(purpose_simple) + C(accomd_type_enc) + C(month_travel) + C(us_state_enc)\n",
      "Number of rows in df_clean_nb after dropping missing values: 132666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/jlslu2025/lib/python3.13/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/jlslu2025/lib/python3.13/site-packages/statsmodels/genmod/families/family.py:1367: ValueWarning: Negative binomial dispersion parameter alpha not set. Using default value alpha=1.0.\n",
      "  warnings.warn(\"Negative binomial dispersion parameter alpha not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negative Binomial Regression Results:\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:             los_capped   No. Observations:               132666\n",
      "Model:                            GLM   Df Residuals:                   132590\n",
      "Model Family:        NegativeBinomial   Df Model:                           75\n",
      "Link Function:                    log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:            -3.7630e+05\n",
      "Date:                Fri, 28 Mar 2025   Deviance:                       8931.3\n",
      "Time:                        09:03:17   Pearson chi2:                 8.24e+03\n",
      "No. Iterations:                    65   Pseudo R-squ. (CS):           0.007270\n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Intercept                         0.4554      0.241      1.891      0.059      -0.016       0.927\n",
      "C(sex_enc)[T.0]                  -0.1144      0.230     -0.498      0.618      -0.564       0.336\n",
      "C(sex_enc)[T.1]                  -0.1037      0.230     -0.452      0.652      -0.554       0.346\n",
      "C(marital_status_enc)[T.0]        0.0600      0.315      0.190      0.849      -0.558       0.678\n",
      "C(marital_status_enc)[T.1]        0.0368      0.316      0.117      0.907      -0.582       0.656\n",
      "C(marital_status_enc)[T.2]        0.0159      0.315      0.051      0.960      -0.602       0.634\n",
      "C(employment_status_enc)[T.0]    -0.0022      0.056     -0.039      0.969      -0.112       0.107\n",
      "C(employment_status_enc)[T.1]     0.0137      0.066      0.208      0.835      -0.115       0.143\n",
      "C(employment_status_enc)[T.2]    -0.0100      0.073     -0.138      0.890      -0.153       0.133\n",
      "C(purpose_simple)[T.2.0]          0.4182      0.037     11.338      0.000       0.346       0.491\n",
      "C(purpose_simple)[T.3.0]          0.3963      0.441      0.899      0.369      -0.468       1.260\n",
      "C(purpose_simple)[T.4.0]          0.2246      0.049      4.600      0.000       0.129       0.320\n",
      "C(purpose_simple)[T.5.0]          0.2817      0.036      7.821      0.000       0.211       0.352\n",
      "C(accomd_type_enc)[T.1]           0.0413      0.016      2.543      0.011       0.009       0.073\n",
      "C(accomd_type_enc)[T.2]           0.0935      0.009     10.082      0.000       0.075       0.112\n",
      "C(month_travel)[T.2]             -0.0132      0.015     -0.877      0.381      -0.043       0.016\n",
      "C(month_travel)[T.3]             -0.0009      0.015     -0.060      0.952      -0.030       0.028\n",
      "C(month_travel)[T.4]             -0.0203      0.015     -1.317      0.188      -0.050       0.010\n",
      "C(month_travel)[T.5]             -0.0188      0.015     -1.265      0.206      -0.048       0.010\n",
      "C(month_travel)[T.6]             -0.0094      0.015     -0.637      0.524      -0.038       0.019\n",
      "C(month_travel)[T.7]              0.0233      0.015      1.582      0.114      -0.006       0.052\n",
      "C(month_travel)[T.8]             -0.0401      0.016     -2.566      0.010      -0.071      -0.009\n",
      "C(month_travel)[T.9]             -0.0352      0.017     -2.036      0.042      -0.069      -0.001\n",
      "C(month_travel)[T.10]            -0.0331      0.016     -2.080      0.038      -0.064      -0.002\n",
      "C(month_travel)[T.11]            -0.0207      0.016     -1.326      0.185      -0.051       0.010\n",
      "C(month_travel)[T.12]             0.0317      0.015      2.082      0.037       0.002       0.062\n",
      "C(us_state_enc)[T.2]             -0.1933      0.213     -0.909      0.364      -0.610       0.224\n",
      "C(us_state_enc)[T.3]              0.0790      0.043      1.845      0.065      -0.005       0.163\n",
      "C(us_state_enc)[T.4]              0.0022      0.042      0.053      0.958      -0.080       0.085\n",
      "C(us_state_enc)[T.5]             -0.1430      0.053     -2.714      0.007      -0.246      -0.040\n",
      "C(us_state_enc)[T.6]             -0.1128      0.054     -2.082      0.037      -0.219      -0.007\n",
      "C(us_state_enc)[T.7]             -0.1675      0.058     -2.873      0.004      -0.282      -0.053\n",
      "C(us_state_enc)[T.8]             -0.0038      0.052     -0.073      0.941      -0.106       0.098\n",
      "C(us_state_enc)[T.9]              0.0229      0.011      2.026      0.043       0.001       0.045\n",
      "C(us_state_enc)[T.10]            -0.0239      0.022     -1.098      0.272      -0.067       0.019\n",
      "C(us_state_enc)[T.11]             0.0373      0.148      0.252      0.801      -0.253       0.327\n",
      "C(us_state_enc)[T.12]             0.0807      0.073      1.101      0.271      -0.063       0.224\n",
      "C(us_state_enc)[T.13]            -0.1336      0.057     -2.325      0.020      -0.246      -0.021\n",
      "C(us_state_enc)[T.14]             0.0066      0.040      0.165      0.869      -0.071       0.085\n",
      "C(us_state_enc)[T.15]             0.0093      0.042      0.223      0.823      -0.072       0.091\n",
      "C(us_state_enc)[T.16]            -0.0823      0.044     -1.856      0.063      -0.169       0.005\n",
      "C(us_state_enc)[T.17]             0.0277      0.044      0.627      0.531      -0.059       0.114\n",
      "C(us_state_enc)[T.18]            -0.0920      0.042     -2.168      0.030      -0.175      -0.009\n",
      "C(us_state_enc)[T.20]             0.1124      0.046      2.422      0.015       0.021       0.203\n",
      "C(us_state_enc)[T.21]            -0.0750      0.034     -2.224      0.026      -0.141      -0.009\n",
      "C(us_state_enc)[T.22]            -0.1720      0.069     -2.478      0.013      -0.308      -0.036\n",
      "C(us_state_enc)[T.23]             0.0069      0.041      0.169      0.866      -0.073       0.087\n",
      "C(us_state_enc)[T.24]             0.0174      0.040      0.435      0.663      -0.061       0.095\n",
      "C(us_state_enc)[T.25]             0.0492      0.039      1.265      0.206      -0.027       0.125\n",
      "C(us_state_enc)[T.26]            -0.0410      0.034     -1.220      0.222      -0.107       0.025\n",
      "C(us_state_enc)[T.27]             0.0477      0.077      0.615      0.538      -0.104       0.199\n",
      "C(us_state_enc)[T.28]            -0.0831      0.046     -1.793      0.073      -0.174       0.008\n",
      "C(us_state_enc)[T.29]            -0.1160      0.065     -1.791      0.073      -0.243       0.011\n",
      "C(us_state_enc)[T.30]             0.0370      0.043      0.859      0.390      -0.047       0.121\n",
      "C(us_state_enc)[T.31]            -0.1183      0.051     -2.341      0.019      -0.217      -0.019\n",
      "C(us_state_enc)[T.32]             0.0532      0.083      0.637      0.524      -0.110       0.217\n",
      "C(us_state_enc)[T.33]             0.0481      0.015      3.184      0.001       0.018       0.078\n",
      "C(us_state_enc)[T.34]            -0.0275      0.032     -0.865      0.387      -0.090       0.035\n",
      "C(us_state_enc)[T.35]            -0.0107      0.088     -0.122      0.903      -0.182       0.161\n",
      "C(us_state_enc)[T.36]             0.0020      0.038      0.053      0.958      -0.073       0.077\n",
      "C(us_state_enc)[T.37]            -0.0415      0.039     -1.053      0.292      -0.119       0.036\n",
      "C(us_state_enc)[T.38]             0.0915      0.067      1.370      0.171      -0.039       0.223\n",
      "C(us_state_enc)[T.39]             0.0107      0.033      0.321      0.748      -0.055       0.076\n",
      "C(us_state_enc)[T.40]             0.0069      0.059      0.116      0.907      -0.109       0.122\n",
      "C(us_state_enc)[T.41]            -0.0409      0.040     -1.010      0.312      -0.120       0.038\n",
      "C(us_state_enc)[T.42]            -0.0299      0.067     -0.446      0.656      -0.161       0.102\n",
      "C(us_state_enc)[T.43]            -0.0247      0.031     -0.792      0.428      -0.086       0.036\n",
      "C(us_state_enc)[T.44]            -0.0639      0.025     -2.593      0.010      -0.112      -0.016\n",
      "C(us_state_enc)[T.46]             0.0441      0.057      0.780      0.435      -0.067       0.155\n",
      "C(us_state_enc)[T.47]             0.1041      0.066      1.585      0.113      -0.025       0.233\n",
      "C(us_state_enc)[T.48]            -0.0452      0.036     -1.268      0.205      -0.115       0.025\n",
      "C(us_state_enc)[T.49]            -0.0612      0.059     -1.032      0.302      -0.177       0.055\n",
      "C(us_state_enc)[T.51]             0.1344      0.065      2.080      0.037       0.008       0.261\n",
      "C(us_state_enc)[T.52]             0.0789      0.032      2.504      0.012       0.017       0.141\n",
      "C(us_state_enc)[T.53]            -0.2224      0.095     -2.336      0.019      -0.409      -0.036\n",
      "immigrant_population          -2.067e-05      6e-06     -3.445      0.001   -3.24e-05   -8.91e-06\n",
      "import_from_slu               -5.015e-09   7.04e-09     -0.712      0.476   -1.88e-08    8.78e-09\n",
      "age                               0.0025      0.000     10.573      0.000       0.002       0.003\n",
      "distance_miles                 1.771e-05   2.77e-05      0.640      0.522   -3.65e-05     7.2e-05\n",
      "state_percapita_income         1.382e-05      3e-06      4.603      0.000    7.93e-06    1.97e-05\n",
      "state_unemployment                0.0433      0.018      2.431      0.015       0.008       0.078\n",
      "=================================================================================================\n",
      "\n",
      "Incident Rate Ratios (IRR):\n",
      "                                 IRR  Lower CI  Upper CI       P-value\n",
      "Intercept                   1.576762  0.983640  2.527530  5.856442e-02\n",
      "C(sex_enc)[T.0]             0.891859  0.568653  1.398767  6.181819e-01\n",
      "C(sex_enc)[T.1]             0.901479  0.574759  1.413920  6.515138e-01\n",
      "C(marital_status_enc)[T.0]  1.061865  0.572580  1.969255  8.489283e-01\n",
      "C(marital_status_enc)[T.1]  1.037510  0.558719  1.926598  9.071696e-01\n",
      "...                              ...       ...       ...           ...\n",
      "import_from_slu             1.000000  1.000000  1.000000  4.763273e-01\n",
      "age                         1.002547  1.002075  1.003020  3.983495e-26\n",
      "distance_miles              1.000018  0.999963  1.000072  5.222801e-01\n",
      "state_percapita_income      1.000014  1.000008  1.000020  4.156927e-06\n",
      "state_unemployment          1.044214  1.008421  1.081277  1.504971e-02\n",
      "\n",
      "[81 rows x 4 columns]\n",
      "Columns in df_clean_nb: ['los_capped', 'immigrant_population', 'import_from_slu', 'age', 'distance_miles', 'state_percapita_income', 'state_unemployment', 'sex_enc', 'marital_status_enc', 'employment_status_enc', 'purpose_simple', 'accomd_type_enc', 'month_travel', 'us_state_enc', 'predicted', 'residuals']\n",
      "Checking if 'us_state_enc' is in df_clean_nb.columns: True\n",
      "X dtypes after converting to float:\n",
      "const                      float64\n",
      "immigrant_population       float64\n",
      "import_from_slu            float64\n",
      "age                        float64\n",
      "distance_miles             float64\n",
      "state_unemployment         float64\n",
      "sex_enc_0                  float64\n",
      "sex_enc_1                  float64\n",
      "marital_status_enc_0       float64\n",
      "marital_status_enc_1       float64\n",
      "marital_status_enc_2       float64\n",
      "employment_status_enc_0    float64\n",
      "employment_status_enc_1    float64\n",
      "employment_status_enc_2    float64\n",
      "purpose_simple_2.0         float64\n",
      "purpose_simple_3.0         float64\n",
      "purpose_simple_4.0         float64\n",
      "purpose_simple_5.0         float64\n",
      "accomd_type_enc_1          float64\n",
      "accomd_type_enc_2          float64\n",
      "month_travel_2             float64\n",
      "month_travel_3             float64\n",
      "month_travel_4             float64\n",
      "month_travel_5             float64\n",
      "month_travel_6             float64\n",
      "month_travel_7             float64\n",
      "month_travel_8             float64\n",
      "month_travel_9             float64\n",
      "month_travel_10            float64\n",
      "month_travel_11            float64\n",
      "month_travel_12            float64\n",
      "dtype: object\n",
      "Length of df_clean_model: 132666\n",
      "Length of y: 132666\n",
      "Number of rows in X: 132666\n",
      "Length of groups: 132666\n",
      "\n",
      "Approximated Multilevel Model Results:\n",
      "               Mixed Linear Model Regression Results\n",
      "===================================================================\n",
      "Model:               MixedLM    Dependent Variable:    los_capped  \n",
      "No. Observations:    132666     Method:                REML        \n",
      "No. Groups:          50         Scale:                 2.3970      \n",
      "Min. group size:     21         Log-Likelihood:        -246425.8720\n",
      "Max. group size:     15897      Converged:             Yes         \n",
      "Mean group size:     2653.3                                        \n",
      "-------------------------------------------------------------------\n",
      "                        Coef.  Std.Err.    z    P>|z| [0.025 0.975]\n",
      "-------------------------------------------------------------------\n",
      "const                    4.046    0.611   6.619 0.000  2.848  5.244\n",
      "immigrant_population    -0.000    0.000  -1.076 0.282 -0.000  0.000\n",
      "import_from_slu         -0.000    0.000  -1.485 0.138 -0.000  0.000\n",
      "age                      0.015    0.000  42.969 0.000  0.014  0.015\n",
      "distance_miles           0.000    0.000   3.364 0.001  0.000  0.000\n",
      "state_unemployment      -0.087    0.054  -1.616 0.106 -0.192  0.018\n",
      "sex_enc_0               -0.652    0.330  -1.975 0.048 -1.300 -0.005\n",
      "sex_enc_1               -0.594    0.330  -1.797 0.072 -1.241  0.054\n",
      "marital_status_enc_0     0.323    0.448   0.720 0.471 -0.555  1.201\n",
      "marital_status_enc_1     0.185    0.449   0.413 0.680 -0.695  1.065\n",
      "marital_status_enc_2     0.077    0.448   0.172 0.863 -0.801  0.955\n",
      "employment_status_enc_0 -0.005    0.080  -0.061 0.952 -0.161  0.151\n",
      "employment_status_enc_1  0.082    0.094   0.873 0.383 -0.102  0.266\n",
      "employment_status_enc_2 -0.046    0.104  -0.439 0.661 -0.250  0.158\n",
      "purpose_simple_2.0       2.214    0.052  42.937 0.000  2.113  2.315\n",
      "purpose_simple_3.0       2.061    0.634   3.249 0.001  0.818  3.304\n",
      "purpose_simple_4.0       1.092    0.069  15.869 0.000  0.957  1.227\n",
      "purpose_simple_5.0       1.422    0.050  28.291 0.000  1.324  1.521\n",
      "accomd_type_enc_1        0.233    0.023  10.031 0.000  0.188  0.279\n",
      "accomd_type_enc_2        0.537    0.013  40.404 0.000  0.511  0.563\n",
      "month_travel_2          -0.077    0.022  -3.593 0.000 -0.120 -0.035\n",
      "month_travel_3          -0.016    0.021  -0.757 0.449 -0.057  0.025\n",
      "month_travel_4          -0.128    0.022  -5.789 0.000 -0.171 -0.084\n",
      "month_travel_5          -0.121    0.021  -5.718 0.000 -0.163 -0.080\n",
      "month_travel_6          -0.065    0.021  -3.085 0.002 -0.106 -0.024\n",
      "month_travel_7           0.115    0.021   5.470 0.000  0.074  0.157\n",
      "month_travel_8          -0.238    0.022 -10.651 0.000 -0.281 -0.194\n",
      "month_travel_9          -0.208    0.025  -8.412 0.000 -0.256 -0.159\n",
      "month_travel_10         -0.193    0.023  -8.479 0.000 -0.237 -0.148\n",
      "month_travel_11         -0.126    0.022  -5.651 0.000 -0.170 -0.082\n",
      "month_travel_12          0.169    0.022   7.734 0.000  0.126  0.212\n",
      "Group Var                0.088    0.013                            \n",
      "===================================================================\n",
      "\n",
      "\n",
      "Incident Rate Ratios (IRR) Mixed Model:\n",
      "                               IRR   Lower CI    Upper CI        P-value\n",
      "const                    57.154832  17.248342  189.390656   3.623060e-11\n",
      "immigrant_population      0.999970   0.999915    1.000025   2.818386e-01\n",
      "import_from_slu           1.000000   1.000000    1.000000   1.375140e-01\n",
      "age                       1.014891   1.014207    1.015576   0.000000e+00\n",
      "distance_miles            1.000192   1.000080    1.000305   7.673289e-04\n",
      "state_unemployment        0.916799   0.825123    1.018661   1.060921e-01\n",
      "sex_enc_0                 0.520873   0.272629    0.995156   4.830624e-02\n",
      "sex_enc_1                 0.552243   0.289029    1.055163   7.226883e-02\n",
      "marital_status_enc_0      1.380875   0.573849    3.322857   4.713303e-01\n",
      "marital_status_enc_1      1.203477   0.499193    2.901397   6.799540e-01\n",
      "marital_status_enc_2      1.080227   0.448888    2.599513   8.632498e-01\n",
      "employment_status_enc_0   0.995181   0.851188    1.163534   9.516982e-01\n",
      "employment_status_enc_1   1.085579   0.902851    1.305289   3.825566e-01\n",
      "employment_status_enc_2   0.955320   0.779039    1.171489   6.605229e-01\n",
      "purpose_simple_2.0        9.151866   8.272160   10.125125   0.000000e+00\n",
      "purpose_simple_3.0        7.852617   2.265387   27.219893   1.156969e-03\n",
      "purpose_simple_4.0        2.980026   2.604053    3.410283   1.038352e-56\n",
      "purpose_simple_5.0        4.146618   3.757521    4.576006  4.418540e-176\n",
      "accomd_type_enc_1         1.262573   1.206345    1.321423   1.117561e-23\n",
      "accomd_type_enc_2         1.710584   1.666612    1.755716   0.000000e+00\n",
      "month_travel_2            0.925485   0.887202    0.965421   3.273532e-04\n",
      "month_travel_3            0.984108   0.944144    1.025764   4.488417e-01\n",
      "month_travel_4            0.880287   0.843091    0.919123   7.094578e-09\n",
      "month_travel_5            0.885674   0.849576    0.923307   1.075684e-08\n",
      "month_travel_6            0.937154   0.899297    0.976605   2.034031e-03\n",
      "month_travel_7            1.122275   1.076836    1.169631   4.489153e-08\n",
      "month_travel_8            0.788510   0.754775    0.823753   1.732272e-26\n",
      "month_travel_9            0.812355   0.773957    0.852657   4.029793e-17\n",
      "month_travel_10           0.824675   0.788735    0.862253   2.272157e-17\n",
      "month_travel_11           0.881701   0.844027    0.921056   1.596082e-08\n",
      "month_travel_12           1.183882   1.134309    1.235621   1.039110e-14\n",
      "Group Var                 1.037330   1.020396    1.054544   1.275383e-05\n",
      "\n",
      "Please select where to save the Word document...\n",
      "Analysis report saved to: /Users/janai/Library/CloudStorage/OneDrive-SharedLibraries-jlconsulting.llc/Projects - Documents/Research/Saint Lucia Tourism Piece/fixed formatting?.docx\n",
      "\n",
      "Analysis complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.genmod.families.family import NegativeBinomial\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from scipy import stats\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from io import BytesIO\n",
    "import statsmodels.discrete.discrete_model as discrete\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "\n",
    "def select_file(title, file_types, save=False):\n",
    "    \"\"\"Allow user to select a file\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes('-topmost', True)\n",
    "    \n",
    "    try:\n",
    "        if save:\n",
    "            file_path = filedialog.asksaveasfilename(\n",
    "                title=title,\n",
    "                filetypes=file_types,\n",
    "                defaultextension=file_types[0][1]\n",
    "            )\n",
    "        else:\n",
    "            file_path = filedialog.askopenfilename(\n",
    "                title=title,\n",
    "                filetypes=file_types\n",
    "            )\n",
    "    finally:\n",
    "        root.destroy()\n",
    "    \n",
    "    return file_path if file_path else None\n",
    "\n",
    "# Allow user to select input file\n",
    "print(\"Please select the input Excel file...\")\n",
    "file_path = select_file(\n",
    "    \"Select Excel Data File\", \n",
    "    [(\"Excel files\", \"*.xlsx *.xls\"), (\"All files\", \"*.*\")]\n",
    ")\n",
    "\n",
    "if not file_path:\n",
    "    print(\"No file selected. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Import the data\n",
    "print(f\"Loading data from: {file_path}\")\n",
    "df = pd.read_excel(file_path, sheet_name=\"Stata\")\n",
    "\n",
    "# Setup\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Encode categorical variables if not already encoded\n",
    "categorical_vars = ['sex', 'marital_status', 'employment_status', 'purpose', 'accomd_type', 'us_state']\n",
    "encoded_vars = {}\n",
    "\n",
    "for var in categorical_vars:\n",
    "    if var in df.columns:\n",
    "        # Check if variable is already numeric\n",
    "        if not pd.api.types.is_numeric_dtype(df[var]):\n",
    "            new_var = f\"{var}_enc\"\n",
    "            df[new_var] = pd.Categorical(df[var]).codes\n",
    "            encoded_vars[var] = new_var\n",
    "        else:\n",
    "            encoded_vars[var] = var\n",
    "\n",
    "# Set the truncation point for los (assuming truncation at 0)\n",
    "df['los_trunc'] = df['los'].copy()\n",
    "df.loc[df['los_trunc'] <= 0, 'los_trunc'] = np.nan\n",
    "\n",
    "# Check for missing data\n",
    "print(\"\\nMissing data summary:\")\n",
    "missing_data_summary = df.isnull().sum()\n",
    "print(missing_data_summary)\n",
    "\n",
    "print(\"\\nMissing data patterns:\")\n",
    "missing_patterns = df.isnull().sum(axis=1)\n",
    "missing_patterns_counts = missing_patterns.value_counts().sort_index()\n",
    "print(missing_patterns_counts)\n",
    "\n",
    "# Visualize los distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['los_trunc'], discrete=True)\n",
    "plt.title('Histogram of Length of Stay')\n",
    "plt.tight_layout()\n",
    "los_hist_img = BytesIO()\n",
    "plt.savefig(los_hist_img, format='png')\n",
    "los_hist_img.seek(0)\n",
    "plt.close()\n",
    "\n",
    "# Summarize los by purpose\n",
    "purpose_stats = None\n",
    "if 'purpose_enc' in df.columns:\n",
    "    print(\"\\nLength of stay by purpose:\")\n",
    "    purpose_stats = df.groupby('purpose_enc')['los_trunc'].agg(['count', 'mean', 'median', 'min', 'max', 'std'])\n",
    "    print(purpose_stats)\n",
    "\n",
    "# Detailed summary of los_trunc\n",
    "print(\"\\nDetailed summary of length of stay:\")\n",
    "los_describe = df['los_trunc'].describe(percentiles=[.25, .5, .75, .90, .95, .99])\n",
    "print(los_describe)\n",
    "\n",
    "# Cleaning process\n",
    "# Step 1: Drop missing datapoints for key variables\n",
    "key_vars = ['los', 'immigrant_population', 'import_from_slu', 'age', \n",
    "            encoded_vars.get('sex', 'sex_enc'), \n",
    "            encoded_vars.get('marital_status', 'marital_status_enc'), \n",
    "            encoded_vars.get('employment_status', 'employment_status_enc'), \n",
    "            'distance_miles', \n",
    "            encoded_vars.get('purpose', 'purpose_enc'), \n",
    "            encoded_vars.get('accomd_type', 'accomd_type_enc'), \n",
    "            'month_travel', 'state_percapita_income', 'state_unemployment']\n",
    "\n",
    "# Count missing values per row for key variables\n",
    "df['missing'] = df[key_vars].isnull().sum(axis=1)\n",
    "print(\"\\nNumber of missing values per observation:\")\n",
    "missing_values_count = df['missing'].value_counts().sort_index()\n",
    "print(missing_values_count)\n",
    "\n",
    "# Drop observations with missing values in key variables\n",
    "df_clean = df[df['missing'] == 0].drop('missing', axis=1)\n",
    "print(f\"\\nRemaining observations after dropping missing values: {len(df_clean)}\")\n",
    "\n",
    "# Step 2: Drop outliers in length of stay\n",
    "los_p95 = np.percentile(df_clean['los_trunc'].dropna(), 95)\n",
    "df_clean['los_capped'] = df_clean['los_trunc'].copy()\n",
    "df_clean.loc[df_clean['los_capped'] > los_p95, 'los_capped'] = los_p95\n",
    "\n",
    "df_clean = df_clean[df_clean['los_trunc'] <= los_p95]\n",
    "print(f\"After filtering to 95th percentile, remaining observations: {len(df_clean)}\")\n",
    "\n",
    "\n",
    "# Visualize the capped los distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_clean['los_capped'], discrete=True)\n",
    "plt.title('Histogram of Capped Length of Stay')\n",
    "plt.tight_layout()\n",
    "los_capped_img = BytesIO()\n",
    "plt.savefig(los_capped_img, format='png')\n",
    "los_capped_img.seek(0)\n",
    "plt.close()\n",
    "\n",
    "# Step 3: Clean up the purpose of trip column\n",
    "# Create a new simplified purpose variable\n",
    "purpose_mapping = {\n",
    "    1: 1,  # BUSINESS/MEETING -> Business\n",
    "    2: 1,  # CONVENTION -> Business\n",
    "    3: 1,  # CREW -> Business\n",
    "    5: 2,  # EVENT -> Events\n",
    "    6: 2,  # EVENTS -> Events\n",
    "    7: 4,  # HONEYMOON -> Pleasure\n",
    "    8: 5,  # INTRANSIT PASSEN -> Other\n",
    "    9: 5,  # OTHER -> Other\n",
    "    10: 4, # PLEASURE/HOLIDAY -> Pleasure\n",
    "    11: 5, # RESIDENT -> Other\n",
    "    12: 2, # SAINT LUCIA CARN -> Events\n",
    "    13: 2, # SAINT LUCIA JAZZ -> Events\n",
    "    14: 5, # SPORTS -> Other\n",
    "    15: 5, # STUDY -> Other\n",
    "    16: 5, # VISITING FRIENDS -> Other\n",
    "    17: 3, # WEDDING -> Wedding\n",
    "    18: 4, # pLEASURE/HOLIDAY -> Pleasure\n",
    "    4: 5,  # CRICKET -> Other\n",
    "}\n",
    "\n",
    "purpose_labels = {\n",
    "    1: \"Business\",\n",
    "    2: \"Events\",\n",
    "    3: \"Wedding\",\n",
    "    4: \"Pleasure\",\n",
    "    5: \"Other\"\n",
    "}\n",
    "\n",
    "# Add the simplified purpose variable\n",
    "purpose_enc_col = encoded_vars.get('purpose', 'purpose_enc')\n",
    "df_clean['purpose_simple'] = df_clean[purpose_enc_col].map(purpose_mapping)\n",
    "\n",
    "# Check the new variable\n",
    "print(\"\\nPurpose simple distribution:\")\n",
    "purpose_counts = df_clean['purpose_simple'].value_counts().sort_index()\n",
    "purpose_distribution = []\n",
    "for code, count in purpose_counts.items():\n",
    "    purpose_line = f\"{code} ({purpose_labels.get(code, 'Unknown')}): {count}\"\n",
    "    purpose_distribution.append(purpose_line)\n",
    "    print(purpose_line)\n",
    "\n",
    "# Create a Word document for output\n",
    "doc = Document()\n",
    "doc.add_heading('Multilevel Truncated Negative Binomial Regression for Length of Stay Analysis', 0)\n",
    "doc.add_heading('Data Preparation and Cleaning', level=1)\n",
    "\n",
    "# Add missing data information\n",
    "doc.add_paragraph('Missing Data Summary:')\n",
    "missing_table = doc.add_table(rows=len(missing_data_summary)+1, cols=2)\n",
    "missing_table.style = 'Table Grid'\n",
    "missing_table.cell(0, 0).text = 'Variable'\n",
    "missing_table.cell(0, 1).text = 'Missing Count'\n",
    "for i, (var, count) in enumerate(missing_data_summary.items(), 1):\n",
    "    missing_table.cell(i, 0).text = str(var)\n",
    "    missing_table.cell(i, 1).text = str(count)\n",
    "\n",
    "doc.add_paragraph('\\nMissing Data Patterns:')\n",
    "patterns_table = doc.add_table(rows=len(missing_patterns_counts)+1, cols=2)\n",
    "patterns_table.style = 'Table Grid'\n",
    "patterns_table.cell(0, 0).text = 'Number of Missing Variables'\n",
    "patterns_table.cell(0, 1).text = 'Count'\n",
    "for i, (pattern, count) in enumerate(missing_patterns_counts.items(), 1):\n",
    "    patterns_table.cell(i, 0).text = str(pattern)\n",
    "    patterns_table.cell(i, 1).text = str(count)\n",
    "\n",
    "# Add Length of Stay histogram\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Length of Stay Distribution', level=2)\n",
    "doc.add_picture(los_hist_img, width=Inches(6))\n",
    "doc.add_paragraph('Figure 1: Histogram of Length of Stay (Before Capping)')\n",
    "\n",
    "# Add Capped LOS histogram\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Capped Length of Stay Distribution', level=2)\n",
    "doc.add_picture(los_capped_img, width=Inches(6))\n",
    "doc.add_paragraph('Figure 2: Histogram of Length of Stay (After Capping at 95th Percentile)')\n",
    "\n",
    "# Add LOS summary statistics\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Length of Stay Summary Statistics 95% Capped', level=2)\n",
    "los_stats_table = doc.add_table(rows=len(los_describe)+1, cols=2)\n",
    "los_stats_table.style = 'Table Grid'\n",
    "los_stats_table.cell(0, 0).text = 'Statistic'\n",
    "los_stats_table.cell(0, 1).text = 'Value'\n",
    "for i, (stat, value) in enumerate(los_describe.items(), 1):\n",
    "    los_stats_table.cell(i, 0).text = str(stat)\n",
    "    los_stats_table.cell(i, 1).text = f\"{value:.4f}\" if isinstance(value, (int, float)) else str(value)\n",
    "\n",
    "# Add Purpose distribution\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Purpose of Visit Distribution', level=2)\n",
    "purpose_table = doc.add_table(rows=len(purpose_distribution)+1, cols=1)\n",
    "purpose_table.style = 'Table Grid'\n",
    "purpose_table.cell(0, 0).text = 'Purpose Category'\n",
    "for i, purpose_text in enumerate(purpose_distribution, 1):\n",
    "    purpose_table.cell(i, 0).text = purpose_text\n",
    "\n",
    "# Fit simple negative binomial regression with continuous variables correctly specified\n",
    "print(\"\\nFitting simple negative binomial regression model...\")\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Negative Binomial Regression Model', level=1)\n",
    "\n",
    "# Define continuous variables and create proper formula\n",
    "continuous_vars = ['immigrant_population', 'import_from_slu', 'age', 'distance_miles', \n",
    "                   'state_percapita_income', 'state_unemployment']\n",
    "\n",
    "# Make sure all continuous variables are properly formatted as numeric\n",
    "for var in continuous_vars:\n",
    "    if var in df_clean.columns:\n",
    "        df_clean[var] = pd.to_numeric(df_clean[var], errors='coerce')\n",
    "\n",
    "# Create formula with continuous variables properly treated\n",
    "formula_parts = []\n",
    "for var in continuous_vars:\n",
    "    if var in df_clean.columns:\n",
    "        formula_parts.append(var)\n",
    "\n",
    "# Add categorical variables with proper C() notation\n",
    "categorical_model_vars = ['sex_enc', 'marital_status_enc', 'employment_status_enc', \n",
    "                         'purpose_simple', 'accomd_type_enc', 'month_travel', 'us_state_enc']\n",
    "\n",
    "for var in categorical_model_vars:\n",
    "    if var in df_clean.columns:\n",
    "        # Use the encoded variable name or the original if available\n",
    "        var_to_use = var\n",
    "        formula_parts.append(f\"C({var_to_use})\")\n",
    "\n",
    "# Combine into final formula\n",
    "formula = 'los_capped ~ ' + ' + '.join(formula_parts)\n",
    "print(f\"Model formula: {formula}\")\n",
    "\n",
    "# Add formula to document\n",
    "doc.add_paragraph(f\"Model formula: {formula}\")\n",
    "\n",
    "# Drop rows with missing values in formula variables\n",
    "##formula_vars = ['los_capped'] + continuous_vars + categorical_model_vars\n",
    "formula_vars = ['los_capped'] + continuous_vars + categorical_model_vars\n",
    "df_clean_nb = df_clean[formula_vars].dropna()\n",
    "df_clean_nb = df_clean_nb.reset_index(drop=True)\n",
    "print(f\"Number of rows in df_clean_nb after dropping missing values: {len(df_clean_nb)}\")\n",
    "\n",
    "\n",
    "# Fit negative binomial model\n",
    "nb_model = smf.glm(formula=formula, \n",
    "                  data=df_clean_nb, \n",
    "                  family=sm.families.NegativeBinomial(link=sm.families.links.log()))\n",
    "\n",
    "try:\n",
    "    nb_results = nb_model.fit()\n",
    "    print(\"\\nNegative Binomial Regression Results:\")\n",
    "    summary_text = str(nb_results.summary())\n",
    "    print(summary_text)\n",
    "    \n",
    "    # Add model summary to document\n",
    "    #doc.add_paragraph('\\nModel Summary:')\n",
    "    summary_paragraph_neg = doc.add_paragraph()\n",
    "    summary_run_neg = summary_paragraph_neg.add_run(summary_text)\n",
    "    summary_run_neg.font.name = 'Courier New'  # Use monospace font\n",
    "    #for line in summary_text.split('\\n'):\n",
    "        #doc.add_paragraph(line)\n",
    "    \n",
    "    # Convert coefficients to incident rate ratios (IRR)\n",
    "    print(\"\\nIncident Rate Ratios (IRR):\")\n",
    "    irr = np.exp(nb_results.params)\n",
    "    irr_conf = np.exp(nb_results.conf_int())\n",
    "    irr_df = pd.DataFrame({'IRR': irr, 'Lower CI': irr_conf[0], 'Upper CI': irr_conf[1], \n",
    "                          'P-value': nb_results.pvalues})\n",
    "    print(irr_df)\n",
    "    \n",
    "    # Add IRR table to document\n",
    "    doc.add_paragraph('\\n')\n",
    "    doc.add_heading('Incident Rate Ratios (IRR)', level=2)\n",
    "    irr_table = doc.add_table(rows=len(irr_df)+1, cols=5)\n",
    "    irr_table.style = 'Table Grid'\n",
    "    irr_table.cell(0, 0).text = 'Variable'\n",
    "    irr_table.cell(0, 1).text = 'IRR'\n",
    "    irr_table.cell(0, 2).text = 'Lower CI'\n",
    "    irr_table.cell(0, 3).text = 'Upper CI'\n",
    "    irr_table.cell(0, 4).text = 'P-value'\n",
    "    \n",
    "    for i, (var, row) in enumerate(irr_df.iterrows(), 1):\n",
    "        irr_table.cell(i, 0).text = str(var)\n",
    "        irr_table.cell(i, 1).text = f\"{row['IRR']:.4f}\"\n",
    "        irr_table.cell(i, 2).text = f\"{row['Lower CI']:.4f}\"\n",
    "        irr_table.cell(i, 3).text = f\"{row['Upper CI']:.4f}\"\n",
    "        irr_table.cell(i, 4).text = f\"{row['P-value']:.4f}\"\n",
    "    \n",
    "    # Predictions and diagnostics\n",
    "    df_clean_nb['predicted'] = nb_results.predict()\n",
    "    df_clean_nb['residuals'] = df_clean_nb['los_capped'] - df_clean_nb['predicted']\n",
    "    \n",
    "    # Plot residuals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_clean_nb['predicted'], df_clean_nb['residuals'], alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot')\n",
    "    plt.tight_layout()\n",
    "    residuals_img = BytesIO()\n",
    "    plt.savefig(residuals_img, format='png')\n",
    "    residuals_img.seek(0)\n",
    "    plt.close()\n",
    "    \n",
    "    # Add residuals plot to document\n",
    "    doc.add_paragraph('\\n')\n",
    "    doc.add_heading('Diagnostics', level=2)\n",
    "    doc.add_picture(residuals_img, width=Inches(6))\n",
    "    doc.add_paragraph('Figure 3: Residuals Plot')\n",
    "    \n",
    "    # Check for heterogeneity across states if us_state is in the data\n",
    "    if 'us_state_enc' in df_clean_nb.columns or 'us_state' in df_clean_nb.columns:\n",
    "        state_var = 'us_state_enc' if 'us_state_enc' in df_clean_nb.columns else 'us_state'\n",
    "        state_means = df_clean_nb.groupby(state_var)['los_capped'].mean().sort_values()\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        state_means.plot(kind='bar')\n",
    "        plt.xlabel('State')\n",
    "        plt.ylabel('Average Length of Stay')\n",
    "        plt.title('Mean Length of Stay by State')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        los_by_state_img = BytesIO()\n",
    "        plt.savefig(los_by_state_img, format='png')\n",
    "        los_by_state_img.seek(0)\n",
    "        plt.close()\n",
    "        \n",
    "        # Add state analysis to document\n",
    "        doc.add_paragraph('\\n')\n",
    "        doc.add_heading('State Analysis', level=2)\n",
    "        doc.add_picture(los_by_state_img, width=Inches(6))\n",
    "        doc.add_paragraph('Figure 4: Mean Length of Stay by State')\n",
    "    \n",
    "    # Approximated Multilevel Model\n",
    "    doc.add_paragraph('\\n')\n",
    "    doc.add_heading('Approximated Multilevel Model', level=1)\n",
    "    doc.add_paragraph('Using MixedLM to approximate a multilevel model with random effects for states.')\n",
    "    \n",
    "    # Debugging: Check df_clean_nb and the condition\n",
    "    print(\"Columns in df_clean_nb:\", df_clean_nb.columns.tolist())\n",
    "    print(\"Checking if 'us_state_enc' is in df_clean_nb.columns:\", 'us_state_enc' in df_clean_nb.columns)\n",
    "\n",
    "# Update variable lists\n",
    "    continuous_vars = ['immigrant_population', 'import_from_slu', 'age', 'distance_miles', 'state_unemployment']\n",
    "    categorical_model_vars = ['sex_enc', 'marital_status_enc', 'employment_status_enc', 'purpose_simple', 'accomd_type_enc','us_state_enc', 'month_travel']\n",
    "\n",
    "    # Check if us_state variable exists for multilevel modeling\n",
    "    if 'us_state_enc' in df_clean_nb.columns:\n",
    "        # For demonstration, we'll use a linear mixed model as an approximation\n",
    "        # Prepare model variables\n",
    "        \n",
    "        # Ensure no missing values in variables used for mixed effects model\n",
    "        model_vars = ['los_capped'] + continuous_vars + categorical_model_vars \n",
    "        df_clean_model = df_clean_nb[model_vars].dropna()\n",
    "\n",
    "        df_clean_model = df_clean_model.reset_index(drop=True)\n",
    "        \n",
    "        y = df_clean_model['los_capped']\n",
    "        \n",
    "        # Create X matrix for fixed effects\n",
    "        X_vars = []\n",
    "        for var in continuous_vars:\n",
    "            if var in df_clean_model.columns:\n",
    "                X_vars.append(var)\n",
    "        \n",
    "        X = df_clean_model[X_vars].copy()\n",
    "        \n",
    "        # Add categorical variables (one-hot encoded)\n",
    "        for var in categorical_model_vars:\n",
    "            if var in df_clean_model.columns and var != 'us_state_enc':  # Exclude the grouping variable\n",
    "                dummies = pd.get_dummies(df_clean_model[var], prefix=var, drop_first=True)\n",
    "                X = pd.concat([X, dummies], axis=1)\n",
    "        \n",
    "        # Add intercept\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        X=X.astype(float)\n",
    "        print(\"X dtypes after converting to float:\")\n",
    "        print(X.dtypes)\n",
    "        \n",
    "        # Define groups for random effects\n",
    "        groups = df_clean_model['us_state_enc']\n",
    "\n",
    "        print(f\"Length of df_clean_model: {len(df_clean_model)}\")\n",
    "        print(f\"Length of y: {len(y)}\")\n",
    "        print(f\"Number of rows in X: {X.shape[0]}\")\n",
    "        print(f\"Length of groups: {len(groups)}\")\n",
    "\n",
    "        if len(y) != X.shape[0] or len(y) != len(groups):\n",
    "            print('Length mismatch between y, X, and groups. Check data preparation.')\n",
    "            print(f\"y length:{len(y)}\")\n",
    "            print(f\"X rows: {X.shape[0]}\")\n",
    "            print(f\"groups length: {len(groups)}\")\n",
    "            # Check for NaN values in X\n",
    "            print(\"NaN counts in X columns:\")\n",
    "            print(X.isnull().sum())\n",
    "            raise ValueError(\"Lengths of y, X, and groups do not match!\")\n",
    "\n",
    "\n",
    "       \n",
    "        \n",
    "        # Fit mixed effects model\n",
    "        mixed_model = MixedLM(y, X, groups)\n",
    "        try:\n",
    "            mixed_results = mixed_model.fit()\n",
    "            mixed_summary = str(mixed_results.summary())\n",
    "            print(\"\\nApproximated Multilevel Model Results:\")\n",
    "            print(mixed_summary)\n",
    "            \n",
    "            # Add to document\n",
    "            doc.add_paragraph('Model Summary:')\n",
    "            summary_paragraph = doc.add_paragraph()\n",
    "            summary_run = summary_paragraph.add_run(mixed_summary)\n",
    "            summary_run.font.name = 'Courier New'  # Use monospace font\n",
    "            #summary_run.font.size = Pt(10)  # Optional: Adjust font size\n",
    "            #for line in mixed_summary.split('\\n'):\n",
    "                #doc.add_paragraph(line)\n",
    "            \n",
    "            # Add variance components\n",
    "            doc.add_paragraph('\\nVariance Components:')\n",
    "            vc_table = doc.add_table(rows=3, cols=2)\n",
    "            vc_table.style = 'Table Grid'\n",
    "            vc_table.cell(0, 0).text = 'Component'\n",
    "            vc_table.cell(0, 1).text = 'Estimate'\n",
    "            vc_table.cell(1, 0).text = 'State Random Effect Variance'\n",
    "            vc_table.cell(1, 1).text = f\"{mixed_results.cov_re.iloc[0, 0]:.4f}\"\n",
    "            vc_table.cell(2, 0).text = 'Residual Variance'\n",
    "            vc_table.cell(2, 1).text = f\"{mixed_results.scale:.4f}\"\n",
    "            \n",
    "            # Calculate intraclass correlation coefficient (ICC)\n",
    "            state_var = mixed_results.cov_re.iloc[0, 0]\n",
    "            residual_var = mixed_results.scale\n",
    "            icc = state_var / (state_var + residual_var)\n",
    "\n",
    "            # Add model summary to document\n",
    "            #doc.add_paragraph('\\nMixed Summary:')\n",
    "            #for line in mixed_summary.split('\\n'):\n",
    "                #doc.add_paragraph(line)\n",
    "            \n",
    "            # Convert coefficients to incident rate ratios (IRR)\n",
    "            print(\"\\nIncident Rate Ratios (IRR) Mixed Model:\")\n",
    "            irr_mixed = np.exp(mixed_results.params)\n",
    "            irr_conf_mixed = np.exp(mixed_results.conf_int())\n",
    "            irr_df_mixed = pd.DataFrame({'IRR': irr_mixed, 'Lower CI': irr_conf_mixed[0], 'Upper CI': irr_conf_mixed[1], \n",
    "                                'P-value': mixed_results.pvalues})\n",
    "            print(irr_df_mixed)\n",
    "            \n",
    "            # Add IRR table to document\n",
    "            doc.add_paragraph('\\n')\n",
    "            doc.add_heading('Incident Rate Ratios (IRR) Mixed Model', level=2)\n",
    "            irr_table_mixed = doc.add_table(rows=len(irr_df_mixed)+1, cols=5)\n",
    "            irr_table_mixed.style = 'Table Grid'\n",
    "            irr_table_mixed.cell(0, 0).text = 'Variable'\n",
    "            irr_table_mixed.cell(0, 1).text = 'IRR'\n",
    "            irr_table_mixed.cell(0, 2).text = 'Lower CI'\n",
    "            irr_table_mixed.cell(0, 3).text = 'Upper CI'\n",
    "            irr_table_mixed.cell(0, 4).text = 'P-value'\n",
    "            \n",
    "            for i, (var, row) in enumerate(irr_df_mixed.iterrows(), 1):\n",
    "                irr_table_mixed.cell(i, 0).text = str(var)\n",
    "                irr_table_mixed.cell(i, 1).text = f\"{row['IRR']:.4f}\"\n",
    "                irr_table_mixed.cell(i, 2).text = f\"{row['Lower CI']:.4f}\"\n",
    "                irr_table_mixed.cell(i, 3).text = f\"{row['Upper CI']:.4f}\"\n",
    "                irr_table_mixed.cell(i, 4).text = f\"{row['P-value']:.4f}\"\n",
    "            \n",
    "            doc.add_paragraph(f'\\nIntraclass Correlation Coefficient (ICC): {icc:.4f}')\n",
    "            doc.add_paragraph('The ICC represents the proportion of the total variance in length of stay ' +\n",
    "                             'that is attributable to differences between states.')\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error fitting mixed model: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            doc.add_paragraph(error_msg)\n",
    "            doc.add_paragraph(\"The mixed effects model failed to converge. This can happen due to \" +\n",
    "                             \"insufficient variation in the grouping variable or other model specification issues.\")\n",
    "    else:\n",
    "        no_state_msg = \"State variable not found for multilevel modeling.\"\n",
    "        print(no_state_msg)\n",
    "        doc.add_paragraph(no_state_msg)\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = f\"\\nError in model fitting: {str(e)}\"\n",
    "    print(error_msg)\n",
    "    doc.add_paragraph(error_msg)\n",
    "    doc.add_paragraph(\"You may need to check your data or consider using a different modeling approach.\")\n",
    "\n",
    "# Save the Word document\n",
    "print(\"\\nPlease select where to save the Word document...\")\n",
    "doc_path = select_file(\n",
    "    \"Save Analysis Report As\", \n",
    "    [(\"Word Document\", \"*.docx\"), (\"All files\", \"*.*\")],\n",
    "    save=True\n",
    ")\n",
    "\n",
    "if doc_path:\n",
    "    if not doc_path.endswith('.docx'):\n",
    "        doc_path += '.docx'\n",
    "    doc.save(doc_path)\n",
    "    print(f\"Analysis report saved to: {doc_path}\")\n",
    "else:\n",
    "    print(\"Document not saved as no location was selected.\")\n",
    "\n",
    "print(\"\\nAnalysis complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(continuous_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136252\n"
     ]
    }
   ],
   "source": [
    "print(len(df_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(purpose_enc_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los_capped                float64\n",
      "immigrant_population      float64\n",
      "import_from_slu           float64\n",
      "age                       float64\n",
      "distance_miles            float64\n",
      "state_percapita_income    float64\n",
      "state_unemployment        float64\n",
      "sex_enc                      int8\n",
      "marital_status_enc           int8\n",
      "employment_status_enc        int8\n",
      "purpose_simple            float64\n",
      "accomd_type_enc              int8\n",
      "month_travel                int64\n",
      "us_state_enc                 int8\n",
      "predicted                 float64\n",
      "residuals                 float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_clean_nb.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dtypes:\n",
      "const                      float64\n",
      "immigrant_population       float64\n",
      "import_from_slu            float64\n",
      "age                        float64\n",
      "distance_miles             float64\n",
      "state_unemployment         float64\n",
      "sex_enc_0                  float64\n",
      "sex_enc_1                  float64\n",
      "marital_status_enc_0       float64\n",
      "marital_status_enc_1       float64\n",
      "marital_status_enc_2       float64\n",
      "employment_status_enc_0    float64\n",
      "employment_status_enc_1    float64\n",
      "employment_status_enc_2    float64\n",
      "purpose_simple_2.0         float64\n",
      "purpose_simple_3.0         float64\n",
      "purpose_simple_4.0         float64\n",
      "purpose_simple_5.0         float64\n",
      "accomd_type_enc_1          float64\n",
      "accomd_type_enc_2          float64\n",
      "dtype: object\n",
      "X shape: (132666, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"X dtypes:\")\n",
    "print(X.dtypes)\n",
    "print(\"X shape:\", X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y dtypes:\n",
      "float64\n",
      "y shape: (132666,)\n"
     ]
    }
   ],
   "source": [
    "print(\"y dtypes:\")\n",
    "print(y.dtypes)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statsmodels version: 0.14.4\n",
      "pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "import statsmodels\n",
    "import pandas\n",
    "print(\"statsmodels version:\", statsmodels.__version__)\n",
    "print(\"pandas version:\", pandas.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const                      float64\n",
      "immigrant_population       float64\n",
      "import_from_slu            float64\n",
      "age                        float64\n",
      "distance_miles             float64\n",
      "state_unemployment         float64\n",
      "sex_enc_0                  float64\n",
      "sex_enc_1                  float64\n",
      "marital_status_enc_0       float64\n",
      "marital_status_enc_1       float64\n",
      "marital_status_enc_2       float64\n",
      "employment_status_enc_0    float64\n",
      "employment_status_enc_1    float64\n",
      "employment_status_enc_2    float64\n",
      "purpose_simple_2.0         float64\n",
      "purpose_simple_3.0         float64\n",
      "purpose_simple_4.0         float64\n",
      "purpose_simple_5.0         float64\n",
      "accomd_type_enc_1          float64\n",
      "accomd_type_enc_2          float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF for continuous and dummy variables:\n",
      "                   Variable          VIF\n",
      "0      immigrant_population     1.723484\n",
      "1           import_from_slu     1.473680\n",
      "2                       age    14.068868\n",
      "3            distance_miles    25.902065\n",
      "4        state_unemployment    44.121726\n",
      "5                 sex_enc_0  2179.234036\n",
      "6                 sex_enc_1  1772.683770\n",
      "7      marital_status_enc_0  2939.177076\n",
      "8      marital_status_enc_1    86.025036\n",
      "9      marital_status_enc_2  1148.248160\n",
      "10  employment_status_enc_0   344.018936\n",
      "11  employment_status_enc_1     3.507670\n",
      "12  employment_status_enc_2     2.399140\n",
      "13       purpose_simple_2.0    24.286657\n",
      "14       purpose_simple_3.0     1.006259\n",
      "15       purpose_simple_4.0     2.107539\n",
      "16       purpose_simple_5.0   111.615834\n",
      "17        accomd_type_enc_1     1.045595\n",
      "18        accomd_type_enc_2     1.197235\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Check VIF for continuous variables\n",
    "X_continuous = X[[col for col in X.columns if col != 'const']]  # Exclude intercept\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X_continuous.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_continuous.values, i) for i in range(X_continuous.shape[1])]\n",
    "print(\"VIF for continuous and dummy variables:\")\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of X: 198800924.27223864\n"
     ]
    }
   ],
   "source": [
    "# Check condition number of X\n",
    "condition_number = np.linalg.cond(X.values)\n",
    "print(\"Condition number of X:\", condition_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False]\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_clean_model.columns.duplicated())\n",
    "print(df_clean_model.columns[df_clean_model.columns.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>immigrant_population</th>\n",
       "      <th>import_from_slu</th>\n",
       "      <th>age</th>\n",
       "      <th>distance_miles</th>\n",
       "      <th>state_unemployment</th>\n",
       "      <th>sex_enc_0</th>\n",
       "      <th>sex_enc_1</th>\n",
       "      <th>marital_status_enc_0</th>\n",
       "      <th>marital_status_enc_1</th>\n",
       "      <th>marital_status_enc_2</th>\n",
       "      <th>employment_status_enc_0</th>\n",
       "      <th>employment_status_enc_1</th>\n",
       "      <th>employment_status_enc_2</th>\n",
       "      <th>purpose_simple_2.0</th>\n",
       "      <th>purpose_simple_3.0</th>\n",
       "      <th>purpose_simple_4.0</th>\n",
       "      <th>purpose_simple_5.0</th>\n",
       "      <th>accomd_type_enc_1</th>\n",
       "      <th>accomd_type_enc_2</th>\n",
       "      <th>month_travel_2</th>\n",
       "      <th>month_travel_3</th>\n",
       "      <th>month_travel_4</th>\n",
       "      <th>month_travel_5</th>\n",
       "      <th>month_travel_6</th>\n",
       "      <th>month_travel_7</th>\n",
       "      <th>month_travel_8</th>\n",
       "      <th>month_travel_9</th>\n",
       "      <th>month_travel_10</th>\n",
       "      <th>month_travel_11</th>\n",
       "      <th>month_travel_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>132666.0</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>1.326660e+05</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.00000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "      <td>132666.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2105.071345</td>\n",
       "      <td>3.631182e+05</td>\n",
       "      <td>44.519568</td>\n",
       "      <td>2285.069030</td>\n",
       "      <td>3.986560</td>\n",
       "      <td>0.551453</td>\n",
       "      <td>0.448382</td>\n",
       "      <td>0.704174</td>\n",
       "      <td>0.020382</td>\n",
       "      <td>0.275353</td>\n",
       "      <td>0.985897</td>\n",
       "      <td>0.007214</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.168717</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.008148</td>\n",
       "      <td>0.815755</td>\n",
       "      <td>0.035058</td>\n",
       "      <td>0.123144</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>0.09640</td>\n",
       "      <td>0.080405</td>\n",
       "      <td>0.096701</td>\n",
       "      <td>0.103953</td>\n",
       "      <td>0.101541</td>\n",
       "      <td>0.077646</td>\n",
       "      <td>0.052493</td>\n",
       "      <td>0.071827</td>\n",
       "      <td>0.077013</td>\n",
       "      <td>0.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3693.203160</td>\n",
       "      <td>8.456676e+05</td>\n",
       "      <td>14.529632</td>\n",
       "      <td>515.740383</td>\n",
       "      <td>0.683964</td>\n",
       "      <td>0.497347</td>\n",
       "      <td>0.497330</td>\n",
       "      <td>0.456415</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.446694</td>\n",
       "      <td>0.117916</td>\n",
       "      <td>0.084626</td>\n",
       "      <td>0.063316</td>\n",
       "      <td>0.374503</td>\n",
       "      <td>0.006725</td>\n",
       "      <td>0.089900</td>\n",
       "      <td>0.387685</td>\n",
       "      <td>0.183927</td>\n",
       "      <td>0.328603</td>\n",
       "      <td>0.283114</td>\n",
       "      <td>0.29514</td>\n",
       "      <td>0.271920</td>\n",
       "      <td>0.295552</td>\n",
       "      <td>0.305200</td>\n",
       "      <td>0.302045</td>\n",
       "      <td>0.267615</td>\n",
       "      <td>0.223019</td>\n",
       "      <td>0.258202</td>\n",
       "      <td>0.266613</td>\n",
       "      <td>0.277389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>18.001369</td>\n",
       "      <td>1856.680000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>2.123000e+03</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1988.450000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>5.948200e+04</td>\n",
       "      <td>43.112936</td>\n",
       "      <td>2114.750000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1598.000000</td>\n",
       "      <td>1.714190e+05</td>\n",
       "      <td>55.498289</td>\n",
       "      <td>2484.190000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11707.000000</td>\n",
       "      <td>3.413006e+06</td>\n",
       "      <td>253.103354</td>\n",
       "      <td>6306.840000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          const  immigrant_population  import_from_slu            age  \\\n",
       "count  132666.0         132666.000000     1.326660e+05  132666.000000   \n",
       "mean        1.0           2105.071345     3.631182e+05      44.519568   \n",
       "std         0.0           3693.203160     8.456676e+05      14.529632   \n",
       "min         1.0              0.000000     0.000000e+00      18.001369   \n",
       "25%         1.0             53.000000     2.123000e+03      32.000000   \n",
       "50%         1.0            400.000000     5.948200e+04      43.112936   \n",
       "75%         1.0           1598.000000     1.714190e+05      55.498289   \n",
       "max         1.0          11707.000000     3.413006e+06     253.103354   \n",
       "\n",
       "       distance_miles  state_unemployment      sex_enc_0      sex_enc_1  \\\n",
       "count   132666.000000       132666.000000  132666.000000  132666.000000   \n",
       "mean      2285.069030            3.986560       0.551453       0.448382   \n",
       "std        515.740383            0.683964       0.497347       0.497330   \n",
       "min       1856.680000            1.900000       0.000000       0.000000   \n",
       "25%       1988.450000            3.400000       0.000000       0.000000   \n",
       "50%       2114.750000            4.100000       1.000000       0.000000   \n",
       "75%       2484.190000            4.400000       1.000000       1.000000   \n",
       "max       6306.840000            5.700000       1.000000       1.000000   \n",
       "\n",
       "       marital_status_enc_0  marital_status_enc_1  marital_status_enc_2  \\\n",
       "count         132666.000000         132666.000000         132666.000000   \n",
       "mean               0.704174              0.020382              0.275353   \n",
       "std                0.456415              0.141304              0.446694   \n",
       "min                0.000000              0.000000              0.000000   \n",
       "25%                0.000000              0.000000              0.000000   \n",
       "50%                1.000000              0.000000              0.000000   \n",
       "75%                1.000000              0.000000              1.000000   \n",
       "max                1.000000              1.000000              1.000000   \n",
       "\n",
       "       employment_status_enc_0  employment_status_enc_1  \\\n",
       "count            132666.000000            132666.000000   \n",
       "mean                  0.985897                 0.007214   \n",
       "std                   0.117916                 0.084626   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   1.000000                 0.000000   \n",
       "50%                   1.000000                 0.000000   \n",
       "75%                   1.000000                 0.000000   \n",
       "max                   1.000000                 1.000000   \n",
       "\n",
       "       employment_status_enc_2  purpose_simple_2.0  purpose_simple_3.0  \\\n",
       "count            132666.000000       132666.000000       132666.000000   \n",
       "mean                  0.004025            0.168717            0.000045   \n",
       "std                   0.063316            0.374503            0.006725   \n",
       "min                   0.000000            0.000000            0.000000   \n",
       "25%                   0.000000            0.000000            0.000000   \n",
       "50%                   0.000000            0.000000            0.000000   \n",
       "75%                   0.000000            0.000000            0.000000   \n",
       "max                   1.000000            1.000000            1.000000   \n",
       "\n",
       "       purpose_simple_4.0  purpose_simple_5.0  accomd_type_enc_1  \\\n",
       "count       132666.000000       132666.000000      132666.000000   \n",
       "mean             0.008148            0.815755           0.035058   \n",
       "std              0.089900            0.387685           0.183927   \n",
       "min              0.000000            0.000000           0.000000   \n",
       "25%              0.000000            1.000000           0.000000   \n",
       "50%              0.000000            1.000000           0.000000   \n",
       "75%              0.000000            1.000000           0.000000   \n",
       "max              1.000000            1.000000           1.000000   \n",
       "\n",
       "       accomd_type_enc_2  month_travel_2  month_travel_3  month_travel_4  \\\n",
       "count      132666.000000   132666.000000    132666.00000   132666.000000   \n",
       "mean            0.123144        0.087875         0.09640        0.080405   \n",
       "std             0.328603        0.283114         0.29514        0.271920   \n",
       "min             0.000000        0.000000         0.00000        0.000000   \n",
       "25%             0.000000        0.000000         0.00000        0.000000   \n",
       "50%             0.000000        0.000000         0.00000        0.000000   \n",
       "75%             0.000000        0.000000         0.00000        0.000000   \n",
       "max             1.000000        1.000000         1.00000        1.000000   \n",
       "\n",
       "       month_travel_5  month_travel_6  month_travel_7  month_travel_8  \\\n",
       "count   132666.000000   132666.000000   132666.000000   132666.000000   \n",
       "mean         0.096701        0.103953        0.101541        0.077646   \n",
       "std          0.295552        0.305200        0.302045        0.267615   \n",
       "min          0.000000        0.000000        0.000000        0.000000   \n",
       "25%          0.000000        0.000000        0.000000        0.000000   \n",
       "50%          0.000000        0.000000        0.000000        0.000000   \n",
       "75%          0.000000        0.000000        0.000000        0.000000   \n",
       "max          1.000000        1.000000        1.000000        1.000000   \n",
       "\n",
       "       month_travel_9  month_travel_10  month_travel_11  month_travel_12  \n",
       "count   132666.000000    132666.000000    132666.000000    132666.000000  \n",
       "mean         0.052493         0.071827         0.077013         0.084000  \n",
       "std          0.223019         0.258202         0.266613         0.277389  \n",
       "min          0.000000         0.000000         0.000000         0.000000  \n",
       "25%          0.000000         0.000000         0.000000         0.000000  \n",
       "50%          0.000000         0.000000         0.000000         0.000000  \n",
       "75%          0.000000         0.000000         0.000000         0.000000  \n",
       "max          1.000000         1.000000         1.000000         1.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_clean_model: (132666, 11)\n",
      "Columns in df_clean_model: ['los_capped', 'immigrant_population', 'import_from_slu', 'age', 'distance_miles', 'state_unemployment', 'sex_enc', 'marital_status_enc', 'employment_status_enc', 'purpose_simple', 'accomd_type_enc']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "us_state_enc column is missing in df_clean_model!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mColumns in df_clean_model:\u001b[39m\u001b[33m\"\u001b[39m, df_clean_model.columns.tolist())\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mus_state_enc\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df_clean_model.columns:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mus_state_enc column is missing in df_clean_model!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df_clean_model.empty:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdf_clean_model is empty!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: us_state_enc column is missing in df_clean_model!"
     ]
    }
   ],
   "source": [
    "# Debugging before groupby\n",
    "print(\"Shape of df_clean_model:\", df_clean_model.shape)\n",
    "print(\"Columns in df_clean_model:\", df_clean_model.columns.tolist())\n",
    "if 'us_state_enc' not in df_clean_model.columns:\n",
    "    raise ValueError(\"us_state_enc column is missing in df_clean_model!\")\n",
    "if df_clean_model.empty:\n",
    "    raise ValueError(\"df_clean_model is empty!\")\n",
    "# Number of observations per group\n",
    "group_counts = df_clean_model.groupby(by='us_state_enc').size()\n",
    "print(\"Number of observations per group:\")\n",
    "print(group_counts)\n",
    "\n",
    "# Variation in y within groups\n",
    "group_means = df_clean_model.groupby(by='us_state_enc')['los_capped'].mean()\n",
    "group_stds = df_clean_model.groupby(by='us_state_enc')['los_capped'].std()\n",
    "print(\"\\nMean and standard deviation of los_capped per group:\")\n",
    "print(pd.DataFrame({'Mean': group_means, 'Std': group_stds}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jlslu2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
