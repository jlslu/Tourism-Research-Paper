{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Assessment of the length of stay in Saint Lucia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code chunk loads the data and performs the following cleaning techniques:\n",
    "    removes instances where there are blank values which are codied as -1\n",
    "    removes ages more than 100 years\n",
    "    defines tourist as between 1-365 nights\n",
    "    streamlines purpose of visit\n",
    "    logs variables (exepect length of stay and age)\n",
    "    makes a list of continous and categorical variables\n",
    "    converts month of stay to a seasonal variable\n",
    "    creates a tourist type variable\n",
    "\n",
    "Notes:\n",
    "    The tourist type variable is defined as St Lucian born if someone was born in Saint Lucia but a resident of the US and US born if they were both born and a resident of the US\n",
    "    all persons in the file are over the age of 18\n",
    "    Immigrant density is defined as st lucian immigrants per 100,000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.genmod.families.family import NegativeBinomial\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from scipy import stats\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from io import BytesIO\n",
    "import statsmodels.discrete.discrete_model as discrete\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "\n",
    "def select_file(title, file_types, save=False):\n",
    "    \"\"\"Allow user to select a file\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes('-topmost', True)\n",
    "    \n",
    "    try:\n",
    "        if save:\n",
    "            file_path = filedialog.asksaveasfilename(\n",
    "                title=title,\n",
    "                filetypes=file_types,\n",
    "                defaultextension=file_types[0][1]\n",
    "            )\n",
    "        else:\n",
    "            file_path = filedialog.askopenfilename(\n",
    "                title=title,\n",
    "                filetypes=file_types\n",
    "            )\n",
    "    finally:\n",
    "        root.destroy()\n",
    "    \n",
    "    return file_path if file_path else None\n",
    "\n",
    "# Allow user to select input file\n",
    "print(\"Please select the input Excel file...\")\n",
    "file_path = select_file(\n",
    "    \"Select Excel Data File\", \n",
    "    [(\"Excel files\", \"*.xlsx *.xls\"), (\"All files\", \"*.*\")]\n",
    ")\n",
    "\n",
    "if not file_path:\n",
    "    print(\"No file selected. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Import the data\n",
    "print(f\"Loading data from: {file_path}\")\n",
    "df = pd.read_excel(file_path, sheet_name=\"Sheet\")\n",
    "\n",
    "# Setup\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Remove instances where tourist type is \"SLU born tourist\"\n",
    "if 'tourist_type' in df.columns:\n",
    "    df = df[df['tourist_type'] != 'SLU born tourist']\n",
    "    \n",
    "\n",
    "# Encode categorical variables if not already encoded\n",
    "categorical_vars = ['sex', 'marital_status', 'employment_status', 'purpose', 'accomd_type', 'us_state','tourist_type','region_census', 'region_climate']\n",
    "encoded_vars = {}\n",
    "\n",
    "for var in categorical_vars:\n",
    "    if var in df.columns:\n",
    "        # Check if variable is already numeric\n",
    "        if not pd.api.types.is_numeric_dtype(df[var]):\n",
    "            new_var = f\"{var}_enc\"\n",
    "            df[new_var] = pd.Categorical(df[var]).codes\n",
    "            encoded_vars[var] = new_var\n",
    "        else:\n",
    "            encoded_vars[var] = var\n",
    "\n",
    "# Set the truncation point for los (assuming truncation at 0)\n",
    "df['los_trunc'] = df['los'].copy()\n",
    "df.loc[df['los_trunc'] <= 0, 'los_trunc'] = np.nan\n",
    "\n",
    "# Check for missing data\n",
    "print(\"\\nMissing data summary:\")\n",
    "missing_data_summary = df.isnull().sum()\n",
    "print(missing_data_summary)\n",
    "\n",
    "print(\"\\nMissing data patterns:\")\n",
    "missing_patterns = df.isnull().sum(axis=1)\n",
    "missing_patterns_counts = missing_patterns.value_counts().sort_index()\n",
    "print(missing_patterns_counts)\n",
    "\n",
    "# Visualize los distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['los_trunc'], discrete=True)\n",
    "plt.title('Histogram of Length of Stay')\n",
    "plt.tight_layout()\n",
    "los_hist_img = BytesIO()\n",
    "plt.savefig(los_hist_img, format='png')\n",
    "los_hist_img.seek(0)\n",
    "plt.close()\n",
    "\n",
    "# Summarize los by purpose\n",
    "purpose_stats = None\n",
    "if 'purpose_enc' in df.columns:\n",
    "    print(\"\\nLength of stay by purpose:\")\n",
    "    purpose_stats = df.groupby('purpose_enc')['los_trunc'].agg(['count', 'mean', 'median', 'min', 'max', 'std'])\n",
    "    print(purpose_stats)\n",
    "\n",
    "# Detailed summary of los_trunc\n",
    "print(\"\\nDetailed summary of length of stay:\")\n",
    "los_describe = df['los_trunc'].describe(percentiles=[.25, .5, .75, .90, .95, .99])\n",
    "print(los_describe)\n",
    "\n",
    "# Based on month travel create a new variable called seasons i.e. winter, spring, summer, fall\n",
    "# Define seasons based on month_travel if it is a number\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df['season'] = df['month_travel'].apply(lambda x: get_season(x) if pd.notnull(x) and isinstance(x, (int, float)) else np.nan)\n",
    "\n",
    "#encoding the season variable\n",
    "if 'season' in df.columns:\n",
    "    df['season_enc'] = pd.Categorical(df['season']).codes\n",
    "\n",
    "# Cleaning process\n",
    "# Step 1: Drop missing datapoints for key variables\n",
    "key_vars = ['los', 'immigrant_population', 'import_from_slu', 'age', 'climate_distance','economic_distance',\n",
    "            encoded_vars.get('sex', 'sex_enc'), \n",
    "            encoded_vars.get('marital_status', 'marital_status_enc'), \n",
    "            encoded_vars.get('employment_status', 'employment_status_enc'), \n",
    "            'distance_miles', \n",
    "            encoded_vars.get('purpose', 'purpose_enc'), \n",
    "            encoded_vars.get('accomd_type', 'accomd_type_enc'), \n",
    "            encoded_vars.get('tourist_type', 'tourist_type_enc'),\n",
    "            encoded_vars.get('region_census', 'region_census_enc'),\n",
    "            encoded_vars.get('region_climate', 'region_climate_enc'),\n",
    "            'state_percapita_income', 'state_unemployment','immigrant_density', 'season_enc']\n",
    "           \n",
    "             \n",
    "\n",
    "\n",
    "\n",
    "#Now that we have the encoded variables, we need to remove blank and -1 values\n",
    "# Remove rows where sex_enc is -1\n",
    "df = df[df['sex_enc'] != -1]\n",
    "# Remove rows where age > 100\n",
    "df = df[df['age'] <= 100]  \n",
    "# Remove rows where marital_status_enc is -1\n",
    "df= df[df['marital_status_enc'] != -1]\n",
    "\n",
    "#Remove rows where employment_status_enc is -1\n",
    "df = df[df['employment_status_enc'] != -1]\n",
    "\n",
    "# Count missing values per row for key variables\n",
    "df['missing'] = df[key_vars].isnull().sum(axis=1)\n",
    "print(\"\\nNumber of missing values per observation:\")\n",
    "missing_values_count = df['missing'].value_counts().sort_index()\n",
    "print(missing_values_count)\n",
    "\n",
    "# Drop observations with missing values in key variables\n",
    "df_clean = df[df['missing'] == 0].drop('missing', axis=1)\n",
    "print(f\"\\nRemaining observations after dropping missing values: {len(df_clean)}\")\n",
    "\n",
    "# Step 2: Drop outliers in length of stay\n",
    "#02 June 2025 edit removed the windorization of los_trunc\n",
    "#los_p95 = np.percentile(df_clean['los_trunc'].dropna(), 95)\n",
    "df_clean['los_capped'] = df_clean['los_trunc'].copy()\n",
    "#df_clean.loc[df_clean['los_capped'] > los_p95, 'los_capped'] = los_p95\n",
    "\n",
    "#df_clean = df_clean[df_clean['los_trunc'] <= los_p95]\n",
    "print(f\"After filtering to 95th percentile, remaining observations: {len(df_clean)}\")\n",
    "\n",
    "# Remove any instance of los_capped that is less than 2. Prior to this, over 1000 persons had stays less than 1 including honeymooners. This appears to be a data entry error.\n",
    "#02 June 2025 edit use the tourism definition of length of stay where its more than 1 day and less than 366 days\n",
    "\n",
    "df_clean = df_clean[(df_clean['los_capped'] > 1) & (df_clean['los_capped'] < 366)]  # Keep stays more than 1 and less than 366 days\n",
    "\n",
    "#remove instances where sex_enc is -1\n",
    "\n",
    "#30 June 2025\n",
    "#remove  if us_state enc is 45 or 19 (US virgin islands or Mon repos)\n",
    "\n",
    "# Visualize the capped los distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_clean['los_capped'], discrete=True)\n",
    "plt.title('Histogram of Capped Length of Stay')\n",
    "plt.tight_layout()\n",
    "los_capped_img = BytesIO()\n",
    "plt.savefig(los_capped_img, format='png')\n",
    "los_capped_img.seek(0)\n",
    "plt.close()\n",
    "\n",
    "# Step 3: Clean up the purpose of trip column\n",
    "# Create a new simplified purpose variable\n",
    "purpose_mapping = {\n",
    "    0: 1,  # BUSINESS/MEETING -> Business\n",
    "    1: 1,  # CONVENTION -> Business\n",
    "    2: 1,  # CREW -> Business\n",
    "    3: 5,  # CRICKET -> Other\n",
    "    4: 2,  # EVENT -> Events\n",
    "    5: 2,  # EVENTS -> Events\n",
    "    6: 4,  # HONEYMOON -> Pleasure\n",
    "    7: 5,  # INTRANSIT PASSENGER -> Other\n",
    "    8: 5,  # OTHER -> Other\n",
    "    9: 4,  # PLEASURE/HOLIDAY/VACATION -> Pleasure\n",
    "    10: 5, # RESIDENT -> Other\n",
    "    11: 2, # SAINT LUCIA CARNIVAL -> Events\n",
    "    12: 2, # SAINT LUCIA JAZZ AND ARTS FESTIVAL -> Events\n",
    "    13: 5, # SPORTS -> Other\n",
    "    14: 5, # STUDY -> Other\n",
    "    15: 5, # VISITING FRIENDS/RELATIVES -> Other\n",
    "    16: 3, # WEDDING -> Wedding\n",
    "    -1: 5, # Missing/Unknown -> Other\n",
    "}\n",
    "\n",
    "purpose_labels = {\n",
    "    1: \"Business\",\n",
    "    2: \"Events\", \n",
    "    3: \"Wedding\",\n",
    "    4: \"Pleasure\",\n",
    "    5: \"Other\"\n",
    "}\n",
    "\n",
    "# Add the simplified purpose variable\n",
    "purpose_enc_col = encoded_vars.get('purpose', 'purpose_enc')\n",
    "df_clean['purpose_simple'] = df_clean[purpose_enc_col].map(purpose_mapping)\n",
    "\n",
    "# Check the new variable\n",
    "print(\"\\nPurpose simple distribution:\")\n",
    "purpose_counts = df_clean['purpose_simple'].value_counts().sort_index()\n",
    "purpose_distribution = []\n",
    "for code, count in purpose_counts.items():\n",
    "    purpose_line = f\"{code} ({purpose_labels.get(code, 'Unknown')}): {count}\"\n",
    "    purpose_distribution.append(purpose_line)\n",
    "    print(purpose_line)\n",
    "\n",
    "# Create a Word document for output\n",
    "doc = Document()\n",
    "doc.add_heading('Multilevel Truncated Negative Binomial Regression for Length of Stay Analysis', 0)\n",
    "doc.add_heading('Data Preparation and Cleaning', level=1)\n",
    "\n",
    "\n",
    "\n",
    "# Add Length of Stay histogram\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Length of Stay Distribution', level=2)\n",
    "doc.add_picture(los_hist_img, width=Inches(6))\n",
    "doc.add_paragraph('Figure 1: Histogram of Length of Stay (Before Capping)')\n",
    "\n",
    "# Add Capped LOS histogram\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Capped Length of Stay Distribution', level=2)\n",
    "doc.add_picture(los_capped_img, width=Inches(6))\n",
    "doc.add_paragraph('Figure 2: Histogram of Length of Stay (After Capping at 95th Percentile)')\n",
    "\n",
    "# Add LOS summary statistics\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Length of Stay Summary Statistics', level=2)\n",
    "los_stats_table = doc.add_table(rows=len(los_describe)+1, cols=2)\n",
    "los_stats_table.style = 'Table Grid'\n",
    "los_stats_table.cell(0, 0).text = 'Statistic'\n",
    "los_stats_table.cell(0, 1).text = 'Value'\n",
    "for i, (stat, value) in enumerate(los_describe.items(), 1):\n",
    "    los_stats_table.cell(i, 0).text = str(stat)\n",
    "    los_stats_table.cell(i, 1).text = f\"{value:.4f}\" if isinstance(value, (int, float)) else str(value)\n",
    "\n",
    "# Add Purpose distribution\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Purpose of Visit Distribution', level=2)\n",
    "purpose_table = doc.add_table(rows=len(purpose_distribution)+1, cols=1)\n",
    "purpose_table.style = 'Table Grid'\n",
    "purpose_table.cell(0, 0).text = 'Purpose Category'\n",
    "for i, purpose_text in enumerate(purpose_distribution, 1):\n",
    "    purpose_table.cell(i, 0).text = purpose_text\n",
    "\n",
    "# Log continuous variables\n",
    "# Convert columns to numeric before log transformation\n",
    "for col in ['immigrant_population', 'import_from_slu', 'distance_miles', 'state_percapita_income']:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "#df_clean['age'] = np.log1p(df_clean['age'])\n",
    "df_clean['immigrant_population_log'] = np.log1p(df_clean['immigrant_population'])\n",
    "df_clean['distance_miles_log'] = np.log1p(df_clean['distance_miles'])\n",
    "df_clean['state_percapita_income_log'] = np.log1p(df_clean['state_percapita_income'])\n",
    "df_clean['import_from_slu_log'] = np.log1p(df_clean['import_from_slu'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting simple negative binomial regression model...\n",
      "Model formula: los_capped ~ immigrant_population_log + import_from_slu_log + age + state_percapita_income_log + state_unemployment + immigrant_density + climate_distance + economic_distance + C(sex_enc) + C(marital_status_enc) + C(employment_status_enc) + C(purpose_simple) + C(accomd_type_enc) + C(season_enc) + C(us_state_enc) + C(region_census_enc)\n",
      "Number of rows in df_clean_nb after dropping missing values: 139278\n",
      "Categorical Variables After Labeling:\n",
      "==================================================\n",
      "\n",
      "sex_enc:\n",
      "  Female: 76562\n",
      "  Male: 62716\n",
      "\n",
      "marital_status_enc:\n",
      "  Married: 97765\n",
      "  Single: 38573\n",
      "  Other: 2940\n",
      "\n",
      "employment_status_enc:\n",
      "  Employed: 137720\n",
      "  Unemployed: 1009\n",
      "  Student: 549\n",
      "\n",
      "purpose_simple:\n",
      "  Pleasure: 122211\n",
      "  Wedding: 6533\n",
      "  Other: 6028\n",
      "  Business: 2472\n",
      "  Events: 2034\n",
      "\n",
      "accomd_type_enc:\n",
      "  Hotel: 115209\n",
      "  Sharing: 18999\n",
      "  Other: 5070\n",
      "\n",
      "season_enc:\n",
      "  Summer: 39501\n",
      "  Spring: 37803\n",
      "  Winter: 34187\n",
      "  Unknown: 27787\n",
      "\n",
      "region_census_enc:\n",
      "  South: 51491\n",
      "  Midwest: 51110\n",
      "  Northeast: 24760\n",
      "  West: 11917\n",
      "\n",
      "us_state_enc:\n",
      "  New_York: 16608\n",
      "  Florida: 9813\n",
      "  Texas: 8662\n",
      "  Georgia: 8271\n",
      "  North_Carolina: 7765\n",
      "  Pennsylvania: 6748\n",
      "  New_Jersey: 6624\n",
      "  Massachusetts: 5527\n",
      "  Virginia: 4953\n",
      "  Ohio: 4681\n",
      "  California: 4526\n",
      "  Illinois: 4349\n",
      "  Maryland: 3946\n",
      "  South_Carolina: 3793\n",
      "  Michigan: 3683\n",
      "  Tennessee: 3493\n",
      "  Connecticut: 3085\n",
      "  Alabama: 2576\n",
      "  Indiana: 2437\n",
      "  Missouri: 2293\n",
      "  Wisconsin: 2284\n",
      "  Colorado: 2253\n",
      "  Kentucky: 1774\n",
      "  Louisiana: 1673\n",
      "  Arizona: 1669\n",
      "  Minnesota: 1462\n",
      "  Mississippi: 1336\n",
      "  New_Hampshire: 1232\n",
      "  Kansas: 1136\n",
      "  Oklahoma: 1101\n",
      "  Nebraska: 1018\n",
      "  Iowa: 982\n",
      "  Washington: 920\n",
      "  Arkansas: 918\n",
      "  Maine: 745\n",
      "  Delaware: 708\n",
      "  Rhode_Island: 635\n",
      "  Utah: 567\n",
      "  Oregon: 509\n",
      "  Nevada: 495\n",
      "  West_Virginia: 316\n",
      "  Vermont: 299\n",
      "  Idaho: 285\n",
      "  South_Dakota: 276\n",
      "  Montana: 249\n",
      "  New_Mexico: 198\n",
      "  Wyoming: 190\n",
      "  North_Dakota: 159\n",
      "  Hawaii: 35\n",
      "  Alaska: 21\n",
      "\n",
      "Original formula (now with labels): los_capped ~ immigrant_population_log + import_from_slu_log + age + state_percapita_income_log + state_unemployment + immigrant_density + climate_distance + economic_distance + C(sex_enc) + C(marital_status_enc) + C(employment_status_enc) + C(purpose_simple) + C(accomd_type_enc) + C(season_enc) + C(us_state_enc) + C(region_census_enc)\n"
     ]
    }
   ],
   "source": [
    "# Add labels for all encoded variables in regression models and \n",
    "# add continuous variables to the regression model\n",
    "print(\"\\nFitting simple negative binomial regression model...\")\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Negative Binomial Regression Model', level=1)\n",
    "\n",
    "# Define continuous variables and create proper formula\n",
    "continuous_vars = ['immigrant_population_log', 'import_from_slu_log', 'age',  \n",
    "                   'state_percapita_income_log', 'state_unemployment','immigrant_density','climate_distance','economic_distance']\n",
    "\n",
    "# Make sure all continuous variables are properly formatted as numeric\n",
    "for var in continuous_vars:\n",
    "    if var in df_clean.columns:\n",
    "        df_clean[var] = pd.to_numeric(df_clean[var], errors='coerce')\n",
    "\n",
    "# Create formula with continuous variables properly treated\n",
    "formula_parts = []\n",
    "for var in continuous_vars:\n",
    "    if var in df_clean.columns:\n",
    "        formula_parts.append(var)\n",
    "\n",
    "# Add categorical variables with proper C() notation\n",
    "categorical_model_vars = ['sex_enc', 'marital_status_enc', 'employment_status_enc', \n",
    "                         'purpose_simple', 'accomd_type_enc', 'season_enc', 'us_state_enc','region_census_enc']\n",
    "\n",
    "for var in categorical_model_vars:\n",
    "    if var in df_clean.columns:\n",
    "        # Use the encoded variable name or the original if available\n",
    "        var_to_use = var\n",
    "        formula_parts.append(f\"C({var_to_use})\")\n",
    "\n",
    "# Combine into final formula\n",
    "formula = 'los_capped ~ ' + ' + '.join(formula_parts)\n",
    "print(f\"Model formula: {formula}\")\n",
    "\n",
    "# Add formula to document\n",
    "doc.add_paragraph(f\"Model formula: {formula}\")\n",
    "\n",
    "# Drop rows with missing values in formula variables\n",
    "\n",
    "formula_vars = ['los_capped'] + continuous_vars + categorical_model_vars\n",
    "df_clean_nb = df_clean[formula_vars].dropna()\n",
    "df_clean_nb = df_clean_nb.reset_index(drop=True)\n",
    "print(f\"Number of rows in df_clean_nb after dropping missing values: {len(df_clean_nb)}\")\n",
    "\n",
    "# Add this code chunk after your data preparation and before fitting regression models\n",
    "# This will ensure all categorical variables have proper labels in regression outputs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_labeled_categorical_variables(df_clean_nb):\n",
    "    \"\"\"\n",
    "    Create properly labeled categorical variables for regression analysis\n",
    "    This function maps encoded values back to meaningful labels\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define all the label mappings based on your data structure\n",
    "    \n",
    "    # Sex labels (assuming 0=Female, 1=Male based on typical encoding)\n",
    "    sex_labels = {\n",
    "        0: 'Female',\n",
    "        1: 'Male'\n",
    "    }\n",
    "    \n",
    "    # Marital Status labels (you may need to adjust these based on your actual encoding)\n",
    "    marital_status_labels = {\n",
    "        0: 'Married',\n",
    "        1: 'Other', \n",
    "        2: 'Single'\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Employment Status labels (adjust based on your encoding)\n",
    "    employment_status_labels = {\n",
    "        0: 'Employed',\n",
    "        1: 'Unemployed',\n",
    "        2: 'Student'\n",
    "       \n",
    "    }\n",
    "    \n",
    "    # Purpose labels (based on your existing purpose_simple mapping)\n",
    "    purpose_labels = {\n",
    "        1: 'Business',\n",
    "        2: 'Events',\n",
    "        3: 'Wedding', \n",
    "        4: 'Pleasure',\n",
    "        5: 'Other'\n",
    "    }\n",
    "    \n",
    "    # Accommodation type labels (adjust based on your data)\n",
    "    accomd_type_labels = {\n",
    "        0: 'Hotel',\n",
    "        1: 'Other',\n",
    "        2: 'Sharing'\n",
    "       \n",
    "    }\n",
    "    \n",
    "    # Season labels (based on your season encoding)\n",
    "    season_labels = {\n",
    "        1: 'Spring',\n",
    "        2: 'Summer', \n",
    "        3: 'Winter',\n",
    "        4: 'Fall'\n",
    "    }\n",
    "    \n",
    "    # Tourist type labels (if you have this variable)\n",
    "    tourist_type_labels = {\n",
    "        0: 'US_Born',\n",
    "        1: 'SLU_Born',\n",
    "        2: 'Other'\n",
    "    }\n",
    "    \n",
    "    # Region census labels (you'll need to adjust based on your actual regions)\n",
    "    region_census_labels = {\n",
    "        1: 'Northeast',\n",
    "        2: 'Midwest',\n",
    "        3: 'South', \n",
    "        4: 'West',\n",
    "        5: 'Other'\n",
    "    }\n",
    "    \n",
    "    # US State labels (create a mapping from encoded values to state names)\n",
    "    # You may want to create this mapping based on your actual state encoding\n",
    "    us_state_labels = {\n",
    "        1: 'Alabama', 2: 'Alaska', 3: 'Arizona', 4: 'Arkansas', 5: 'California', \n",
    "        6: 'Colorado', 7: 'Connecticut', 8: 'Delaware', 9: 'Florida', 10: 'Georgia',\n",
    "        11: 'Hawaii', 12: 'Idaho', 13: 'Illinois', 14: 'Indiana', 15: 'Iowa', \n",
    "        16: 'Kansas', 17: 'Kentucky', 18: 'Louisiana', 19: 'MON_REPOS', 20: 'Maine',\n",
    "        21: 'Maryland', 22: 'Massachusetts', 23: 'Michigan', 24: 'Minnesota', 25: 'Mississippi',\n",
    "        26: 'Missouri', 27: 'Montana', 28: 'Nebraska', 29: 'Nevada', 30: 'New_Hampshire',\n",
    "        31: 'New_Jersey', 32: 'New_Mexico', 33: 'New_York', 34: 'North_Carolina', 35: 'North_Dakota',\n",
    "        36: 'Ohio', 37: 'Oklahoma', 38: 'Oregon', 39: 'Pennsylvania', 40: 'Rhode_Island',\n",
    "        41: 'South_Carolina', 42: 'South_Dakota', 43: 'Tennessee', 44: 'Texas', 45: 'US_VIRGIN_ISLANDS',\n",
    "        46: 'Utah', 47: 'Vermont', 48: 'Virginia', 49: 'Washington', 50: 'Washington_DC',\n",
    "        51: 'West_Virginia', 52: 'Wisconsin', 53: 'Wyoming'\n",
    "    }\n",
    "    \n",
    "    # Replace the encoded columns with labeled versions (same column names)\n",
    "    \n",
    "    if 'sex_enc' in df_clean_nb.columns:\n",
    "        df_clean_nb['sex_enc'] = df_clean_nb['sex_enc'].map(sex_labels).fillna('Unknown')\n",
    "        df_clean_nb['sex_enc'] = pd.Categorical(df_clean_nb['sex_enc'])\n",
    "    \n",
    "    if 'marital_status_enc' in df_clean_nb.columns:\n",
    "        df_clean_nb['marital_status_enc'] = df_clean_nb['marital_status_enc'].map(marital_status_labels).fillna('Unknown')\n",
    "        df_clean_nb['marital_status_enc'] = pd.Categorical(df_clean_nb['marital_status_enc'])\n",
    "    \n",
    "    if 'employment_status_enc' in df_clean_nb.columns:\n",
    "        df_clean_nb['employment_status_enc'] = df_clean_nb['employment_status_enc'].map(employment_status_labels).fillna('Unknown')\n",
    "        df_clean_nb['employment_status_enc'] = pd.Categorical(df_clean_nb['employment_status_enc'])\n",
    "    \n",
    "    if 'purpose_simple' in df_clean_nb.columns:\n",
    "        df_clean_nb['purpose_simple'] = df_clean_nb['purpose_simple'].map(purpose_labels).fillna('Unknown')\n",
    "        df_clean_nb['purpose_simple'] = pd.Categorical(df_clean_nb['purpose_simple'])\n",
    "    \n",
    "    if 'accomd_type_enc' in df_clean_nb.columns:\n",
    "        df_clean_nb['accomd_type_enc'] = df_clean_nb['accomd_type_enc'].map(accomd_type_labels).fillna('Unknown')\n",
    "        df_clean_nb['accomd_type_enc'] = pd.Categorical(df_clean_nb['accomd_type_enc'])\n",
    "    \n",
    "    if 'season_enc' in df_clean_nb.columns:\n",
    "        df_clean_nb['season_enc'] = df_clean_nb['season_enc'].map(season_labels).fillna('Unknown')\n",
    "        df_clean_nb['season_enc'] = pd.Categorical(df_clean_nb['season_enc'])\n",
    "    \n",
    "    if 'tourist_type_enc' in df_clean_nb.columns:\n",
    "        df_clean_nb['tourist_type_enc'] = df_clean_nb['tourist_type_enc'].map(tourist_type_labels).fillna('Unknown')\n",
    "        df_clean_nb['tourist_type_enc'] = pd.Categorical(df_clean_nb['tourist_type_enc'])\n",
    "    \n",
    "    if 'region_census_enc' in df_clean_nb.columns:\n",
    "        df_clean_nb['region_census_enc'] = df_clean_nb['region_census_enc'].map(region_census_labels).fillna('Unknown')\n",
    "        df_clean_nb['region_census_enc'] = pd.Categorical(df_clean_nb['region_census_enc'])\n",
    "    \n",
    "    if 'us_state_enc' in df_clean_nb.columns:\n",
    "        df_clean_nb['us_state_enc'] = df_clean_nb['us_state_enc'].map(us_state_labels).fillna('Unknown')\n",
    "        df_clean_nb['us_state_enc'] = pd.Categorical(df_clean_nb['us_state_enc'])\n",
    "    \n",
    "    return df_clean_nb\n",
    "\n",
    "def print_categorical_mappings(df_clean_nb):\n",
    "    \"\"\"\n",
    "    Print the current categorical variables for verification\n",
    "    \"\"\"\n",
    "    print(\"Categorical Variables After Labeling:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    categorical_vars = [\n",
    "        'sex_enc', 'marital_status_enc', 'employment_status_enc', \n",
    "        'purpose_simple', 'accomd_type_enc', 'season_enc', \n",
    "        'region_census_enc', 'us_state_enc'\n",
    "    ]\n",
    "    \n",
    "    for var in categorical_vars:\n",
    "        if var in df_clean_nb.columns:\n",
    "            print(f\"\\n{var}:\")\n",
    "            value_counts = df_clean_nb[var].value_counts()\n",
    "            for value, count in value_counts.items():\n",
    "                print(f\"  {value}: {count}\")\n",
    "\n",
    "# Usage - replace your existing dataframe and formulas:\n",
    "\n",
    "# Same name dataframe with labeled categorical variables\n",
    "df_clean_nb = create_labeled_categorical_variables(df_clean_nb)\n",
    "print_categorical_mappings(df_clean_nb)\n",
    "\n",
    "# Your existing formulas will now work with labeled variables automatically\n",
    "# because we replaced the encoded columns with labeled versions using the same names\n",
    "\n",
    "# Same name formula with labeled variables (your original full formula)\n",
    "# This will now use the labeled categorical variables instead of numeric codes\n",
    "print(f\"\\nOriginal formula (now with labels): {formula}\")\n",
    "\n",
    "# Same name formula simple with labeled variables \n",
    "# This will now use the labeled categorical variables instead of numeric codes\n",
    "#print(f\"\\nSimple formula (now with labels): {formula_simple}\")\n",
    "\n",
    "# Same name formula simple with no state and labeled variables\n",
    "# This will now use the labeled categorical variables instead of numeric codes  \n",
    "#print(f\"\\nSimple formula without state (now with labels): {formula_simple_nostate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code chunk runs three negative binomials. In each case all continous data is in logs form with the exception of length of stay, age and continous variables which are ratio's such as the unemployment rate. \n",
    "\n",
    "The first negative binomial runs all variables. In that model we see that the VIF for age, distance and state per capita exceeds 100. We also note that the state per capita variable is insignificant. Marital status and employment status are insignificant. \n",
    " \n",
    "\n",
    "The second negative binomial attempts to address the issues with the first by removing insignificant and highly correlated variables\n",
    "\n",
    "The third model is the same as the second with the exception that it removes the categorical variable US state and reestimates the standard errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/jlslu2025/lib/python3.13/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/jlslu2025/lib/python3.13/site-packages/statsmodels/genmod/families/family.py:1367: ValueWarning: Negative binomial dispersion parameter alpha not set. Using default value alpha=1.0.\n",
      "  warnings.warn(\"Negative binomial dispersion parameter alpha not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negative Binomial Regression Results:\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:             los_capped   No. Observations:               139278\n",
      "Model:                            GLM   Df Residuals:                   139213\n",
      "Model Family:        NegativeBinomial   Df Model:                           64\n",
      "Link Function:                    log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:            -4.1576e+05\n",
      "Date:                Mon, 30 Jun 2025   Deviance:                       35883.\n",
      "Time:                        14:49:19   Pearson chi2:                 2.65e+05\n",
      "No. Iterations:                     8   Pseudo R-squ. (CS):            0.02774\n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================================\n",
      "                                             coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "Intercept                                  0.0122      0.003      4.285      0.000       0.007       0.018\n",
      "C(sex_enc)[T.Male]                         0.0101      0.006      1.738      0.082      -0.001       0.022\n",
      "C(marital_status_enc)[T.Other]             0.0048      0.020      0.241      0.810      -0.035       0.044\n",
      "C(marital_status_enc)[T.Single]           -0.0012      0.007     -0.173      0.863      -0.015       0.012\n",
      "C(employment_status_enc)[T.Student]       -0.0412      0.046     -0.894      0.371      -0.132       0.049\n",
      "C(employment_status_enc)[T.Unemployed]     0.0992      0.034      2.943      0.003       0.033       0.165\n",
      "C(purpose_simple)[T.Events]                0.1879      0.033      5.777      0.000       0.124       0.252\n",
      "C(purpose_simple)[T.Other]                 0.4268      0.026     16.419      0.000       0.376       0.478\n",
      "C(purpose_simple)[T.Pleasure]              0.1322      0.022      5.995      0.000       0.089       0.175\n",
      "C(purpose_simple)[T.Wedding]               0.0081      0.026      0.315      0.753      -0.042       0.058\n",
      "C(accomd_type_enc)[T.Other]                0.1242      0.015      8.084      0.000       0.094       0.154\n",
      "C(accomd_type_enc)[T.Sharing]              0.2214      0.009     24.946      0.000       0.204       0.239\n",
      "C(season_enc)[T.Summer]                   -0.0434      0.008     -5.555      0.000      -0.059      -0.028\n",
      "C(season_enc)[T.Unknown]                  -0.1001      0.009    -11.740      0.000      -0.117      -0.083\n",
      "C(season_enc)[T.Winter]                    0.0400      0.008      4.979      0.000       0.024       0.056\n",
      "C(us_state_enc)[T.Alaska]                 -0.0418      0.184     -0.227      0.820      -0.403       0.319\n",
      "C(us_state_enc)[T.Arizona]                -0.0684      0.033     -2.097      0.036      -0.132      -0.004\n",
      "C(us_state_enc)[T.Arkansas]                0.0939      0.035      2.683      0.007       0.025       0.162\n",
      "C(us_state_enc)[T.California]             -0.0158      0.025     -0.628      0.530      -0.065       0.033\n",
      "C(us_state_enc)[T.Colorado]                0.0720      0.036      2.029      0.042       0.002       0.142\n",
      "C(us_state_enc)[T.Connecticut]             0.0112      0.022      0.506      0.613      -0.032       0.054\n",
      "C(us_state_enc)[T.Delaware]               -0.0269      0.037     -0.718      0.473      -0.100       0.047\n",
      "C(us_state_enc)[T.Florida]                 0.0683      0.018      3.837      0.000       0.033       0.103\n",
      "C(us_state_enc)[T.Georgia]                -0.0339      0.017     -1.969      0.049      -0.068      -0.000\n",
      "C(us_state_enc)[T.Hawaii]                  0.1620      0.152      1.068      0.285      -0.135       0.459\n",
      "C(us_state_enc)[T.Idaho]                  -0.1088      0.064     -1.705      0.088      -0.234       0.016\n",
      "C(us_state_enc)[T.Illinois]               -0.0859      0.022     -3.923      0.000      -0.129      -0.043\n",
      "C(us_state_enc)[T.Indiana]                -0.0259      0.022     -1.162      0.245      -0.070       0.018\n",
      "C(us_state_enc)[T.Iowa]                   -0.0538      0.035     -1.533      0.125      -0.123       0.015\n",
      "C(us_state_enc)[T.Kansas]                 -0.1413      0.032     -4.446      0.000      -0.204      -0.079\n",
      "C(us_state_enc)[T.Kentucky]                0.0984      0.034      2.858      0.004       0.031       0.166\n",
      "C(us_state_enc)[T.Louisiana]               0.0427      0.034      1.257      0.209      -0.024       0.109\n",
      "C(us_state_enc)[T.Maine]                  -0.0686      0.038     -1.817      0.069      -0.143       0.005\n",
      "C(us_state_enc)[T.Maryland]               -0.0216      0.020     -1.071      0.284      -0.061       0.018\n",
      "C(us_state_enc)[T.Massachusetts]           0.0361      0.018      1.968      0.049       0.000       0.072\n",
      "C(us_state_enc)[T.Michigan]                0.0561      0.026      2.125      0.034       0.004       0.108\n",
      "C(us_state_enc)[T.Minnesota]               0.0048      0.028      0.170      0.865      -0.051       0.061\n",
      "C(us_state_enc)[T.Mississippi]            -0.0951      0.028     -3.370      0.001      -0.150      -0.040\n",
      "C(us_state_enc)[T.Missouri]               -0.0881      0.023     -3.877      0.000      -0.133      -0.044\n",
      "C(us_state_enc)[T.Montana]                -0.0455      0.069     -0.658      0.510      -0.181       0.090\n",
      "C(us_state_enc)[T.Nebraska]                0.0264      0.035      0.746      0.455      -0.043       0.096\n",
      "C(us_state_enc)[T.Nevada]                  0.0242      0.048      0.507      0.612      -0.069       0.118\n",
      "C(us_state_enc)[T.New_Hampshire]           0.0357      0.029      1.229      0.219      -0.021       0.093\n",
      "C(us_state_enc)[T.New_Jersey]             -0.0169      0.015     -1.108      0.268      -0.047       0.013\n",
      "C(us_state_enc)[T.New_Mexico]              0.0163      0.072      0.227      0.820      -0.124       0.157\n",
      "C(us_state_enc)[T.New_York]               -0.0169      0.016     -1.068      0.286      -0.048       0.014\n",
      "C(us_state_enc)[T.North_Carolina]         -0.0147      0.018     -0.840      0.401      -0.049       0.020\n",
      "C(us_state_enc)[T.North_Dakota]           -0.0372      0.072     -0.514      0.607      -0.179       0.105\n",
      "C(us_state_enc)[T.Ohio]                    0.0024      0.017      0.139      0.890      -0.031       0.036\n",
      "C(us_state_enc)[T.Oklahoma]               -0.0723      0.038     -1.915      0.055      -0.146       0.002\n",
      "C(us_state_enc)[T.Oregon]                  0.3271      0.046      7.133      0.000       0.237       0.417\n",
      "C(us_state_enc)[T.Pennsylvania]            0.0438      0.016      2.728      0.006       0.012       0.075\n",
      "C(us_state_enc)[T.Rhode_Island]            0.1292      0.038      3.368      0.001       0.054       0.204\n",
      "C(us_state_enc)[T.South_Carolina]          0.0110      0.028      0.392      0.695      -0.044       0.066\n",
      "C(us_state_enc)[T.South_Dakota]            0.3921      0.055      7.093      0.000       0.284       0.500\n",
      "C(us_state_enc)[T.Tennessee]              -0.0362      0.021     -1.747      0.081      -0.077       0.004\n",
      "C(us_state_enc)[T.Texas]                   0.0368      0.019      1.944      0.052      -0.000       0.074\n",
      "C(us_state_enc)[T.Utah]                   -0.0939      0.047     -2.017      0.044      -0.185      -0.003\n",
      "C(us_state_enc)[T.Vermont]                -0.1150      0.055     -2.108      0.035      -0.222      -0.008\n",
      "C(us_state_enc)[T.Virginia]               -0.0306      0.020     -1.498      0.134      -0.071       0.009\n",
      "C(us_state_enc)[T.Washington]             -0.0167      0.037     -0.456      0.649      -0.089       0.055\n",
      "C(us_state_enc)[T.West_Virginia]          -0.0123      0.056     -0.220      0.826      -0.122       0.097\n",
      "C(us_state_enc)[T.Wisconsin]              -0.0079      0.025     -0.319      0.749      -0.057       0.041\n",
      "C(us_state_enc)[T.Wyoming]                -0.1383      0.077     -1.792      0.073      -0.289       0.013\n",
      "C(region_census_enc)[T.Northeast]          0.0415      0.014      3.005      0.003       0.014       0.069\n",
      "C(region_census_enc)[T.South]             -0.0611      0.018     -3.463      0.001      -0.096      -0.027\n",
      "C(region_census_enc)[T.West]               0.0723      0.024      3.018      0.003       0.025       0.119\n",
      "immigrant_population_log                  -0.0204      0.004     -4.917      0.000      -0.029      -0.012\n",
      "import_from_slu_log                        0.0033      0.001      2.247      0.025       0.000       0.006\n",
      "age                                        0.0050      0.000     23.816      0.000       0.005       0.005\n",
      "state_percapita_income_log                 0.1545      0.005     31.028      0.000       0.145       0.164\n",
      "state_unemployment                        -0.0179      0.012     -1.464      0.143      -0.042       0.006\n",
      "immigrant_density                          0.0004      0.001      0.726      0.468      -0.001       0.001\n",
      "climate_distance                          -0.1182      0.053     -2.215      0.027      -0.223      -0.014\n",
      "economic_distance                          0.0139      0.007      1.908      0.056      -0.000       0.028\n",
      "==========================================================================================================\n",
      "\n",
      "Incident Rate Ratios (IRR):\n",
      "                                          IRR  Lower CI  Upper CI  \\\n",
      "Intercept                            1.012268  1.006638  1.017930   \n",
      "C(sex_enc)[T.Male]                   1.010177  0.998710  1.021776   \n",
      "C(marital_status_enc)[T.Other]       1.004857  0.966037  1.045236   \n",
      "C(marital_status_enc)[T.Single]      0.998794  0.985251  1.012524   \n",
      "C(employment_status_enc)[T.Student]  0.959637  0.876776  1.050328   \n",
      "...                                       ...       ...       ...   \n",
      "state_percapita_income_log           1.167029  1.155698  1.178471   \n",
      "state_unemployment                   0.982307  0.959103  1.006072   \n",
      "immigrant_density                    1.000370  0.999371  1.001370   \n",
      "climate_distance                     0.888507  0.800261  0.986483   \n",
      "economic_distance                    1.013952  0.999621  1.028488   \n",
      "\n",
      "                                           P-value  \n",
      "Intercept                             1.830135e-05  \n",
      "C(sex_enc)[T.Male]                    8.214988e-02  \n",
      "C(marital_status_enc)[T.Other]        8.095283e-01  \n",
      "C(marital_status_enc)[T.Single]       8.625163e-01  \n",
      "C(employment_status_enc)[T.Student]   3.712032e-01  \n",
      "...                                            ...  \n",
      "state_percapita_income_log           2.234248e-211  \n",
      "state_unemployment                    1.432979e-01  \n",
      "immigrant_density                     4.680989e-01  \n",
      "climate_distance                      2.676350e-02  \n",
      "economic_distance                     5.642292e-02  \n",
      "\n",
      "[75 rows x 4 columns]\n",
      "Error fitting negative binomial model (Simpler Model): ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n"
     ]
    }
   ],
   "source": [
    "# Fit simple negative binomial regression with continuous variables correctly specified\n",
    "\n",
    "\n",
    "# Fit negative binomial model with all variables\n",
    "nb_model = smf.glm(formula=formula, \n",
    "                  data=df_clean_nb, \n",
    "                  family=sm.families.NegativeBinomial(link=sm.families.links.log()))\n",
    "\n",
    "try:\n",
    "    nb_results = nb_model.fit()\n",
    "    print(\"\\nNegative Binomial Regression Results:\")\n",
    "    summary_text = str(nb_results.summary())\n",
    "    print(summary_text)\n",
    "    \n",
    "    # Add model summary to document\n",
    "    #doc.add_paragraph('\\nModel Summary:')\n",
    "    summary_paragraph_neg = doc.add_paragraph()\n",
    "    summary_run_neg = summary_paragraph_neg.add_run(summary_text)\n",
    "    summary_run_neg.font.name = 'Courier New'  # Use monospace font\n",
    "    #for line in summary_text.split('\\n'):\n",
    "        #doc.add_paragraph(line)\n",
    "    \n",
    "    # Convert coefficients to incident rate ratios (IRR)\n",
    "    print(\"\\nIncident Rate Ratios (IRR):\")\n",
    "    irr = np.exp(nb_results.params)\n",
    "    irr_conf = np.exp(nb_results.conf_int())\n",
    "    irr_df = pd.DataFrame({'IRR': irr, 'Lower CI': irr_conf[0], 'Upper CI': irr_conf[1], \n",
    "                          'P-value': nb_results.pvalues})\n",
    "    print(irr_df)\n",
    "    \n",
    "    # Add IRR table to document\n",
    "    doc.add_paragraph('\\n')\n",
    "    doc.add_heading('Incident Rate Ratios (IRR)', level=2)\n",
    "    irr_table = doc.add_table(rows=len(irr_df)+1, cols=5)\n",
    "    irr_table.style = 'Table Grid'\n",
    "    irr_table.cell(0, 0).text = 'Variable'\n",
    "    irr_table.cell(0, 1).text = 'IRR'\n",
    "    irr_table.cell(0, 2).text = 'Lower CI'\n",
    "    irr_table.cell(0, 3).text = 'Upper CI'\n",
    "    irr_table.cell(0, 4).text = 'P-value'\n",
    "    \n",
    "    for i, (var, row) in enumerate(irr_df.iterrows(), 1):\n",
    "        irr_table.cell(i, 0).text = str(var)\n",
    "        irr_table.cell(i, 1).text = f\"{row['IRR']:.4f}\"\n",
    "        irr_table.cell(i, 2).text = f\"{row['Lower CI']:.4f}\"\n",
    "        irr_table.cell(i, 3).text = f\"{row['Upper CI']:.4f}\"\n",
    "        irr_table.cell(i, 4).text = f\"{row['P-value']:.4f}\"\n",
    "    \n",
    "    \n",
    "    # Predictions and diagnostics\n",
    "    df_clean_nb['predicted'] = nb_results.predict()\n",
    "    df_clean_nb['residuals'] = df_clean_nb['los_capped'] - df_clean_nb['predicted']\n",
    "    \n",
    "    # Plot residuals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_clean_nb['predicted'], df_clean_nb['residuals'], alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot')\n",
    "    plt.tight_layout()\n",
    "    residuals_img = BytesIO()\n",
    "    plt.savefig(residuals_img, format='png')\n",
    "    residuals_img.seek(0)\n",
    "    plt.close()\n",
    "    \n",
    "    # Add residuals plot to document\n",
    "    doc.add_paragraph('\\n')\n",
    "    doc.add_heading('Diagnostics', level=2)\n",
    "    doc.add_picture(residuals_img, width=Inches(6))\n",
    "    doc.add_paragraph('Figure 3: Residuals Plot')\n",
    "\n",
    "    #Add VIF for continuous variables and categorical variables\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = df_clean_nb[continuous_vars + categorical_model_vars].columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df_clean_nb[continuous_vars + categorical_model_vars].values, i) \n",
    "                       for i in range(df_clean_nb[continuous_vars + categorical_model_vars].shape[1])]\n",
    "    print(\"VIF for continuous and dummy variables:\")\n",
    "    print(vif_data)\n",
    "\n",
    "    #Add VIF table to document\n",
    "    doc.add_paragraph('\\nVariance Inflation Factor (VIF) for Continuous and Categorical Variables:')    \n",
    "    vif_table = doc.add_table(rows=len(vif_data)+1, cols=2)\n",
    "    vif_table.style = 'Table Grid'\n",
    "    vif_table.cell(0, 0).text = 'Variable'\n",
    "    vif_table.cell(0, 1).text = 'VIF'\n",
    "    for i, (var, vif) in enumerate(zip(vif_data[\"Variable\"], vif_data[\"VIF\"]), 1):\n",
    "        vif_table.cell(i, 0).text = str(var)\n",
    "        vif_table.cell(i, 1).text = f\"{vif:.4f}\"\n",
    "    doc.add_paragraph('VIF values above 10 indicate potential multicollinearity issues.')\n",
    "#except Exception as e:\n",
    "    #error_msg = f\"Error fitting negative binomial model: {str(e)}\"\n",
    "    #print(error_msg)\n",
    "    #doc.add_paragraph(error_msg)\n",
    "    #doc.add_paragraph(\"The negative binomial regression model failed to converge. This can happen due to \" +\n",
    "                     #\"insufficient variation in the data or other model specification issues.\")\n",
    "    \n",
    "\n",
    "# Fit a negative binomial model with fewer variables\n",
    "\n",
    "#New continuous variables for the simpler model\n",
    "    continuous_vars_simple = [ 'import_from_slu_log', 'age',  \n",
    "                    'state_unemployment','immigrant_density','climate_distance','economic_distance']\n",
    "\n",
    "#New categorical variables for the simpler model\n",
    "\n",
    "    categorical_model_vars_simple = ['sex_enc',  \n",
    "                         'purpose_simple', 'accomd_type_enc', 'season_enc', 'us_state_enc']\n",
    "    \n",
    "    categorical_model_vars_simple_nostate= ['sex_enc',  \n",
    "                         'purpose_simple', 'accomd_type_enc', 'season_enc',  'region_census_enc']\n",
    "    \n",
    "#New formula for the simpler model\n",
    "    formula_parts_simple = []\n",
    "    for var in continuous_vars_simple:\n",
    "        if var in df_clean_nb.columns:\n",
    "            formula_parts_simple.append(var)\n",
    "    # Add categorical variables with proper C() notation\n",
    "    for var in categorical_model_vars_simple:\n",
    "        if var in df_clean_nb.columns:\n",
    "            # Use the encoded variable name or the original if available\n",
    "            var_to_use = var\n",
    "            formula_parts_simple.append(f\"C({var_to_use})\")\n",
    "    # Combine into final formula\n",
    "    formula_simple = 'los_capped ~ ' + ' + '.join(formula_parts_simple)\n",
    "    print(f\"Model formula for simpler model: {formula_simple}\")\n",
    "    # Add formula to document\n",
    "    doc.add_paragraph(f\"Model formula for simpler model: {formula_simple}\")\n",
    "\n",
    "#New formula for the simpler model without the state variable\n",
    "    formula_parts_simple_nostate = []\n",
    "    for var in continuous_vars_simple:\n",
    "        if var in df_clean_nb.columns:\n",
    "            formula_parts_simple_nostate.append(var)\n",
    "    # Add categorical variables with proper C() notation\n",
    "    for var in categorical_model_vars_simple_nostate:\n",
    "        if var in df_clean_nb.columns:\n",
    "            # Use the encoded variable name or the original if available\n",
    "            var_to_use = var\n",
    "            formula_parts_simple_nostate.append(f\"C({var_to_use})\")\n",
    "    # Combine into final formula\n",
    "    formula_simple_nostate = 'los_capped ~ ' + ' + '.join(formula_parts_simple_nostate)\n",
    "    print(f\"Model formula for simpler model without state: {formula_simple_nostate}\")\n",
    "\n",
    "# Fit negative binomial model with fewer variables\n",
    "    nb_model_simple = smf.glm(formula=formula_simple, \n",
    "                          data=df_clean_nb, \n",
    "                          family=sm.families.NegativeBinomial(link=sm.families.links.log()))\n",
    "    try:\n",
    "        nb_results_simple = nb_model_simple.fit()\n",
    "        print(\"\\nNegative Binomial Regression Results (Simpler Model):\")\n",
    "        summary_text_simple = str(nb_results_simple.summary())\n",
    "        print(summary_text_simple)\n",
    "        \n",
    "        # Add model summary to document\n",
    "        summary_paragraph_neg_simple = doc.add_paragraph()\n",
    "        summary_run_neg_simple = summary_paragraph_neg_simple.add_run(summary_text_simple)\n",
    "        summary_run_neg_simple.font.name = 'Courier New'  # Use monospace font\n",
    "        \n",
    "        # Convert coefficients to incident rate ratios (IRR)\n",
    "        print(\"\\nIncident Rate Ratios (IRR) for Simpler Model:\")\n",
    "        irr_simple = np.exp(nb_results_simple.params)\n",
    "        irr_conf_simple = np.exp(nb_results_simple.conf_int())\n",
    "        irr_df_simple = pd.DataFrame({'IRR': irr_simple, 'Lower CI': irr_conf_simple[0], 'Upper CI': irr_conf_simple[1], \n",
    "                                    'P-value': nb_results_simple.pvalues})\n",
    "        print(irr_df_simple)\n",
    "        \n",
    "        # Add IRR table to document\n",
    "        doc.add_paragraph('\\n')\n",
    "        doc.add_heading('Incident Rate Ratios (IRR) - Simpler Model', level=2)\n",
    "        irr_table_simple = doc.add_table(rows=len(irr_df_simple)+1, cols=5)\n",
    "        irr_table_simple.style = 'Table Grid'\n",
    "        irr_table_simple.cell(0, 0).text = 'Variable'\n",
    "        irr_table_simple.cell(0, 1).text = 'IRR'\n",
    "        irr_table_simple.cell(0, 2).text = 'Lower CI'\n",
    "        irr_table_simple.cell(0, 3).text = 'Upper CI'\n",
    "        irr_table_simple.cell(0, 4).text = 'P-value'\n",
    "        \n",
    "        for i, (var, row) in enumerate(irr_df_simple.iterrows(), 1):\n",
    "            irr_table_simple.cell(i, 0).text = str(var)\n",
    "            irr_table_simple.cell(i, 1).text = f\"{row['IRR']:.4f}\"\n",
    "            irr_table_simple.cell(i, 2).text = f\"{row['Lower CI']:.4f}\"\n",
    "            irr_table_simple.cell(i, 3).text = f\"{row['Upper CI']:.4f}\"\n",
    "            irr_table_simple.cell(i, 4).text = f\"{row['P-value']:.4f}\"\n",
    "        # Predictions and diagnostics for simpler model\n",
    "        df_clean_nb['predicted_simple'] = nb_results_simple.predict()\n",
    "        df_clean_nb['residuals_simple'] = df_clean_nb['los_capped'] - df_clean_nb['predicted_simple']\n",
    "        # Plot residuals for simpler model\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(df_clean_nb['predicted_simple'], df_clean_nb['residuals_simple'], alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='-')\n",
    "        plt.xlabel('Predicted Values (Simpler Model)')\n",
    "        plt.ylabel('Residuals (Simpler Model)')\n",
    "        plt.title('Residual Plot (Simpler Model)')\n",
    "        plt.tight_layout()\n",
    "        residuals_img_simple = BytesIO()\n",
    "        plt.savefig(residuals_img_simple, format='png')\n",
    "        residuals_img_simple.seek(0)\n",
    "        plt.close()\n",
    "        # Add residuals plot for simpler model to document\n",
    "        doc.add_paragraph('\\n')\n",
    "        doc.add_heading('Diagnostics - Simpler Model', level=2)\n",
    "        doc.add_picture(residuals_img_simple, width=Inches(6))\n",
    "        doc.add_paragraph('Figure 4: Residuals Plot (Simpler Model)')\n",
    "        # Add VIF for continuous variables and categorical variables for simpler model\n",
    "        vif_data_simple = pd.DataFrame()\n",
    "        vif_data_simple[\"Variable\"] = df_clean_nb[continuous_vars_simple + categorical_model_vars_simple].columns\n",
    "        vif_data_simple[\"VIF\"] = [variance_inflation_factor(df_clean_nb[continuous_vars_simple + categorical_model_vars_simple].values, i) \n",
    "                                for i in range(df_clean_nb[continuous_vars_simple + categorical_model_vars_simple].shape[1])]\n",
    "        print(\"VIF for continuous and dummy variables (Simpler Model):\")\n",
    "        print(vif_data_simple)\n",
    "        # Add VIF table for simpler model to document\n",
    "        doc.add_paragraph('\\nVariance Inflation Factor (VIF) for Continuous and Categorical Variables - Simpler Model:')\n",
    "        vif_table_simple = doc.add_table(rows=len(vif_data_simple)+1, cols=2)\n",
    "        vif_table_simple.style = 'Table Grid'\n",
    "        vif_table_simple.cell(0, 0).text = 'Variable'\n",
    "        vif_table_simple.cell(0, 1).text = 'VIF'\n",
    "        for i, (var, vif) in enumerate(zip(vif_data_simple[\"Variable\"], vif_data_simple[\"VIF\"]), 1):\n",
    "            vif_table_simple.cell(i, 0).text = str(var)\n",
    "            vif_table_simple.cell(i, 1).text = f\"{vif:.4f}\"\n",
    "        doc.add_paragraph('VIF values above 10 indicate potential multicollinearity issues in the simpler model.')\n",
    "\n",
    "        #start of third model without the state variable\n",
    "        nb_model_simple = smf.glm(formula=formula_simple_nostate, \n",
    "                          data=df_clean_nb, \n",
    "                          family=sm.families.NegativeBinomial(link=sm.families.links.log()))\n",
    "        try:\n",
    "            nb_results_simple = nb_model_simple.fit()\n",
    "            print(\"\\nNegative Binomial Regression Results (Simpler Model):\")\n",
    "            summary_text_simple = str(nb_results_simple.summary())\n",
    "            print(summary_text_simple)\n",
    "            \n",
    "            # Add model summary to document\n",
    "            summary_paragraph_neg_simple = doc.add_paragraph()\n",
    "            summary_run_neg_simple = summary_paragraph_neg_simple.add_run(summary_text_simple)\n",
    "            summary_run_neg_simple.font.name = 'Courier New'  # Use monospace font\n",
    "            \n",
    "            # Convert coefficients to incident rate ratios (IRR)\n",
    "            print(\"\\nIncident Rate Ratios (IRR) for Simpler Model:\")\n",
    "            irr_simple = np.exp(nb_results_simple.params)\n",
    "            irr_conf_simple = np.exp(nb_results_simple.conf_int())\n",
    "            irr_df_simple = pd.DataFrame({'IRR': irr_simple, 'Lower CI': irr_conf_simple[0], 'Upper CI': irr_conf_simple[1], \n",
    "                                        'P-value': nb_results_simple.pvalues})\n",
    "            print(irr_df_simple)\n",
    "            \n",
    "            # Add IRR table to document\n",
    "            doc.add_paragraph('\\n')\n",
    "            doc.add_heading('Incident Rate Ratios (IRR) - Simpler Model', level=2)\n",
    "            irr_table_simple = doc.add_table(rows=len(irr_df_simple)+1, cols=5)\n",
    "            irr_table_simple.style = 'Table Grid'\n",
    "            irr_table_simple.cell(0, 0).text = 'Variable'\n",
    "            irr_table_simple.cell(0, 1).text = 'IRR'\n",
    "            irr_table_simple.cell(0, 2).text = 'Lower CI'\n",
    "            irr_table_simple.cell(0, 3).text = 'Upper CI'\n",
    "            irr_table_simple.cell(0, 4).text = 'P-value'\n",
    "            \n",
    "            for i, (var, row) in enumerate(irr_df_simple.iterrows(), 1):\n",
    "                irr_table_simple.cell(i, 0).text = str(var)\n",
    "                irr_table_simple.cell(i, 1).text = f\"{row['IRR']:.4f}\"\n",
    "                irr_table_simple.cell(i, 2).text = f\"{row['Lower CI']:.4f}\"\n",
    "                irr_table_simple.cell(i, 3).text = f\"{row['Upper CI']:.4f}\"\n",
    "                irr_table_simple.cell(i, 4).text = f\"{row['P-value']:.4f}\"\n",
    "            # Predictions and diagnostics for simpler model\n",
    "            df_clean_nb['predicted_simple'] = nb_results_simple.predict()\n",
    "            df_clean_nb['residuals_simple'] = df_clean_nb['los_capped'] - df_clean_nb['predicted_simple']\n",
    "            # Plot residuals for simpler model\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(df_clean_nb['predicted_simple'], df_clean_nb['residuals_simple'], alpha=0.5)\n",
    "            plt.axhline(y=0, color='r', linestyle='-')\n",
    "            plt.xlabel('Predicted Values (Simpler Model)')\n",
    "            plt.ylabel('Residuals (Simpler Model)')\n",
    "            plt.title('Residual Plot (Simpler Model)')\n",
    "            plt.tight_layout()\n",
    "            residuals_img_simple = BytesIO()\n",
    "            plt.savefig(residuals_img_simple, format='png')\n",
    "            residuals_img_simple.seek(0)\n",
    "            plt.close()\n",
    "            # Add residuals plot for simpler model to document\n",
    "            doc.add_paragraph('\\n')\n",
    "            doc.add_heading('Diagnostics - Simpler Model', level=2)\n",
    "            doc.add_picture(residuals_img_simple, width=Inches(6))\n",
    "            doc.add_paragraph('Figure 4: Residuals Plot (Simpler Model)')\n",
    "            # Add VIF for continuous variables and categorical variables for simpler model\n",
    "            vif_data_simple = pd.DataFrame()\n",
    "            vif_data_simple[\"Variable\"] = df_clean_nb[continuous_vars_simple + categorical_model_vars_simple].columns\n",
    "            vif_data_simple[\"VIF\"] = [variance_inflation_factor(df_clean_nb[continuous_vars_simple + categorical_model_vars_simple].values, i) \n",
    "                                    for i in range(df_clean_nb[continuous_vars_simple + categorical_model_vars_simple].shape[1])]\n",
    "            print(\"VIF for continuous and dummy variables (Simpler Model):\")\n",
    "            print(vif_data_simple)\n",
    "            # Add VIF table for simpler model to document\n",
    "            doc.add_paragraph('\\nVariance Inflation Factor (VIF) for Continuous and Categorical Variables - Simpler Model:')\n",
    "            vif_table_simple = doc.add_table(rows=len(vif_data_simple)+1, cols=2)\n",
    "            vif_table_simple.style = 'Table Grid'\n",
    "            vif_table_simple.cell(0, 0).text = 'Variable'\n",
    "            vif_table_simple.cell(0, 1).text = 'VIF'\n",
    "            for i, (var, vif) in enumerate(zip(vif_data_simple[\"Variable\"], vif_data_simple[\"VIF\"]), 1):\n",
    "                vif_table_simple.cell(i, 0).text = str(var)\n",
    "                vif_table_simple.cell(i, 1).text = f\"{vif:.4f}\"\n",
    "            doc.add_paragraph('VIF values above 10 indicate potential multicollinearity issues in the simpler model.')\n",
    "        #end of third model\n",
    "    \n",
    "        except Exception as e:\n",
    "            error_msg_simple = f\"Error fitting negative binomial model (Simpler Model): {str(e)}\"\n",
    "            print(error_msg_simple)\n",
    "            doc.add_paragraph(error_msg_simple)\n",
    "            doc.add_paragraph(\"The negative binomial regression model with fewer variables failed to converge. \" +\n",
    "                            \"This can happen due to insufficient variation in the data or other model specification issues.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg_simple = f\"Error fitting negative binomial model (Simpler Model): {str(e)}\"\n",
    "        print(error_msg_simple)\n",
    "        doc.add_paragraph(error_msg_simple)\n",
    "        doc.add_paragraph(\"The negative binomial regression model with fewer variables failed to converge. \" +\n",
    "                         \"This can happen due to insufficient variation in the data or other model specification issues.\")\n",
    "except Exception as e:\n",
    "    error_msg_simple = f\"Error fitting negative binomial model (Simpler Model): {str(e)}\"\n",
    "    print(error_msg_simple)\n",
    "    doc.add_paragraph(error_msg_simple)\n",
    "    doc.add_paragraph(\"The negative binomial regression model with fewer variables failed to converge. \" +\n",
    "                        \"This can happen due to insufficient variation in the data or other model specification issues.\")       \n",
    "\n",
    "\n",
    "\n",
    "# End of negative binomial models\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw a scatter plot between age_log and distance_miles_log\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df_clean_nb['age'], y=df_clean_nb['distance_miles_log'])\n",
    "plt.title('Scatter Plot of  Age vs Log Distance Miles')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Log Distance Miles')\n",
    "plt.savefig('scatter_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "#scatter_img = BytesIO()\n",
    "#plt.savefig(scatter_img, format='png')\n",
    "#scatter_img.seek(0)\n",
    "plt.close()\n",
    "# Add scatter plot to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Scatter Plot of Age vs Log Distance Miles', level=2)\n",
    "doc.add_picture('scatter_plot.png', width=Inches(6))\n",
    "doc.add_paragraph('Figure 5: Scatter Plot of Age vs Log Distance Miles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component re runs the third model with clustered standard errors around state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Binomial Model with Clustered Robust Standard Errors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def fit_nb_with_clustered_se(df_clean_nb, formula_simple_nostate):\n",
    "    \"\"\"\n",
    "    Fit negative binomial model with robust standard errors clustered by US state\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fit the model (same as before)\n",
    "    nb_model_simple = smf.glm(formula=formula_simple_nostate, \n",
    "                              data=df_clean_nb, \n",
    "                              family=sm.families.NegativeBinomial(link=sm.families.links.log()))\n",
    "    \n",
    "    # Fit with clustered robust standard errors\n",
    "    nb_results_clustered = nb_model_simple.fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': df_clean_nb['us_state_enc']}  # Cluster by state\n",
    "    )\n",
    "    \n",
    "    return nb_results_clustered\n",
    "\n",
    "def compare_standard_errors(df_clean_nb, formula_simple_nostate):\n",
    "    \"\"\"\n",
    "    Compare regular vs clustered standard errors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model setup\n",
    "    nb_model_simple = smf.glm(formula=formula_simple_nostate, \n",
    "                              data=df_clean_nb, \n",
    "                              family=sm.families.NegativeBinomial(link=sm.families.links.log()))\n",
    "    \n",
    "    # Regular standard errors\n",
    "    nb_results_regular = nb_model_simple.fit()\n",
    "    \n",
    "    # Clustered standard errors  \n",
    "    nb_results_clustered = nb_model_simple.fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': df_clean_nb['us_state_enc']}\n",
    "    )\n",
    "    \n",
    "    # Other robust SE options you might consider:\n",
    "    # Heteroskedasticity-robust (White's)\n",
    "    nb_results_robust = nb_model_simple.fit(cov_type='HC1')\n",
    "    \n",
    "    # Newey-West (for time series, if applicable)\n",
    "    # nb_results_nw = nb_model_simple.fit(cov_type='HAC', cov_kwds={'maxlags': 1})\n",
    "    \n",
    "    # Compare results\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Variable': nb_results_regular.params.index,\n",
    "        'Coefficient': nb_results_regular.params.values,\n",
    "        'SE_Regular': nb_results_regular.bse.values,\n",
    "        'SE_Clustered': nb_results_clustered.bse.values,\n",
    "        'SE_Robust': nb_results_robust.bse.values,\n",
    "        'P_Regular': nb_results_regular.pvalues.values,\n",
    "        'P_Clustered': nb_results_clustered.pvalues.values,\n",
    "        'P_Robust': nb_results_robust.pvalues.values\n",
    "    })\n",
    "    \n",
    "    # Calculate ratio of clustered to regular SE\n",
    "    comparison_df['SE_Ratio_Clustered'] = comparison_df['SE_Clustered'] / comparison_df['SE_Regular']\n",
    "    comparison_df['SE_Ratio_Robust'] = comparison_df['SE_Robust'] / comparison_df['SE_Regular']\n",
    "    \n",
    "    return nb_results_clustered, comparison_df\n",
    "\n",
    "def print_clustered_results(results, comparison_df=None):\n",
    "    \"\"\"\n",
    "    Print results with clustered standard errors\n",
    "    \"\"\"\n",
    "    print(\"Negative Binomial Results with Clustered Standard Errors (by US State)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results.summary())\n",
    "    \n",
    "    if comparison_df is not None:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"COMPARISON OF STANDARD ERRORS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"SE_Ratio > 1 indicates clustering increased standard errors\")\n",
    "        print(\"SE_Ratio < 1 indicates clustering decreased standard errors\")\n",
    "        print()\n",
    "        \n",
    "        # Format comparison table nicely\n",
    "        display_df = comparison_df.copy()\n",
    "        for col in ['Coefficient', 'SE_Regular', 'SE_Clustered', 'SE_Robust']:\n",
    "            display_df[col] = display_df[col].round(4)\n",
    "        for col in ['P_Regular', 'P_Clustered', 'P_Robust']:\n",
    "            display_df[col] = display_df[col].round(4)\n",
    "        for col in ['SE_Ratio_Clustered', 'SE_Ratio_Robust']:\n",
    "            display_df[col] = display_df[col].round(3)\n",
    "            \n",
    "        print(display_df[['Variable', 'Coefficient', 'SE_Regular', 'SE_Clustered', \n",
    "                         'SE_Ratio_Clustered', 'P_Regular', 'P_Clustered']].to_string(index=False))\n",
    "\n",
    "def calculate_clustered_irr(results):\n",
    "    \"\"\"\n",
    "    Calculate IRR with clustered confidence intervals\n",
    "    \"\"\"\n",
    "    # IRR (Incident Rate Ratios)\n",
    "    irr = np.exp(results.params)\n",
    "    \n",
    "    # Confidence intervals with clustered SEs\n",
    "    irr_conf = np.exp(results.conf_int())\n",
    "    \n",
    "    irr_df = pd.DataFrame({\n",
    "        'IRR': irr,\n",
    "        'Lower_CI': irr_conf.iloc[:, 0],\n",
    "        'Upper_CI': irr_conf.iloc[:, 1],\n",
    "        'P_value': results.pvalues,\n",
    "        'Significant': results.pvalues < 0.05\n",
    "    })\n",
    "    \n",
    "    return irr_df\n",
    "\n",
    "\n",
    "\n",
    "# Additional clustering options if needed:\n",
    "def alternative_clustering_methods(df_clean_nb, formula_simple_nostate):\n",
    "    \"\"\"\n",
    "    Show alternative clustering approaches\n",
    "    \"\"\"\n",
    "    nb_model = smf.glm(formula=formula_simple_nostate, \n",
    "                       data=df_clean_nb, \n",
    "                       family=sm.families.NegativeBinomial(link=sm.families.links.log()))\n",
    "    \n",
    "    # Method 1: Cluster by state (recommended for your case)\n",
    "    results_state = nb_model.fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': df_clean_nb['us_state_enc']}\n",
    "    )\n",
    "    \n",
    "    # Method 2: Two-way clustering (if you had both state and time)\n",
    "    # results_twoway = nb_model.fit(\n",
    "    #     cov_type='cluster',\n",
    "    #     cov_kwds={'groups': [df_clean_nb['us_state_enc'], df_clean_nb['year']]}\n",
    "    # )\n",
    "    \n",
    "    # Method 3: Bootstrap standard errors (alternative approach)\n",
    "    # This is more computationally intensive but doesn't require specific clustering assumptions\n",
    "    \n",
    "    return results_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/jlslu2025/lib/python3.13/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/jlslu2025/lib/python3.13/site-packages/statsmodels/genmod/families/family.py:1367: ValueWarning: Negative binomial dispersion parameter alpha not set. Using default value alpha=1.0.\n",
      "  warnings.warn(\"Negative binomial dispersion parameter alpha not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Binomial Results with Clustered Standard Errors (by US State)\n",
      "================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:             los_capped   No. Observations:               139278\n",
      "Model:                            GLM   Df Residuals:                   139258\n",
      "Model Family:        NegativeBinomial   Df Model:                           19\n",
      "Link Function:                    log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:            -4.1591e+05\n",
      "Date:                Mon, 30 Jun 2025   Deviance:                       36195.\n",
      "Time:                        14:49:45   Pearson chi2:                 2.74e+05\n",
      "No. Iterations:                     8   Pseudo R-squ. (CS):            0.02556\n",
      "Covariance Type:              cluster                                         \n",
      "=====================================================================================================\n",
      "                                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Intercept                             1.3654      0.261      5.233      0.000       0.854       1.877\n",
      "C(sex_enc)[T.Male]                    0.0100      0.006      1.625      0.104      -0.002       0.022\n",
      "C(purpose_simple)[T.Events]           0.1830      0.047      3.906      0.000       0.091       0.275\n",
      "C(purpose_simple)[T.Other]            0.4328      0.058      7.497      0.000       0.320       0.546\n",
      "C(purpose_simple)[T.Pleasure]         0.1295      0.034      3.760      0.000       0.062       0.197\n",
      "C(purpose_simple)[T.Wedding]          0.0079      0.049      0.163      0.870      -0.087       0.103\n",
      "C(accomd_type_enc)[T.Other]           0.1303      0.028      4.702      0.000       0.076       0.185\n",
      "C(accomd_type_enc)[T.Sharing]         0.2214      0.027      8.277      0.000       0.169       0.274\n",
      "C(season_enc)[T.Summer]              -0.0438      0.011     -4.167      0.000      -0.064      -0.023\n",
      "C(season_enc)[T.Unknown]             -0.1015      0.011     -9.160      0.000      -0.123      -0.080\n",
      "C(season_enc)[T.Winter]               0.0414      0.014      2.960      0.003       0.014       0.069\n",
      "C(region_census_enc)[T.Northeast]     0.0396      0.025      1.596      0.110      -0.009       0.088\n",
      "C(region_census_enc)[T.South]        -0.0254      0.026     -0.985      0.324      -0.076       0.025\n",
      "C(region_census_enc)[T.West]          0.1037      0.033      3.166      0.002       0.040       0.168\n",
      "import_from_slu_log                  -0.0010      0.001     -0.750      0.453      -0.004       0.002\n",
      "age                                   0.0051      0.000     11.957      0.000       0.004       0.006\n",
      "state_unemployment                   -0.0231      0.011     -2.067      0.039      -0.045      -0.001\n",
      "immigrant_density                    -0.0009      0.000     -2.955      0.003      -0.002      -0.000\n",
      "climate_distance                      0.0680      0.061      1.119      0.263      -0.051       0.187\n",
      "economic_distance                     0.4308      0.416      1.036      0.300      -0.384       1.246\n",
      "=====================================================================================================\n",
      "\n",
      "================================================================================\n",
      "COMPARISON OF STANDARD ERRORS\n",
      "================================================================================\n",
      "SE_Ratio > 1 indicates clustering increased standard errors\n",
      "SE_Ratio < 1 indicates clustering decreased standard errors\n",
      "\n",
      "                         Variable  Coefficient  SE_Regular  SE_Clustered  SE_Ratio_Clustered  P_Regular  P_Clustered\n",
      "                        Intercept       1.3654      0.1146        0.2609               2.276     0.0000       0.0000\n",
      "               C(sex_enc)[T.Male]       0.0100      0.0058        0.0061               1.062     0.0843       0.1042\n",
      "      C(purpose_simple)[T.Events]       0.1830      0.0325        0.0468               1.442     0.0000       0.0001\n",
      "       C(purpose_simple)[T.Other]       0.4328      0.0260        0.0577               2.223     0.0000       0.0000\n",
      "    C(purpose_simple)[T.Pleasure]       0.1295      0.0220        0.0344               1.565     0.0000       0.0002\n",
      "     C(purpose_simple)[T.Wedding]       0.0079      0.0256        0.0486               1.898     0.7565       0.8702\n",
      "      C(accomd_type_enc)[T.Other]       0.1303      0.0153        0.0277               1.809     0.0000       0.0000\n",
      "    C(accomd_type_enc)[T.Sharing]       0.2214      0.0088        0.0268               3.041     0.0000       0.0000\n",
      "          C(season_enc)[T.Summer]      -0.0438      0.0078        0.0105               1.348     0.0000       0.0000\n",
      "         C(season_enc)[T.Unknown]      -0.1015      0.0085        0.0111               1.301     0.0000       0.0000\n",
      "          C(season_enc)[T.Winter]       0.0414      0.0080        0.0140               1.747     0.0000       0.0031\n",
      "C(region_census_enc)[T.Northeast]       0.0396      0.0109        0.0248               2.285     0.0003       0.1104\n",
      "    C(region_census_enc)[T.South]      -0.0254      0.0112        0.0258               2.299     0.0235       0.3244\n",
      "     C(region_census_enc)[T.West]       0.1037      0.0132        0.0327               2.472     0.0000       0.0015\n",
      "              import_from_slu_log      -0.0010      0.0007        0.0014               1.992     0.1351       0.4533\n",
      "                              age       0.0051      0.0002        0.0004               2.130     0.0000       0.0000\n",
      "               state_unemployment      -0.0231      0.0047        0.0112               2.366     0.0000       0.0387\n",
      "                immigrant_density      -0.0009      0.0002        0.0003               1.469     0.0000       0.0031\n",
      "                 climate_distance       0.0680      0.0336        0.0607               1.810     0.0428       0.2630\n",
      "                economic_distance       0.4308      0.1756        0.4157               2.368     0.0141       0.3000\n",
      "\\nIncident Rate Ratios with Clustered Standard Errors:\n",
      "                                      IRR  Lower_CI  Upper_CI  P_value  \\\n",
      "Intercept                          3.9173    2.3490    6.5326   0.0000   \n",
      "C(sex_enc)[T.Male]                 1.0100    0.9979    1.0223   0.1042   \n",
      "C(purpose_simple)[T.Events]        1.2008    1.0954    1.3162   0.0001   \n",
      "C(purpose_simple)[T.Other]         1.5415    1.3766    1.7262   0.0000   \n",
      "C(purpose_simple)[T.Pleasure]      1.1383    1.0640    1.2177   0.0002   \n",
      "C(purpose_simple)[T.Wedding]       1.0080    0.9164    1.1087   0.8702   \n",
      "C(accomd_type_enc)[T.Other]        1.1392    1.0790    1.2028   0.0000   \n",
      "C(accomd_type_enc)[T.Sharing]      1.2479    1.1841    1.3151   0.0000   \n",
      "C(season_enc)[T.Summer]            0.9572    0.9377    0.9771   0.0000   \n",
      "C(season_enc)[T.Unknown]           0.9035    0.8841    0.9233   0.0000   \n",
      "C(season_enc)[T.Winter]            1.0423    1.0141    1.0713   0.0031   \n",
      "C(region_census_enc)[T.Northeast]  1.0404    0.9910    1.0923   0.1104   \n",
      "C(region_census_enc)[T.South]      0.9749    0.9270    1.0254   0.3244   \n",
      "C(region_census_enc)[T.West]       1.1093    1.0403    1.1828   0.0015   \n",
      "import_from_slu_log                0.9990    0.9963    1.0016   0.4533   \n",
      "age                                1.0051    1.0042    1.0059   0.0000   \n",
      "state_unemployment                 0.9772    0.9560    0.9988   0.0387   \n",
      "immigrant_density                  0.9991    0.9985    0.9997   0.0031   \n",
      "climate_distance                   1.0704    0.9502    1.2057   0.2630   \n",
      "economic_distance                  1.5385    0.6811    3.4752   0.3000   \n",
      "\n",
      "                                   Significant  \n",
      "Intercept                                 True  \n",
      "C(sex_enc)[T.Male]                       False  \n",
      "C(purpose_simple)[T.Events]               True  \n",
      "C(purpose_simple)[T.Other]                True  \n",
      "C(purpose_simple)[T.Pleasure]             True  \n",
      "C(purpose_simple)[T.Wedding]             False  \n",
      "C(accomd_type_enc)[T.Other]               True  \n",
      "C(accomd_type_enc)[T.Sharing]             True  \n",
      "C(season_enc)[T.Summer]                   True  \n",
      "C(season_enc)[T.Unknown]                  True  \n",
      "C(season_enc)[T.Winter]                   True  \n",
      "C(region_census_enc)[T.Northeast]        False  \n",
      "C(region_census_enc)[T.South]            False  \n",
      "C(region_census_enc)[T.West]              True  \n",
      "import_from_slu_log                      False  \n",
      "age                                       True  \n",
      "state_unemployment                        True  \n",
      "immigrant_density                         True  \n",
      "climate_distance                         False  \n",
      "economic_distance                        False  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nINTERPRETATION GUIDE:\\n\\n1. Clustered SEs account for correlation within states\\n   - Observations from same state may be more similar\\n   - This typically INCREASES standard errors\\n   - More conservative inference (larger p-values)\\n\\n2. When to use clustered SEs:\\n   - You have multiple observations per cluster (state)\\n   - You suspect within-cluster correlation\\n   - State-level policies might affect all observations from that state\\n\\n3. Ratio interpretation:\\n   - SE_Ratio > 1.5: Strong evidence of within-cluster correlation\\n   - SE_Ratio > 2.0: Very strong clustering effects\\n   - SE_Ratio < 1.1: Little evidence of clustering\\n\\n4. Model selection:\\n   - If clustered SEs are much larger, use them for inference\\n   - If they're similar to regular SEs, either is fine\\n   - Always report which type you used\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model with clustered SEs\n",
    "nb_results_clustered, comparison_df = compare_standard_errors(df_clean_nb, formula_simple_nostate)\n",
    "\n",
    "\n",
    "print_clustered_results(nb_results_clustered, comparison_df)\n",
    "\n",
    "# Calculate IRR with clustered confidence intervals\n",
    "irr_clustered = calculate_clustered_irr(nb_results_clustered)\n",
    "print(\"\\\\nIncident Rate Ratios with Clustered Standard Errors:\")\n",
    "print(irr_clustered.round(4))\n",
    "\n",
    "# Add clustered results to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Negative Binomial Regression with Clustered Standard Errors', level=1)\n",
    "summary_paragraph_clustered = doc.add_paragraph()\n",
    "summary_run_clustered = summary_paragraph_clustered.add_run(str(nb_results_clustered.summary()))\n",
    "summary_run_clustered.font.name = 'Courier New'  # Use monospace font\n",
    "# Add IRR table with clustered standard errors\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Incident Rate Ratios (IRR) with Clustered Standard Errors', level=2)\n",
    "irr_table_clustered = doc.add_table(rows=len(irr_clustered)+1, cols=6)\n",
    "irr_table_clustered.style = 'Table Grid'\n",
    "irr_table_clustered.cell(0, 0).text = 'Variable'\n",
    "irr_table_clustered.cell(0, 1).text = 'IRR'\n",
    "irr_table_clustered.cell(0, 2).text = 'Lower CI'\n",
    "irr_table_clustered.cell(0, 3).text = 'Upper CI'\n",
    "irr_table_clustered.cell(0, 4).text = 'P-value'\n",
    "irr_table_clustered.cell(0, 5).text = 'Significant'\n",
    "for i, (var, row) in enumerate(irr_clustered.iterrows(), 1):\n",
    "    irr_table_clustered.cell(i, 0).text = str(var)\n",
    "    irr_table_clustered.cell(i, 1).text = f\"{row['IRR']:.4f}\"\n",
    "    irr_table_clustered.cell(i, 2).text = f\"{row['Lower_CI']:.4f}\"\n",
    "    irr_table_clustered.cell(i, 3).text = f\"{row['Upper_CI']:.4f}\"\n",
    "    irr_table_clustered.cell(i, 4).text = f\"{row['P_value']:.4f}\"\n",
    "    irr_table_clustered.cell(i, 5).text = 'Yes' if row['Significant'] else 'No'\n",
    "# Add comparison of standard errors table\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Comparison of Standard Errors', level=2)\n",
    "comparison_table = doc.add_table(rows=len(comparison_df)+1, cols=7)\n",
    "comparison_table.style = 'Table Grid'\n",
    "comparison_table.cell(0, 0).text = 'Variable'\n",
    "comparison_table.cell(0, 1).text = 'Coefficient'\n",
    "comparison_table.cell(0, 2).text = 'SE Regular'\n",
    "comparison_table.cell(0, 3).text = 'SE Clustered'\n",
    "comparison_table.cell(0, 4).text = 'SE Ratio Clustered'\n",
    "comparison_table.cell(0, 5).text = 'P Regular'\n",
    "comparison_table.cell(0, 6).text = 'P Clustered'   \n",
    "for i, row in comparison_df.iterrows():\n",
    "    comparison_table.cell(i+1, 0).text = str(row['Variable'])\n",
    "    comparison_table.cell(i+1, 1).text = f\"{row['Coefficient']:.4f}\"\n",
    "    comparison_table.cell(i+1, 2).text = f\"{row['SE_Regular']:.4f}\"\n",
    "    comparison_table.cell(i+1, 3).text = f\"{row['SE_Clustered']:.4f}\"\n",
    "    comparison_table.cell(i+1, 4).text = f\"{row['SE_Ratio_Clustered']:.3f}\"\n",
    "    comparison_table.cell(i+1, 5).text = f\"{row['P_Regular']:.4f}\"\n",
    "    comparison_table.cell(i+1, 6).text = f\"{row['P_Clustered']:.4f}\"\n",
    "# Tips for interpretation:\n",
    "\"\"\"\n",
    "INTERPRETATION GUIDE:\n",
    "\n",
    "1. Clustered SEs account for correlation within states\n",
    "   - Observations from same state may be more similar\n",
    "   - This typically INCREASES standard errors\n",
    "   - More conservative inference (larger p-values)\n",
    "\n",
    "2. When to use clustered SEs:\n",
    "   - You have multiple observations per cluster (state)\n",
    "   - You suspect within-cluster correlation\n",
    "   - State-level policies might affect all observations from that state\n",
    "\n",
    "3. Ratio interpretation:\n",
    "   - SE_Ratio > 1.5: Strong evidence of within-cluster correlation\n",
    "   - SE_Ratio > 2.0: Very strong clustering effects\n",
    "   - SE_Ratio < 1.1: Little evidence of clustering\n",
    "\n",
    "4. Model selection:\n",
    "   - If clustered SEs are much larger, use them for inference\n",
    "   - If they're similar to regular SEs, either is fine\n",
    "   - Always report which type you used\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a negative relationship between imports from St Lucia and immigrant density and length of stay. In the charts/tables below we attempt to dig a bit deeper into this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a table showing top 10 states who import the most from SLU and their LOS capped and number of observations\n",
    "top_states = df_clean_nb.groupby('us_state_enc').agg({\n",
    "    'import_from_slu_log': 'sum',\n",
    "    'los_capped': 'mean',\n",
    "    'us_state_enc': 'count',\n",
    "    'immigrant_density': 'mean'\n",
    "}).rename(columns={'us_state_enc': 'num_observations'}).reset_index()\n",
    "top_states = top_states.sort_values(by='import_from_slu_log', ascending=False).head(10)\n",
    "#print the top states table\n",
    "print(\"\\nTop 10 States Importing from SLU:\")\n",
    "print(top_states)\n",
    "\n",
    "# Add top states table to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Top 10 States Importing from SLU', level=2)\n",
    "top_states_table = doc.add_table(rows=len(top_states)+1, cols=5)\n",
    "top_states_table.style = 'Table Grid'\n",
    "top_states_table.cell(0, 0).text = 'State'\n",
    "top_states_table.cell(0, 1).text = 'Total Import from SLU (Log)'\n",
    "top_states_table.cell(0, 2).text = 'Mean Length of Stay (Capped)'\n",
    "top_states_table.cell(0, 3).text = 'Number of Observations'\n",
    "top_states_table.cell(0, 4).text = 'Immigrant Density (Mean)'\n",
    "for i, row in enumerate(top_states.itertuples(), 1):\n",
    "    top_states_table.cell(i, 0).text = str(row.us_state_enc)\n",
    "    top_states_table.cell(i, 1).text = f\"{row.import_from_slu_log:.4f}\"\n",
    "    top_states_table.cell(i, 2).text = f\"{row.los_capped:.4f}\"\n",
    "    top_states_table.cell(i, 3).text = str(row.num_observations)\n",
    "    top_states_table.cell(i, 4).text = f\"{row.immigrant_density:.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_states = df_clean_nb.groupby('us_state_enc').agg({\n",
    "    'import_from_slu_log': 'sum',\n",
    "    'los_capped': 'mean',\n",
    "    'us_state_enc': 'count',\n",
    "    'immigrant_density': 'mean'\n",
    "}).rename(columns={'us_state_enc': 'num_observations'}).reset_index()\n",
    "bottom_states = bottom_states.sort_values(by='import_from_slu_log', ascending=True).head(10)\n",
    "#print the bottom states table\n",
    "print(\"\\nBottom 10 States Importing from SLU:\")\n",
    "print(bottom_states)\n",
    "\n",
    "# Add bottom states table to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Bottom 10 States Importing from SLU', level=2)\n",
    "bottom_states_table = doc.add_table(rows=len(bottom_states)+1, cols=5)\n",
    "bottom_states_table.style = 'Table Grid'\n",
    "bottom_states_table.cell(0, 0).text = 'State'\n",
    "bottom_states_table.cell(0, 1).text = 'Total Import from SLU (Log)'\n",
    "bottom_states_table.cell(0, 2).text = 'Mean Length of Stay (Capped)'\n",
    "bottom_states_table.cell(0, 3).text = 'Number of Observations'\n",
    "bottom_states_table.cell(0, 4).text = 'Immigrant Density (Mean)'\n",
    "for i, row in enumerate(bottom_states.itertuples(), 1):\n",
    "    bottom_states_table.cell(i, 0).text = str(row.us_state_enc)\n",
    "    bottom_states_table.cell(i, 1).text = f\"{row.import_from_slu_log:.4f}\"\n",
    "    bottom_states_table.cell(i, 2).text = f\"{row.los_capped:.4f}\"\n",
    "    bottom_states_table.cell(i, 3).text = str(row.num_observations)\n",
    "    bottom_states_table.cell(i, 4).text = f\"{row.immigrant_density:.4f}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a table showing top 10 states with the highest average density and their LOS capped and number of observations\n",
    "top_density_states = df_clean_nb.groupby('us_state_enc').agg({\n",
    "    'immigrant_density': 'mean',\n",
    "    'los_capped': 'mean',\n",
    "    'us_state_enc': 'count',\n",
    "    'import_from_slu_log': 'sum'\n",
    "}).rename(columns={'us_state_enc': 'num_observations'}).reset_index()\n",
    "top_density_states = top_density_states.sort_values(by='immigrant_density', ascending=False).head(10)\n",
    "#print the top density states table\n",
    "print(\"\\nTop 10 States with Highest Average Density:\")\n",
    "print(top_density_states)\n",
    "# Add top density states table to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Top 10 States with Highest Average Density', level=2)\n",
    "top_density_states_table = doc.add_table(rows=len(top_density_states)+1, cols=5)\n",
    "top_density_states_table.style = 'Table Grid'\n",
    "top_density_states_table.cell(0, 0).text = 'State'\n",
    "top_density_states_table.cell(0, 1).text = 'Immigrant Density (Mean)'\n",
    "top_density_states_table.cell(0, 2).text = 'Mean Length of Stay (Capped)'\n",
    "top_density_states_table.cell(0, 3).text = 'Number of Observations'\n",
    "top_density_states_table.cell(0, 4).text = 'Total Import from SLU (Log)'\n",
    "for i, row in enumerate(top_density_states.itertuples(), 1):\n",
    "    top_density_states_table.cell(i, 0).text = str(row.us_state_enc)\n",
    "    top_density_states_table.cell(i, 1).text = f\"{row.immigrant_density:.4f}\"\n",
    "    top_density_states_table.cell(i, 2).text = f\"{row.los_capped:.4f}\"\n",
    "    top_density_states_table.cell(i, 3).text = str(row.num_observations)\n",
    "    top_density_states_table.cell(i, 4).text = f\"{row.import_from_slu_log:.4f}\"      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_density_states = df_clean_nb.groupby('us_state_enc').agg({\n",
    "    'immigrant_density': 'mean',\n",
    "    'los_capped': 'mean',\n",
    "    'us_state_enc': 'count',\n",
    "    'import_from_slu_log': 'sum'\n",
    "}).rename(columns={'us_state_enc': 'num_observations'}).reset_index()\n",
    "bottom_density_states = bottom_density_states.sort_values(by='immigrant_density', ascending=True).head(10)\n",
    "#print the bottom density states table\n",
    "print(\"\\nBottom 10 States with Highest Average Density:\")\n",
    "print(bottom_density_states)\n",
    "# Add bottom density states table to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Bottom 10 States with Highest Average Density', level=2)\n",
    "bottom_density_states_table = doc.add_table(rows=len(bottom_density_states)+1, cols=5)\n",
    "bottom_density_states_table.style = 'Table Grid'\n",
    "bottom_density_states_table.cell(0, 0).text = 'State'\n",
    "bottom_density_states_table.cell(0, 1).text = 'Immigrant Density (Mean)'\n",
    "bottom_density_states_table.cell(0, 2).text = 'Mean Length of Stay (Capped)'\n",
    "bottom_density_states_table.cell(0, 3).text = 'Number of Observations'\n",
    "bottom_density_states_table.cell(0, 4).text = 'Total Import from SLU (Log)'\n",
    "for i, row in enumerate(bottom_density_states.itertuples(), 1):\n",
    "    bottom_density_states_table.cell(i, 0).text = str(row.us_state_enc)\n",
    "    bottom_density_states_table.cell(i, 1).text = f\"{row.immigrant_density:.4f}\"\n",
    "    bottom_density_states_table.cell(i, 2).text = f\"{row.los_capped:.4f}\"\n",
    "    bottom_density_states_table.cell(i, 3).text = str(row.num_observations)\n",
    "    bottom_density_states_table.cell(i, 4).text = f\"{row.import_from_slu_log:.4f}\"    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a table showing top 10 states by number of arrivals (observations), with their average LOS, immigrant density, and total imports from SLU\n",
    "top_arrival_states = df_clean_nb.groupby('us_state_enc').agg({\n",
    "    'los_capped': 'mean',\n",
    "    'immigrant_density': 'mean',\n",
    "    'import_from_slu_log': 'sum',\n",
    "    'us_state_enc': 'count'\n",
    "}).rename(columns={'us_state_enc': 'num_arrivals'}).reset_index()\n",
    "top_arrival_states = top_arrival_states.sort_values(by='num_arrivals', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 States by Number of Arrivals:\")\n",
    "print(top_arrival_states)\n",
    "\n",
    "# Add top arrival states table to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Top 10 States by Number of Arrivals', level=2)\n",
    "top_arrival_states_table = doc.add_table(rows=len(top_arrival_states)+1, cols=5)\n",
    "top_arrival_states_table.style = 'Table Grid'\n",
    "top_arrival_states_table.cell(0, 0).text = 'State'\n",
    "top_arrival_states_table.cell(0, 1).text = 'Number of Arrivals'\n",
    "top_arrival_states_table.cell(0, 2).text = 'Mean Length of Stay (Capped)'\n",
    "top_arrival_states_table.cell(0, 3).text = 'Immigrant Density (Mean)'\n",
    "top_arrival_states_table.cell(0, 4).text = 'Total Import from SLU (Log)'\n",
    "for i, row in enumerate(top_arrival_states.itertuples(), 1):\n",
    "    top_arrival_states_table.cell(i, 0).text = str(row.us_state_enc)\n",
    "    top_arrival_states_table.cell(i, 1).text = str(row.num_arrivals)\n",
    "    top_arrival_states_table.cell(i, 2).text = f\"{row.los_capped:.4f}\"\n",
    "    top_arrival_states_table.cell(i, 3).text = f\"{row.immigrant_density:.4f}\"\n",
    "    top_arrival_states_table.cell(i, 4).text = f\"{row.import_from_slu_log:.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the average length of stay (LOS) capped between the top 10 states and bottom 10 states\n",
    "top_avg_los = top_states['los_capped'].mean()\n",
    "bottom_avg_los = bottom_states['los_capped'].mean()\n",
    "print(f\"\\nAverage Length of Stay (Capped) - Top 10 States: {top_avg_los:.4f}\")\n",
    "print(f\"Average Length of Stay (Capped) - Bottom 10 States: {bottom_avg_los:.4f}\")\n",
    "\n",
    "#compare the average length of stay (LOS) capped between the top 10 states by density and bottom 10 states by density\n",
    "top_density_avg_los = top_density_states['los_capped'].mean()\n",
    "bottom_density_avg_los = bottom_density_states['los_capped'].mean()\n",
    "print(f\"\\nAverage Length of Stay (Capped) - Top 10 States by Density: {top_density_avg_los:.4f}\")\n",
    "print(f\"Average Length of Stay (Capped) - Bottom 10 States by Density: {bottom_density_avg_los:.4f}\")\n",
    "print(f\"\\nAverage Length of Stay (Capped) - Top 10 States by Arrivals: {top_arrival_states['los_capped'].mean():.4f}\")\n",
    "print(f\"Average Length of Stay (Capped) - Bottom 10 States by Arrivals: {bottom_states['los_capped'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prints all results into one word document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please select where to save the Word document...\n",
      "Analysis report saved to: /Users/janai/Library/CloudStorage/OneDrive-SharedLibraries-jlconsulting.llc/Projects - Documents/Research/Saint Lucia Tourism Piece/30.06.2025 results with names.docx\n",
      "\n",
      "Analysis complete.\n"
     ]
    }
   ],
   "source": [
    "# Save the Word document\n",
    "print(\"\\nPlease select where to save the Word document...\")\n",
    "doc_path = select_file(\n",
    "    \"Save Analysis Report As\", \n",
    "    [(\"Word Document\", \"*.docx\"), (\"All files\", \"*.*\")],\n",
    "    save=True\n",
    ")\n",
    "\n",
    "if doc_path:\n",
    "    if not doc_path.endswith('.docx'):\n",
    "        doc_path += '.docx'\n",
    "    doc.save(doc_path)\n",
    "    print(f\"Analysis report saved to: {doc_path}\")\n",
    "else:\n",
    "    print(\"Document not saved as no location was selected.\")\n",
    "\n",
    "print(\"\\nAnalysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jlslu2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
