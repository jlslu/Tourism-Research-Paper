{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Assessment of the length of stay in Saint Lucia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code chunk loads the data and performs the following cleaning techniques:\n",
    "    removes instances where there are blank values which are codied as -1\n",
    "    removes ages more than 100 years\n",
    "    defines tourist as between 1-365 nights\n",
    "    streamlines purpose of visit\n",
    "    logs variables (exepect length of stay and age)\n",
    "    makes a list of continous and categorical variables\n",
    "    converts month of stay to a seasonal variable\n",
    "    creates a tourist type variable\n",
    "\n",
    "Notes:\n",
    "    The tourist type variable is defined as St Lucian born if someone was born in Saint Lucia but a resident of the US and US born if they were both born and a resident of the US\n",
    "    all persons in the file are over the age of 18\n",
    "    Immigrant density is defined as st lucian immigrants per 100,000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.genmod.families.family import NegativeBinomial\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from scipy import stats\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from io import BytesIO\n",
    "import statsmodels.discrete.discrete_model as discrete\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "\n",
    "def select_file(title, file_types, save=False):\n",
    "    \"\"\"Allow user to select a file\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes('-topmost', True)\n",
    "    \n",
    "    try:\n",
    "        if save:\n",
    "            file_path = filedialog.asksaveasfilename(\n",
    "                title=title,\n",
    "                filetypes=file_types,\n",
    "                defaultextension=file_types[0][1]\n",
    "            )\n",
    "        else:\n",
    "            file_path = filedialog.askopenfilename(\n",
    "                title=title,\n",
    "                filetypes=file_types\n",
    "            )\n",
    "    finally:\n",
    "        root.destroy()\n",
    "    \n",
    "    return file_path if file_path else None\n",
    "\n",
    "# Allow user to select input file\n",
    "print(\"Please select the input Excel file...\")\n",
    "file_path = select_file(\n",
    "    \"Select Excel Data File\", \n",
    "    [(\"Excel files\", \"*.xlsx *.xls\"), (\"All files\", \"*.*\")]\n",
    ")\n",
    "\n",
    "if not file_path:\n",
    "    print(\"No file selected. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Import the data\n",
    "print(f\"Loading data from: {file_path}\")\n",
    "df = pd.read_excel(file_path, sheet_name=\"Sheet\")\n",
    "\n",
    "# Setup\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Remove instances where tourist type is \"SLU born tourist\"\n",
    "if 'tourist_type' in df.columns:\n",
    "    df = df[df['tourist_type'] != 'SLU born tourist']\n",
    "    \n",
    "\n",
    "# Encode categorical variables if not already encoded\n",
    "categorical_vars = ['sex', 'marital_status', 'employment_status', 'purpose', 'accomd_type', 'us_state','tourist_type','region_census', 'region_climate']\n",
    "encoded_vars = {}\n",
    "\n",
    "for var in categorical_vars:\n",
    "    if var in df.columns:\n",
    "        # Check if variable is already numeric\n",
    "        if not pd.api.types.is_numeric_dtype(df[var]):\n",
    "            new_var = f\"{var}_enc\"\n",
    "            df[new_var] = pd.Categorical(df[var]).codes\n",
    "            encoded_vars[var] = new_var\n",
    "        else:\n",
    "            encoded_vars[var] = var\n",
    "\n",
    "# Set the truncation point for los (assuming truncation at 0)\n",
    "df['los_trunc'] = df['los'].copy()\n",
    "df.loc[df['los_trunc'] <= 0, 'los_trunc'] = np.nan\n",
    "\n",
    "# Check for missing data\n",
    "print(\"\\nMissing data summary:\")\n",
    "missing_data_summary = df.isnull().sum()\n",
    "print(missing_data_summary)\n",
    "\n",
    "print(\"\\nMissing data patterns:\")\n",
    "missing_patterns = df.isnull().sum(axis=1)\n",
    "missing_patterns_counts = missing_patterns.value_counts().sort_index()\n",
    "print(missing_patterns_counts)\n",
    "\n",
    "# Visualize los distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['los_trunc'], discrete=True)\n",
    "plt.title('Histogram of Length of Stay')\n",
    "plt.tight_layout()\n",
    "los_hist_img = BytesIO()\n",
    "plt.savefig(los_hist_img, format='png')\n",
    "los_hist_img.seek(0)\n",
    "plt.close()\n",
    "\n",
    "# Summarize los by purpose\n",
    "purpose_stats = None\n",
    "if 'purpose_enc' in df.columns:\n",
    "    print(\"\\nLength of stay by purpose:\")\n",
    "    purpose_stats = df.groupby('purpose_enc')['los_trunc'].agg(['count', 'mean', 'median', 'min', 'max', 'std'])\n",
    "    print(purpose_stats)\n",
    "\n",
    "# Detailed summary of los_trunc\n",
    "print(\"\\nDetailed summary of length of stay:\")\n",
    "los_describe = df['los_trunc'].describe(percentiles=[.25, .5, .75, .90, .95, .99])\n",
    "print(los_describe)\n",
    "\n",
    "# Based on month travel create a new variable called seasons i.e. winter, spring, summer, fall\n",
    "# Define seasons based on month_travel if it is a number\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df['season'] = df['month_travel'].apply(lambda x: get_season(x) if pd.notnull(x) and isinstance(x, (int, float)) else np.nan)\n",
    "\n",
    "#encoding the season variable\n",
    "if 'season' in df.columns:\n",
    "    df['season_enc'] = pd.Categorical(df['season']).codes\n",
    "\n",
    "# Cleaning process\n",
    "# Step 1: Drop missing datapoints for key variables\n",
    "key_vars = ['los', 'immigrant_population', 'import_from_slu', 'age', 'climate_distance',\n",
    "            encoded_vars.get('sex', 'sex_enc'), \n",
    "            encoded_vars.get('marital_status', 'marital_status_enc'), \n",
    "            encoded_vars.get('employment_status', 'employment_status_enc'), \n",
    "            'distance_miles', \n",
    "            encoded_vars.get('purpose', 'purpose_enc'), \n",
    "            encoded_vars.get('accomd_type', 'accomd_type_enc'), \n",
    "            encoded_vars.get('tourist_type', 'tourist_type_enc'),\n",
    "            encoded_vars.get('region_census', 'region_census_enc'),\n",
    "            encoded_vars.get('region_climate', 'region_climate_enc'),\n",
    "            'state_percapita_income', 'state_unemployment','immigrant_density', 'season_enc']\n",
    "           \n",
    "             \n",
    "\n",
    "\n",
    "\n",
    "#Now that we have the encoded variables, we need to remove blank and -1 values\n",
    "# Remove rows where sex_enc is -1\n",
    "df = df[df['sex_enc'] != -1]\n",
    "# Remove rows where age > 100\n",
    "df = df[df['age'] <= 100]  \n",
    "# Remove rows where marital_status_enc is -1\n",
    "df= df[df['marital_status_enc'] != -1]\n",
    "\n",
    "#Remove rows where employment_status_enc is -1\n",
    "df = df[df['employment_status_enc'] != -1]\n",
    "\n",
    "# Count missing values per row for key variables\n",
    "df['missing'] = df[key_vars].isnull().sum(axis=1)\n",
    "print(\"\\nNumber of missing values per observation:\")\n",
    "missing_values_count = df['missing'].value_counts().sort_index()\n",
    "print(missing_values_count)\n",
    "\n",
    "# Drop observations with missing values in key variables\n",
    "df_clean = df[df['missing'] == 0].drop('missing', axis=1)\n",
    "print(f\"\\nRemaining observations after dropping missing values: {len(df_clean)}\")\n",
    "\n",
    "# Step 2: Drop outliers in length of stay\n",
    "#02 June 2025 edit removed the windorization of los_trunc\n",
    "#los_p95 = np.percentile(df_clean['los_trunc'].dropna(), 95)\n",
    "df_clean['los_capped'] = df_clean['los_trunc'].copy()\n",
    "#df_clean.loc[df_clean['los_capped'] > los_p95, 'los_capped'] = los_p95\n",
    "\n",
    "#df_clean = df_clean[df_clean['los_trunc'] <= los_p95]\n",
    "print(f\"After filtering to 95th percentile, remaining observations: {len(df_clean)}\")\n",
    "\n",
    "# Remove any instance of los_capped that is less than 2. Prior to this, over 1000 persons had stays less than 1 including honeymooners. This appears to be a data entry error.\n",
    "#02 June 2025 edit use the tourism definition of length of stay where its more than 1 day and less than 366 days\n",
    "\n",
    "df_clean = df_clean[(df_clean['los_capped'] > 1) & (df_clean['los_capped'] < 366)]  # Keep stays more than 1 and less than 366 days\n",
    "\n",
    "#remove instances where sex_enc is -1\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the capped los distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_clean['los_capped'], discrete=True)\n",
    "plt.title('Histogram of Capped Length of Stay')\n",
    "plt.tight_layout()\n",
    "los_capped_img = BytesIO()\n",
    "plt.savefig(los_capped_img, format='png')\n",
    "los_capped_img.seek(0)\n",
    "plt.close()\n",
    "\n",
    "# Step 3: Clean up the purpose of trip column\n",
    "# Create a new simplified purpose variable\n",
    "purpose_mapping = {\n",
    "    1: 1,  # BUSINESS/MEETING -> Business\n",
    "    2: 1,  # CONVENTION -> Business\n",
    "    3: 1,  # CREW -> Business\n",
    "    5: 2,  # EVENT -> Events\n",
    "    6: 2,  # EVENTS -> Events\n",
    "    7: 4,  # HONEYMOON -> Pleasure\n",
    "    8: 5,  # INTRANSIT PASSEN -> Other\n",
    "    9: 5,  # OTHER -> Other\n",
    "    10: 4, # PLEASURE/HOLIDAY -> Pleasure\n",
    "    11: 5, # RESIDENT -> Other\n",
    "    12: 2, # SAINT LUCIA CARN -> Events\n",
    "    13: 2, # SAINT LUCIA JAZZ -> Events\n",
    "    14: 5, # SPORTS -> Other\n",
    "    15: 5, # STUDY -> Other\n",
    "    16: 5, # VISITING FRIENDS -> Other\n",
    "    17: 3, # WEDDING -> Wedding\n",
    "    18: 4, # pLEASURE/HOLIDAY -> Pleasure\n",
    "    4: 5,  # CRICKET -> Other\n",
    "}\n",
    "\n",
    "purpose_labels = {\n",
    "    1: \"Business\",\n",
    "    2: \"Events\",\n",
    "    3: \"Wedding\",\n",
    "    4: \"Pleasure\",\n",
    "    5: \"Other\"\n",
    "}\n",
    "\n",
    "# Add the simplified purpose variable\n",
    "purpose_enc_col = encoded_vars.get('purpose', 'purpose_enc')\n",
    "df_clean['purpose_simple'] = df_clean[purpose_enc_col].map(purpose_mapping)\n",
    "\n",
    "# Check the new variable\n",
    "print(\"\\nPurpose simple distribution:\")\n",
    "purpose_counts = df_clean['purpose_simple'].value_counts().sort_index()\n",
    "purpose_distribution = []\n",
    "for code, count in purpose_counts.items():\n",
    "    purpose_line = f\"{code} ({purpose_labels.get(code, 'Unknown')}): {count}\"\n",
    "    purpose_distribution.append(purpose_line)\n",
    "    print(purpose_line)\n",
    "\n",
    "# Create a Word document for output\n",
    "doc = Document()\n",
    "doc.add_heading('Multilevel Truncated Negative Binomial Regression for Length of Stay Analysis', 0)\n",
    "doc.add_heading('Data Preparation and Cleaning', level=1)\n",
    "\n",
    "\n",
    "\n",
    "# Add Length of Stay histogram\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Length of Stay Distribution', level=2)\n",
    "doc.add_picture(los_hist_img, width=Inches(6))\n",
    "doc.add_paragraph('Figure 1: Histogram of Length of Stay (Before Capping)')\n",
    "\n",
    "# Add Capped LOS histogram\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Capped Length of Stay Distribution', level=2)\n",
    "doc.add_picture(los_capped_img, width=Inches(6))\n",
    "doc.add_paragraph('Figure 2: Histogram of Length of Stay (After Capping at 95th Percentile)')\n",
    "\n",
    "# Add LOS summary statistics\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Length of Stay Summary Statistics', level=2)\n",
    "los_stats_table = doc.add_table(rows=len(los_describe)+1, cols=2)\n",
    "los_stats_table.style = 'Table Grid'\n",
    "los_stats_table.cell(0, 0).text = 'Statistic'\n",
    "los_stats_table.cell(0, 1).text = 'Value'\n",
    "for i, (stat, value) in enumerate(los_describe.items(), 1):\n",
    "    los_stats_table.cell(i, 0).text = str(stat)\n",
    "    los_stats_table.cell(i, 1).text = f\"{value:.4f}\" if isinstance(value, (int, float)) else str(value)\n",
    "\n",
    "# Add Purpose distribution\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Purpose of Visit Distribution', level=2)\n",
    "purpose_table = doc.add_table(rows=len(purpose_distribution)+1, cols=1)\n",
    "purpose_table.style = 'Table Grid'\n",
    "purpose_table.cell(0, 0).text = 'Purpose Category'\n",
    "for i, purpose_text in enumerate(purpose_distribution, 1):\n",
    "    purpose_table.cell(i, 0).text = purpose_text\n",
    "\n",
    "# Log continuous variables\n",
    "# Convert columns to numeric before log transformation\n",
    "for col in ['immigrant_population', 'import_from_slu', 'distance_miles', 'state_percapita_income']:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "#df_clean['age'] = np.log1p(df_clean['age'])\n",
    "df_clean['immigrant_population_log'] = np.log1p(df_clean['immigrant_population'])\n",
    "df_clean['distance_miles_log'] = np.log1p(df_clean['distance_miles'])\n",
    "df_clean['state_percapita_income_log'] = np.log1p(df_clean['state_percapita_income'])\n",
    "df_clean['import_from_slu_log'] = np.log1p(df_clean['import_from_slu'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code chunk runs three negative binomials. In each case all continous data is in logs form with the exception of length of stay, age and continous variables which are ratio's such as the unemployment rate. \n",
    "\n",
    "The first negative binomial runs all variables. In that model we see that the VIF for age, distance and state per capita exceeds 100. We also note that the state per capita variable is insignificant. Marital status and employment status are insignificant. \n",
    " \n",
    "\n",
    "The second negative binomial attempts to address the issues with the first by removing insignificant and highly correlated variables\n",
    "\n",
    "The third model is the same as the second with the exception that it removes the categorical variable US state and reestimates the standard errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit simple negative binomial regression with continuous variables correctly specified\n",
    "print(\"\\nFitting simple negative binomial regression model...\")\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Negative Binomial Regression Model', level=1)\n",
    "\n",
    "# Define continuous variables and create proper formula\n",
    "continuous_vars = ['immigrant_population_log', 'import_from_slu_log', 'age', 'distance_miles_log', \n",
    "                   'state_percapita_income_log', 'state_unemployment','immigrant_density','climate_distance']\n",
    "\n",
    "# Make sure all continuous variables are properly formatted as numeric\n",
    "for var in continuous_vars:\n",
    "    if var in df_clean.columns:\n",
    "        df_clean[var] = pd.to_numeric(df_clean[var], errors='coerce')\n",
    "\n",
    "# Create formula with continuous variables properly treated\n",
    "formula_parts = []\n",
    "for var in continuous_vars:\n",
    "    if var in df_clean.columns:\n",
    "        formula_parts.append(var)\n",
    "\n",
    "# Add categorical variables with proper C() notation\n",
    "categorical_model_vars = ['sex_enc', 'marital_status_enc', 'employment_status_enc', \n",
    "                         'purpose_simple', 'accomd_type_enc', 'season_enc', 'us_state_enc','region_census_enc', 'region_climate_enc']\n",
    "\n",
    "for var in categorical_model_vars:\n",
    "    if var in df_clean.columns:\n",
    "        # Use the encoded variable name or the original if available\n",
    "        var_to_use = var\n",
    "        formula_parts.append(f\"C({var_to_use})\")\n",
    "\n",
    "# Combine into final formula\n",
    "formula = 'los_capped ~ ' + ' + '.join(formula_parts)\n",
    "print(f\"Model formula: {formula}\")\n",
    "\n",
    "# Add formula to document\n",
    "doc.add_paragraph(f\"Model formula: {formula}\")\n",
    "\n",
    "# Drop rows with missing values in formula variables\n",
    "\n",
    "formula_vars = ['los_capped'] + continuous_vars + categorical_model_vars\n",
    "df_clean_nb = df_clean[formula_vars].dropna()\n",
    "df_clean_nb = df_clean_nb.reset_index(drop=True)\n",
    "print(f\"Number of rows in df_clean_nb after dropping missing values: {len(df_clean_nb)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit negative binomial model with all variables\n",
    "nb_model = smf.glm(formula=formula, \n",
    "                  data=df_clean_nb, \n",
    "                  family=sm.families.NegativeBinomial(link=sm.families.links.log()))\n",
    "\n",
    "try:\n",
    "    nb_results = nb_model.fit()\n",
    "    print(\"\\nNegative Binomial Regression Results:\")\n",
    "    summary_text = str(nb_results.summary())\n",
    "    print(summary_text)\n",
    "    \n",
    "    # Add model summary to document\n",
    "    #doc.add_paragraph('\\nModel Summary:')\n",
    "    summary_paragraph_neg = doc.add_paragraph()\n",
    "    summary_run_neg = summary_paragraph_neg.add_run(summary_text)\n",
    "    summary_run_neg.font.name = 'Courier New'  # Use monospace font\n",
    "    #for line in summary_text.split('\\n'):\n",
    "        #doc.add_paragraph(line)\n",
    "    \n",
    "    # Convert coefficients to incident rate ratios (IRR)\n",
    "    print(\"\\nIncident Rate Ratios (IRR):\")\n",
    "    irr = np.exp(nb_results.params)\n",
    "    irr_conf = np.exp(nb_results.conf_int())\n",
    "    irr_df = pd.DataFrame({'IRR': irr, 'Lower CI': irr_conf[0], 'Upper CI': irr_conf[1], \n",
    "                          'P-value': nb_results.pvalues})\n",
    "    print(irr_df)\n",
    "    \n",
    "    # Add IRR table to document\n",
    "    doc.add_paragraph('\\n')\n",
    "    doc.add_heading('Incident Rate Ratios (IRR)', level=2)\n",
    "    irr_table = doc.add_table(rows=len(irr_df)+1, cols=5)\n",
    "    irr_table.style = 'Table Grid'\n",
    "    irr_table.cell(0, 0).text = 'Variable'\n",
    "    irr_table.cell(0, 1).text = 'IRR'\n",
    "    irr_table.cell(0, 2).text = 'Lower CI'\n",
    "    irr_table.cell(0, 3).text = 'Upper CI'\n",
    "    irr_table.cell(0, 4).text = 'P-value'\n",
    "    \n",
    "    for i, (var, row) in enumerate(irr_df.iterrows(), 1):\n",
    "        irr_table.cell(i, 0).text = str(var)\n",
    "        irr_table.cell(i, 1).text = f\"{row['IRR']:.4f}\"\n",
    "        irr_table.cell(i, 2).text = f\"{row['Lower CI']:.4f}\"\n",
    "        irr_table.cell(i, 3).text = f\"{row['Upper CI']:.4f}\"\n",
    "        irr_table.cell(i, 4).text = f\"{row['P-value']:.4f}\"\n",
    "    \n",
    "    \n",
    "    # Predictions and diagnostics\n",
    "    df_clean_nb['predicted'] = nb_results.predict()\n",
    "    df_clean_nb['residuals'] = df_clean_nb['los_capped'] - df_clean_nb['predicted']\n",
    "    \n",
    "    # Plot residuals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_clean_nb['predicted'], df_clean_nb['residuals'], alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot')\n",
    "    plt.tight_layout()\n",
    "    residuals_img = BytesIO()\n",
    "    plt.savefig(residuals_img, format='png')\n",
    "    residuals_img.seek(0)\n",
    "    plt.close()\n",
    "    \n",
    "    # Add residuals plot to document\n",
    "    doc.add_paragraph('\\n')\n",
    "    doc.add_heading('Diagnostics', level=2)\n",
    "    doc.add_picture(residuals_img, width=Inches(6))\n",
    "    doc.add_paragraph('Figure 3: Residuals Plot')\n",
    "\n",
    "    #Add VIF for continuous variables and categorical variables\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = df_clean_nb[continuous_vars + categorical_model_vars].columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df_clean_nb[continuous_vars + categorical_model_vars].values, i) \n",
    "                       for i in range(df_clean_nb[continuous_vars + categorical_model_vars].shape[1])]\n",
    "    print(\"VIF for continuous and dummy variables:\")\n",
    "    print(vif_data)\n",
    "\n",
    "    #Add VIF table to document\n",
    "    doc.add_paragraph('\\nVariance Inflation Factor (VIF) for Continuous and Categorical Variables:')    \n",
    "    vif_table = doc.add_table(rows=len(vif_data)+1, cols=2)\n",
    "    vif_table.style = 'Table Grid'\n",
    "    vif_table.cell(0, 0).text = 'Variable'\n",
    "    vif_table.cell(0, 1).text = 'VIF'\n",
    "    for i, (var, vif) in enumerate(zip(vif_data[\"Variable\"], vif_data[\"VIF\"]), 1):\n",
    "        vif_table.cell(i, 0).text = str(var)\n",
    "        vif_table.cell(i, 1).text = f\"{vif:.4f}\"\n",
    "    doc.add_paragraph('VIF values above 10 indicate potential multicollinearity issues.')\n",
    "#except Exception as e:\n",
    "    #error_msg = f\"Error fitting negative binomial model: {str(e)}\"\n",
    "    #print(error_msg)\n",
    "    #doc.add_paragraph(error_msg)\n",
    "    #doc.add_paragraph(\"The negative binomial regression model failed to converge. This can happen due to \" +\n",
    "                     #\"insufficient variation in the data or other model specification issues.\")\n",
    "    \n",
    "\n",
    "# Fit a negative binomial model with fewer variables\n",
    "\n",
    "#New continuous variables for the simpler model\n",
    "    continuous_vars_simple = [ 'import_from_slu_log', 'age', 'distance_miles_log', \n",
    "                    'state_unemployment','immigrant_density','climate_distance']\n",
    "\n",
    "#New categorical variables for the simpler model\n",
    "\n",
    "    categorical_model_vars_simple = ['sex_enc',  \n",
    "                         'purpose_simple', 'accomd_type_enc', 'season_enc', 'us_state_enc']\n",
    "    \n",
    "    categorical_model_vars_simple_nostate= ['sex_enc',  \n",
    "                         'purpose_simple', 'accomd_type_enc', 'season_enc',  'region_census_enc']\n",
    "    \n",
    "#New formula for the simpler model\n",
    "    formula_parts_simple = []\n",
    "    for var in continuous_vars_simple:\n",
    "        if var in df_clean_nb.columns:\n",
    "            formula_parts_simple.append(var)\n",
    "    # Add categorical variables with proper C() notation\n",
    "    for var in categorical_model_vars_simple:\n",
    "        if var in df_clean_nb.columns:\n",
    "            # Use the encoded variable name or the original if available\n",
    "            var_to_use = var\n",
    "            formula_parts_simple.append(f\"C({var_to_use})\")\n",
    "    # Combine into final formula\n",
    "    formula_simple = 'los_capped ~ ' + ' + '.join(formula_parts_simple)\n",
    "    print(f\"Model formula for simpler model: {formula_simple}\")\n",
    "    # Add formula to document\n",
    "    doc.add_paragraph(f\"Model formula for simpler model: {formula_simple}\")\n",
    "\n",
    "#New formula for the simpler model without the state variable\n",
    "    formula_parts_simple_nostate = []\n",
    "    for var in continuous_vars_simple:\n",
    "        if var in df_clean_nb.columns:\n",
    "            formula_parts_simple_nostate.append(var)\n",
    "    # Add categorical variables with proper C() notation\n",
    "    for var in categorical_model_vars_simple_nostate:\n",
    "        if var in df_clean_nb.columns:\n",
    "            # Use the encoded variable name or the original if available\n",
    "            var_to_use = var\n",
    "            formula_parts_simple_nostate.append(f\"C({var_to_use})\")\n",
    "    # Combine into final formula\n",
    "    formula_simple_nostate = 'los_capped ~ ' + ' + '.join(formula_parts_simple_nostate)\n",
    "    print(f\"Model formula for simpler model without state: {formula_simple_nostate}\")\n",
    "\n",
    "# Fit negative binomial model with fewer variables\n",
    "    nb_model_simple = smf.glm(formula=formula_simple, \n",
    "                          data=df_clean_nb, \n",
    "                          family=sm.families.NegativeBinomial(link=sm.families.links.log()))\n",
    "    try:\n",
    "        nb_results_simple = nb_model_simple.fit()\n",
    "        print(\"\\nNegative Binomial Regression Results (Simpler Model):\")\n",
    "        summary_text_simple = str(nb_results_simple.summary())\n",
    "        print(summary_text_simple)\n",
    "        \n",
    "        # Add model summary to document\n",
    "        summary_paragraph_neg_simple = doc.add_paragraph()\n",
    "        summary_run_neg_simple = summary_paragraph_neg_simple.add_run(summary_text_simple)\n",
    "        summary_run_neg_simple.font.name = 'Courier New'  # Use monospace font\n",
    "        \n",
    "        # Convert coefficients to incident rate ratios (IRR)\n",
    "        print(\"\\nIncident Rate Ratios (IRR) for Simpler Model:\")\n",
    "        irr_simple = np.exp(nb_results_simple.params)\n",
    "        irr_conf_simple = np.exp(nb_results_simple.conf_int())\n",
    "        irr_df_simple = pd.DataFrame({'IRR': irr_simple, 'Lower CI': irr_conf_simple[0], 'Upper CI': irr_conf_simple[1], \n",
    "                                    'P-value': nb_results_simple.pvalues})\n",
    "        print(irr_df_simple)\n",
    "        \n",
    "        # Add IRR table to document\n",
    "        doc.add_paragraph('\\n')\n",
    "        doc.add_heading('Incident Rate Ratios (IRR) - Simpler Model', level=2)\n",
    "        irr_table_simple = doc.add_table(rows=len(irr_df_simple)+1, cols=5)\n",
    "        irr_table_simple.style = 'Table Grid'\n",
    "        irr_table_simple.cell(0, 0).text = 'Variable'\n",
    "        irr_table_simple.cell(0, 1).text = 'IRR'\n",
    "        irr_table_simple.cell(0, 2).text = 'Lower CI'\n",
    "        irr_table_simple.cell(0, 3).text = 'Upper CI'\n",
    "        irr_table_simple.cell(0, 4).text = 'P-value'\n",
    "        \n",
    "        for i, (var, row) in enumerate(irr_df_simple.iterrows(), 1):\n",
    "            irr_table_simple.cell(i, 0).text = str(var)\n",
    "            irr_table_simple.cell(i, 1).text = f\"{row['IRR']:.4f}\"\n",
    "            irr_table_simple.cell(i, 2).text = f\"{row['Lower CI']:.4f}\"\n",
    "            irr_table_simple.cell(i, 3).text = f\"{row['Upper CI']:.4f}\"\n",
    "            irr_table_simple.cell(i, 4).text = f\"{row['P-value']:.4f}\"\n",
    "        # Predictions and diagnostics for simpler model\n",
    "        df_clean_nb['predicted_simple'] = nb_results_simple.predict()\n",
    "        df_clean_nb['residuals_simple'] = df_clean_nb['los_capped'] - df_clean_nb['predicted_simple']\n",
    "        # Plot residuals for simpler model\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(df_clean_nb['predicted_simple'], df_clean_nb['residuals_simple'], alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='-')\n",
    "        plt.xlabel('Predicted Values (Simpler Model)')\n",
    "        plt.ylabel('Residuals (Simpler Model)')\n",
    "        plt.title('Residual Plot (Simpler Model)')\n",
    "        plt.tight_layout()\n",
    "        residuals_img_simple = BytesIO()\n",
    "        plt.savefig(residuals_img_simple, format='png')\n",
    "        residuals_img_simple.seek(0)\n",
    "        plt.close()\n",
    "        # Add residuals plot for simpler model to document\n",
    "        doc.add_paragraph('\\n')\n",
    "        doc.add_heading('Diagnostics - Simpler Model', level=2)\n",
    "        doc.add_picture(residuals_img_simple, width=Inches(6))\n",
    "        doc.add_paragraph('Figure 4: Residuals Plot (Simpler Model)')\n",
    "        # Add VIF for continuous variables and categorical variables for simpler model\n",
    "        vif_data_simple = pd.DataFrame()\n",
    "        vif_data_simple[\"Variable\"] = df_clean_nb[continuous_vars_simple + categorical_model_vars_simple].columns\n",
    "        vif_data_simple[\"VIF\"] = [variance_inflation_factor(df_clean_nb[continuous_vars_simple + categorical_model_vars_simple].values, i) \n",
    "                                for i in range(df_clean_nb[continuous_vars_simple + categorical_model_vars_simple].shape[1])]\n",
    "        print(\"VIF for continuous and dummy variables (Simpler Model):\")\n",
    "        print(vif_data_simple)\n",
    "        # Add VIF table for simpler model to document\n",
    "        doc.add_paragraph('\\nVariance Inflation Factor (VIF) for Continuous and Categorical Variables - Simpler Model:')\n",
    "        vif_table_simple = doc.add_table(rows=len(vif_data_simple)+1, cols=2)\n",
    "        vif_table_simple.style = 'Table Grid'\n",
    "        vif_table_simple.cell(0, 0).text = 'Variable'\n",
    "        vif_table_simple.cell(0, 1).text = 'VIF'\n",
    "        for i, (var, vif) in enumerate(zip(vif_data_simple[\"Variable\"], vif_data_simple[\"VIF\"]), 1):\n",
    "            vif_table_simple.cell(i, 0).text = str(var)\n",
    "            vif_table_simple.cell(i, 1).text = f\"{vif:.4f}\"\n",
    "        doc.add_paragraph('VIF values above 10 indicate potential multicollinearity issues in the simpler model.')\n",
    "\n",
    "        #start of third model without the state variable\n",
    "        nb_model_simple = smf.glm(formula=formula_simple_nostate, \n",
    "                          data=df_clean_nb, \n",
    "                          family=sm.families.NegativeBinomial(link=sm.families.links.log()))\n",
    "        try:\n",
    "            nb_results_simple = nb_model_simple.fit()\n",
    "            print(\"\\nNegative Binomial Regression Results (Simpler Model):\")\n",
    "            summary_text_simple = str(nb_results_simple.summary())\n",
    "            print(summary_text_simple)\n",
    "            \n",
    "            # Add model summary to document\n",
    "            summary_paragraph_neg_simple = doc.add_paragraph()\n",
    "            summary_run_neg_simple = summary_paragraph_neg_simple.add_run(summary_text_simple)\n",
    "            summary_run_neg_simple.font.name = 'Courier New'  # Use monospace font\n",
    "            \n",
    "            # Convert coefficients to incident rate ratios (IRR)\n",
    "            print(\"\\nIncident Rate Ratios (IRR) for Simpler Model:\")\n",
    "            irr_simple = np.exp(nb_results_simple.params)\n",
    "            irr_conf_simple = np.exp(nb_results_simple.conf_int())\n",
    "            irr_df_simple = pd.DataFrame({'IRR': irr_simple, 'Lower CI': irr_conf_simple[0], 'Upper CI': irr_conf_simple[1], \n",
    "                                        'P-value': nb_results_simple.pvalues})\n",
    "            print(irr_df_simple)\n",
    "            \n",
    "            # Add IRR table to document\n",
    "            doc.add_paragraph('\\n')\n",
    "            doc.add_heading('Incident Rate Ratios (IRR) - Simpler Model', level=2)\n",
    "            irr_table_simple = doc.add_table(rows=len(irr_df_simple)+1, cols=5)\n",
    "            irr_table_simple.style = 'Table Grid'\n",
    "            irr_table_simple.cell(0, 0).text = 'Variable'\n",
    "            irr_table_simple.cell(0, 1).text = 'IRR'\n",
    "            irr_table_simple.cell(0, 2).text = 'Lower CI'\n",
    "            irr_table_simple.cell(0, 3).text = 'Upper CI'\n",
    "            irr_table_simple.cell(0, 4).text = 'P-value'\n",
    "            \n",
    "            for i, (var, row) in enumerate(irr_df_simple.iterrows(), 1):\n",
    "                irr_table_simple.cell(i, 0).text = str(var)\n",
    "                irr_table_simple.cell(i, 1).text = f\"{row['IRR']:.4f}\"\n",
    "                irr_table_simple.cell(i, 2).text = f\"{row['Lower CI']:.4f}\"\n",
    "                irr_table_simple.cell(i, 3).text = f\"{row['Upper CI']:.4f}\"\n",
    "                irr_table_simple.cell(i, 4).text = f\"{row['P-value']:.4f}\"\n",
    "            # Predictions and diagnostics for simpler model\n",
    "            df_clean_nb['predicted_simple'] = nb_results_simple.predict()\n",
    "            df_clean_nb['residuals_simple'] = df_clean_nb['los_capped'] - df_clean_nb['predicted_simple']\n",
    "            # Plot residuals for simpler model\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(df_clean_nb['predicted_simple'], df_clean_nb['residuals_simple'], alpha=0.5)\n",
    "            plt.axhline(y=0, color='r', linestyle='-')\n",
    "            plt.xlabel('Predicted Values (Simpler Model)')\n",
    "            plt.ylabel('Residuals (Simpler Model)')\n",
    "            plt.title('Residual Plot (Simpler Model)')\n",
    "            plt.tight_layout()\n",
    "            residuals_img_simple = BytesIO()\n",
    "            plt.savefig(residuals_img_simple, format='png')\n",
    "            residuals_img_simple.seek(0)\n",
    "            plt.close()\n",
    "            # Add residuals plot for simpler model to document\n",
    "            doc.add_paragraph('\\n')\n",
    "            doc.add_heading('Diagnostics - Simpler Model', level=2)\n",
    "            doc.add_picture(residuals_img_simple, width=Inches(6))\n",
    "            doc.add_paragraph('Figure 4: Residuals Plot (Simpler Model)')\n",
    "            # Add VIF for continuous variables and categorical variables for simpler model\n",
    "            vif_data_simple = pd.DataFrame()\n",
    "            vif_data_simple[\"Variable\"] = df_clean_nb[continuous_vars_simple + categorical_model_vars_simple].columns\n",
    "            vif_data_simple[\"VIF\"] = [variance_inflation_factor(df_clean_nb[continuous_vars_simple + categorical_model_vars_simple].values, i) \n",
    "                                    for i in range(df_clean_nb[continuous_vars_simple + categorical_model_vars_simple].shape[1])]\n",
    "            print(\"VIF for continuous and dummy variables (Simpler Model):\")\n",
    "            print(vif_data_simple)\n",
    "            # Add VIF table for simpler model to document\n",
    "            doc.add_paragraph('\\nVariance Inflation Factor (VIF) for Continuous and Categorical Variables - Simpler Model:')\n",
    "            vif_table_simple = doc.add_table(rows=len(vif_data_simple)+1, cols=2)\n",
    "            vif_table_simple.style = 'Table Grid'\n",
    "            vif_table_simple.cell(0, 0).text = 'Variable'\n",
    "            vif_table_simple.cell(0, 1).text = 'VIF'\n",
    "            for i, (var, vif) in enumerate(zip(vif_data_simple[\"Variable\"], vif_data_simple[\"VIF\"]), 1):\n",
    "                vif_table_simple.cell(i, 0).text = str(var)\n",
    "                vif_table_simple.cell(i, 1).text = f\"{vif:.4f}\"\n",
    "            doc.add_paragraph('VIF values above 10 indicate potential multicollinearity issues in the simpler model.')\n",
    "        #end of third model\n",
    "    \n",
    "        except Exception as e:\n",
    "            error_msg_simple = f\"Error fitting negative binomial model (Simpler Model): {str(e)}\"\n",
    "            print(error_msg_simple)\n",
    "            doc.add_paragraph(error_msg_simple)\n",
    "            doc.add_paragraph(\"The negative binomial regression model with fewer variables failed to converge. \" +\n",
    "                            \"This can happen due to insufficient variation in the data or other model specification issues.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg_simple = f\"Error fitting negative binomial model (Simpler Model): {str(e)}\"\n",
    "        print(error_msg_simple)\n",
    "        doc.add_paragraph(error_msg_simple)\n",
    "        doc.add_paragraph(\"The negative binomial regression model with fewer variables failed to converge. \" +\n",
    "                         \"This can happen due to insufficient variation in the data or other model specification issues.\")\n",
    "except Exception as e:\n",
    "    error_msg_simple = f\"Error fitting negative binomial model (Simpler Model): {str(e)}\"\n",
    "    print(error_msg_simple)\n",
    "    doc.add_paragraph(error_msg_simple)\n",
    "    doc.add_paragraph(\"The negative binomial regression model with fewer variables failed to converge. \" +\n",
    "                        \"This can happen due to insufficient variation in the data or other model specification issues.\")       \n",
    "\n",
    "\n",
    "\n",
    "# End of negative binomial models\n",
    "\n",
    "#Draw a scatter plot between age_log and distance_miles_log\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df_clean_nb['age'], y=df_clean_nb['distance_miles_log'])\n",
    "plt.title('Scatter Plot of  Age vs Log Distance Miles')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Log Distance Miles')\n",
    "plt.savefig('scatter_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "#scatter_img = BytesIO()\n",
    "#plt.savefig(scatter_img, format='png')\n",
    "#scatter_img.seek(0)\n",
    "plt.close()\n",
    "# Add scatter plot to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Scatter Plot of Age vs Log Distance Miles', level=2)\n",
    "doc.add_picture('scatter_plot.png', width=Inches(6))\n",
    "doc.add_paragraph('Figure 5: Scatter Plot of Age vs Log Distance Miles')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component re runs the third model with clustered standard errors around state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Binomial Model with Clustered Robust Standard Errors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def fit_nb_with_clustered_se(df_clean_nb, formula_simple_nostate):\n",
    "    \"\"\"\n",
    "    Fit negative binomial model with robust standard errors clustered by US state\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fit the model (same as before)\n",
    "    nb_model_simple = smf.glm(formula=formula_simple_nostate, \n",
    "                              data=df_clean_nb, \n",
    "                              family=sm.families.NegativeBinomial(link=sm.families.links.log()))\n",
    "    \n",
    "    # Fit with clustered robust standard errors\n",
    "    nb_results_clustered = nb_model_simple.fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': df_clean_nb['us_state_enc']}  # Cluster by state\n",
    "    )\n",
    "    \n",
    "    return nb_results_clustered\n",
    "\n",
    "def compare_standard_errors(df_clean_nb, formula_simple_nostate):\n",
    "    \"\"\"\n",
    "    Compare regular vs clustered standard errors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model setup\n",
    "    nb_model_simple = smf.glm(formula=formula_simple_nostate, \n",
    "                              data=df_clean_nb, \n",
    "                              family=sm.families.NegativeBinomial(link=sm.families.links.log()))\n",
    "    \n",
    "    # Regular standard errors\n",
    "    nb_results_regular = nb_model_simple.fit()\n",
    "    \n",
    "    # Clustered standard errors  \n",
    "    nb_results_clustered = nb_model_simple.fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': df_clean_nb['us_state_enc']}\n",
    "    )\n",
    "    \n",
    "    # Other robust SE options you might consider:\n",
    "    # Heteroskedasticity-robust (White's)\n",
    "    nb_results_robust = nb_model_simple.fit(cov_type='HC1')\n",
    "    \n",
    "    # Newey-West (for time series, if applicable)\n",
    "    # nb_results_nw = nb_model_simple.fit(cov_type='HAC', cov_kwds={'maxlags': 1})\n",
    "    \n",
    "    # Compare results\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Variable': nb_results_regular.params.index,\n",
    "        'Coefficient': nb_results_regular.params.values,\n",
    "        'SE_Regular': nb_results_regular.bse.values,\n",
    "        'SE_Clustered': nb_results_clustered.bse.values,\n",
    "        'SE_Robust': nb_results_robust.bse.values,\n",
    "        'P_Regular': nb_results_regular.pvalues.values,\n",
    "        'P_Clustered': nb_results_clustered.pvalues.values,\n",
    "        'P_Robust': nb_results_robust.pvalues.values\n",
    "    })\n",
    "    \n",
    "    # Calculate ratio of clustered to regular SE\n",
    "    comparison_df['SE_Ratio_Clustered'] = comparison_df['SE_Clustered'] / comparison_df['SE_Regular']\n",
    "    comparison_df['SE_Ratio_Robust'] = comparison_df['SE_Robust'] / comparison_df['SE_Regular']\n",
    "    \n",
    "    return nb_results_clustered, comparison_df\n",
    "\n",
    "def print_clustered_results(results, comparison_df=None):\n",
    "    \"\"\"\n",
    "    Print results with clustered standard errors\n",
    "    \"\"\"\n",
    "    print(\"Negative Binomial Results with Clustered Standard Errors (by US State)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results.summary())\n",
    "    \n",
    "    if comparison_df is not None:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"COMPARISON OF STANDARD ERRORS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"SE_Ratio > 1 indicates clustering increased standard errors\")\n",
    "        print(\"SE_Ratio < 1 indicates clustering decreased standard errors\")\n",
    "        print()\n",
    "        \n",
    "        # Format comparison table nicely\n",
    "        display_df = comparison_df.copy()\n",
    "        for col in ['Coefficient', 'SE_Regular', 'SE_Clustered', 'SE_Robust']:\n",
    "            display_df[col] = display_df[col].round(4)\n",
    "        for col in ['P_Regular', 'P_Clustered', 'P_Robust']:\n",
    "            display_df[col] = display_df[col].round(4)\n",
    "        for col in ['SE_Ratio_Clustered', 'SE_Ratio_Robust']:\n",
    "            display_df[col] = display_df[col].round(3)\n",
    "            \n",
    "        print(display_df[['Variable', 'Coefficient', 'SE_Regular', 'SE_Clustered', \n",
    "                         'SE_Ratio_Clustered', 'P_Regular', 'P_Clustered']].to_string(index=False))\n",
    "\n",
    "def calculate_clustered_irr(results):\n",
    "    \"\"\"\n",
    "    Calculate IRR with clustered confidence intervals\n",
    "    \"\"\"\n",
    "    # IRR (Incident Rate Ratios)\n",
    "    irr = np.exp(results.params)\n",
    "    \n",
    "    # Confidence intervals with clustered SEs\n",
    "    irr_conf = np.exp(results.conf_int())\n",
    "    \n",
    "    irr_df = pd.DataFrame({\n",
    "        'IRR': irr,\n",
    "        'Lower_CI': irr_conf.iloc[:, 0],\n",
    "        'Upper_CI': irr_conf.iloc[:, 1],\n",
    "        'P_value': results.pvalues,\n",
    "        'Significant': results.pvalues < 0.05\n",
    "    })\n",
    "    \n",
    "    return irr_df\n",
    "\n",
    "\n",
    "\n",
    "# Additional clustering options if needed:\n",
    "def alternative_clustering_methods(df_clean_nb, formula_simple_nostate):\n",
    "    \"\"\"\n",
    "    Show alternative clustering approaches\n",
    "    \"\"\"\n",
    "    nb_model = smf.glm(formula=formula_simple_nostate, \n",
    "                       data=df_clean_nb, \n",
    "                       family=sm.families.NegativeBinomial(link=sm.families.links.log()))\n",
    "    \n",
    "    # Method 1: Cluster by state (recommended for your case)\n",
    "    results_state = nb_model.fit(\n",
    "        cov_type='cluster',\n",
    "        cov_kwds={'groups': df_clean_nb['us_state_enc']}\n",
    "    )\n",
    "    \n",
    "    # Method 2: Two-way clustering (if you had both state and time)\n",
    "    # results_twoway = nb_model.fit(\n",
    "    #     cov_type='cluster',\n",
    "    #     cov_kwds={'groups': [df_clean_nb['us_state_enc'], df_clean_nb['year']]}\n",
    "    # )\n",
    "    \n",
    "    # Method 3: Bootstrap standard errors (alternative approach)\n",
    "    # This is more computationally intensive but doesn't require specific clustering assumptions\n",
    "    \n",
    "    return results_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model with clustered SEs\n",
    "nb_results_clustered, comparison_df = compare_standard_errors(df_clean_nb, formula_simple_nostate)\n",
    "\n",
    "\n",
    "print_clustered_results(nb_results_clustered, comparison_df)\n",
    "\n",
    "# Calculate IRR with clustered confidence intervals\n",
    "irr_clustered = calculate_clustered_irr(nb_results_clustered)\n",
    "print(\"\\\\nIncident Rate Ratios with Clustered Standard Errors:\")\n",
    "print(irr_clustered.round(4))\n",
    "\n",
    "# Add clustered results to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Negative Binomial Regression with Clustered Standard Errors', level=1)\n",
    "summary_paragraph_clustered = doc.add_paragraph()\n",
    "summary_run_clustered = summary_paragraph_clustered.add_run(str(nb_results_clustered.summary()))\n",
    "summary_run_clustered.font.name = 'Courier New'  # Use monospace font\n",
    "# Add IRR table with clustered standard errors\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Incident Rate Ratios (IRR) with Clustered Standard Errors', level=2)\n",
    "irr_table_clustered = doc.add_table(rows=len(irr_clustered)+1, cols=6)\n",
    "irr_table_clustered.style = 'Table Grid'\n",
    "irr_table_clustered.cell(0, 0).text = 'Variable'\n",
    "irr_table_clustered.cell(0, 1).text = 'IRR'\n",
    "irr_table_clustered.cell(0, 2).text = 'Lower CI'\n",
    "irr_table_clustered.cell(0, 3).text = 'Upper CI'\n",
    "irr_table_clustered.cell(0, 4).text = 'P-value'\n",
    "irr_table_clustered.cell(0, 5).text = 'Significant'\n",
    "for i, (var, row) in enumerate(irr_clustered.iterrows(), 1):\n",
    "    irr_table_clustered.cell(i, 0).text = str(var)\n",
    "    irr_table_clustered.cell(i, 1).text = f\"{row['IRR']:.4f}\"\n",
    "    irr_table_clustered.cell(i, 2).text = f\"{row['Lower_CI']:.4f}\"\n",
    "    irr_table_clustered.cell(i, 3).text = f\"{row['Upper_CI']:.4f}\"\n",
    "    irr_table_clustered.cell(i, 4).text = f\"{row['P_value']:.4f}\"\n",
    "    irr_table_clustered.cell(i, 5).text = 'Yes' if row['Significant'] else 'No'\n",
    "# Add comparison of standard errors table\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Comparison of Standard Errors', level=2)\n",
    "comparison_table = doc.add_table(rows=len(comparison_df)+1, cols=7)\n",
    "comparison_table.style = 'Table Grid'\n",
    "comparison_table.cell(0, 0).text = 'Variable'\n",
    "comparison_table.cell(0, 1).text = 'Coefficient'\n",
    "comparison_table.cell(0, 2).text = 'SE Regular'\n",
    "comparison_table.cell(0, 3).text = 'SE Clustered'\n",
    "comparison_table.cell(0, 4).text = 'SE Ratio Clustered'\n",
    "comparison_table.cell(0, 5).text = 'P Regular'\n",
    "comparison_table.cell(0, 6).text = 'P Clustered'   \n",
    "for i, row in comparison_df.iterrows():\n",
    "    comparison_table.cell(i+1, 0).text = str(row['Variable'])\n",
    "    comparison_table.cell(i+1, 1).text = f\"{row['Coefficient']:.4f}\"\n",
    "    comparison_table.cell(i+1, 2).text = f\"{row['SE_Regular']:.4f}\"\n",
    "    comparison_table.cell(i+1, 3).text = f\"{row['SE_Clustered']:.4f}\"\n",
    "    comparison_table.cell(i+1, 4).text = f\"{row['SE_Ratio_Clustered']:.3f}\"\n",
    "    comparison_table.cell(i+1, 5).text = f\"{row['P_Regular']:.4f}\"\n",
    "    comparison_table.cell(i+1, 6).text = f\"{row['P_Clustered']:.4f}\"\n",
    "# Tips for interpretation:\n",
    "\"\"\"\n",
    "INTERPRETATION GUIDE:\n",
    "\n",
    "1. Clustered SEs account for correlation within states\n",
    "   - Observations from same state may be more similar\n",
    "   - This typically INCREASES standard errors\n",
    "   - More conservative inference (larger p-values)\n",
    "\n",
    "2. When to use clustered SEs:\n",
    "   - You have multiple observations per cluster (state)\n",
    "   - You suspect within-cluster correlation\n",
    "   - State-level policies might affect all observations from that state\n",
    "\n",
    "3. Ratio interpretation:\n",
    "   - SE_Ratio > 1.5: Strong evidence of within-cluster correlation\n",
    "   - SE_Ratio > 2.0: Very strong clustering effects\n",
    "   - SE_Ratio < 1.1: Little evidence of clustering\n",
    "\n",
    "4. Model selection:\n",
    "   - If clustered SEs are much larger, use them for inference\n",
    "   - If they're similar to regular SEs, either is fine\n",
    "   - Always report which type you used\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a negative relationship between imports from St Lucia and immigrant density and length of stay. In the charts/tables below we attempt to dig a bit deeper into this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a table showing top 10 states who import the most from SLU and their LOS capped and number of observations\n",
    "top_states = df_clean_nb.groupby('us_state_enc').agg({\n",
    "    'import_from_slu_log': 'sum',\n",
    "    'los_capped': 'mean',\n",
    "    'us_state_enc': 'count',\n",
    "    'immigrant_density': 'mean'\n",
    "}).rename(columns={'us_state_enc': 'num_observations'}).reset_index()\n",
    "top_states = top_states.sort_values(by='import_from_slu_log', ascending=False).head(10)\n",
    "#print the top states table\n",
    "print(\"\\nTop 10 States Importing from SLU:\")\n",
    "print(top_states)\n",
    "\n",
    "# Add top states table to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Top 10 States Importing from SLU', level=2)\n",
    "top_states_table = doc.add_table(rows=len(top_states)+1, cols=5)\n",
    "top_states_table.style = 'Table Grid'\n",
    "top_states_table.cell(0, 0).text = 'State'\n",
    "top_states_table.cell(0, 1).text = 'Total Import from SLU (Log)'\n",
    "top_states_table.cell(0, 2).text = 'Mean Length of Stay (Capped)'\n",
    "top_states_table.cell(0, 3).text = 'Number of Observations'\n",
    "top_states_table.cell(0, 4).text = 'Immigrant Density (Mean)'\n",
    "for i, row in enumerate(top_states.itertuples(), 1):\n",
    "    top_states_table.cell(i, 0).text = str(row.us_state_enc)\n",
    "    top_states_table.cell(i, 1).text = f\"{row.import_from_slu_log:.4f}\"\n",
    "    top_states_table.cell(i, 2).text = f\"{row.los_capped:.4f}\"\n",
    "    top_states_table.cell(i, 3).text = str(row.num_observations)\n",
    "    top_states_table.cell(i, 4).text = f\"{row.immigrant_density:.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_states = df_clean_nb.groupby('us_state_enc').agg({\n",
    "    'import_from_slu_log': 'sum',\n",
    "    'los_capped': 'mean',\n",
    "    'us_state_enc': 'count',\n",
    "    'immigrant_density': 'mean'\n",
    "}).rename(columns={'us_state_enc': 'num_observations'}).reset_index()\n",
    "bottom_states = bottom_states.sort_values(by='import_from_slu_log', ascending=True).head(10)\n",
    "#print the bottom states table\n",
    "print(\"\\nBottom 10 States Importing from SLU:\")\n",
    "print(bottom_states)\n",
    "\n",
    "# Add bottom states table to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Bottom 10 States Importing from SLU', level=2)\n",
    "bottom_states_table = doc.add_table(rows=len(bottom_states)+1, cols=5)\n",
    "bottom_states_table.style = 'Table Grid'\n",
    "bottom_states_table.cell(0, 0).text = 'State'\n",
    "bottom_states_table.cell(0, 1).text = 'Total Import from SLU (Log)'\n",
    "bottom_states_table.cell(0, 2).text = 'Mean Length of Stay (Capped)'\n",
    "bottom_states_table.cell(0, 3).text = 'Number of Observations'\n",
    "bottom_states_table.cell(0, 4).text = 'Immigrant Density (Mean)'\n",
    "for i, row in enumerate(bottom_states.itertuples(), 1):\n",
    "    bottom_states_table.cell(i, 0).text = str(row.us_state_enc)\n",
    "    bottom_states_table.cell(i, 1).text = f\"{row.import_from_slu_log:.4f}\"\n",
    "    bottom_states_table.cell(i, 2).text = f\"{row.los_capped:.4f}\"\n",
    "    bottom_states_table.cell(i, 3).text = str(row.num_observations)\n",
    "    bottom_states_table.cell(i, 4).text = f\"{row.immigrant_density:.4f}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a table showing top 10 states with the highest average density and their LOS capped and number of observations\n",
    "top_density_states = df_clean_nb.groupby('us_state_enc').agg({\n",
    "    'immigrant_density': 'mean',\n",
    "    'los_capped': 'mean',\n",
    "    'us_state_enc': 'count',\n",
    "    'import_from_slu_log': 'sum'\n",
    "}).rename(columns={'us_state_enc': 'num_observations'}).reset_index()\n",
    "top_density_states = top_density_states.sort_values(by='immigrant_density', ascending=False).head(10)\n",
    "#print the top density states table\n",
    "print(\"\\nTop 10 States with Highest Average Density:\")\n",
    "print(top_density_states)\n",
    "# Add top density states table to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Top 10 States with Highest Average Density', level=2)\n",
    "top_density_states_table = doc.add_table(rows=len(top_density_states)+1, cols=5)\n",
    "top_density_states_table.style = 'Table Grid'\n",
    "top_density_states_table.cell(0, 0).text = 'State'\n",
    "top_density_states_table.cell(0, 1).text = 'Immigrant Density (Mean)'\n",
    "top_density_states_table.cell(0, 2).text = 'Mean Length of Stay (Capped)'\n",
    "top_density_states_table.cell(0, 3).text = 'Number of Observations'\n",
    "top_density_states_table.cell(0, 4).text = 'Total Import from SLU (Log)'\n",
    "for i, row in enumerate(top_density_states.itertuples(), 1):\n",
    "    top_density_states_table.cell(i, 0).text = str(row.us_state_enc)\n",
    "    top_density_states_table.cell(i, 1).text = f\"{row.immigrant_density:.4f}\"\n",
    "    top_density_states_table.cell(i, 2).text = f\"{row.los_capped:.4f}\"\n",
    "    top_density_states_table.cell(i, 3).text = str(row.num_observations)\n",
    "    top_density_states_table.cell(i, 4).text = f\"{row.import_from_slu_log:.4f}\"      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_density_states = df_clean_nb.groupby('us_state_enc').agg({\n",
    "    'immigrant_density': 'mean',\n",
    "    'los_capped': 'mean',\n",
    "    'us_state_enc': 'count',\n",
    "    'import_from_slu_log': 'sum'\n",
    "}).rename(columns={'us_state_enc': 'num_observations'}).reset_index()\n",
    "bottom_density_states = bottom_density_states.sort_values(by='immigrant_density', ascending=True).head(10)\n",
    "#print the bottom density states table\n",
    "print(\"\\nBottom 10 States with Highest Average Density:\")\n",
    "print(bottom_density_states)\n",
    "# Add bottom density states table to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Bottom 10 States with Highest Average Density', level=2)\n",
    "bottom_density_states_table = doc.add_table(rows=len(bottom_density_states)+1, cols=5)\n",
    "bottom_density_states_table.style = 'Table Grid'\n",
    "bottom_density_states_table.cell(0, 0).text = 'State'\n",
    "bottom_density_states_table.cell(0, 1).text = 'Immigrant Density (Mean)'\n",
    "bottom_density_states_table.cell(0, 2).text = 'Mean Length of Stay (Capped)'\n",
    "bottom_density_states_table.cell(0, 3).text = 'Number of Observations'\n",
    "bottom_density_states_table.cell(0, 4).text = 'Total Import from SLU (Log)'\n",
    "for i, row in enumerate(bottom_density_states.itertuples(), 1):\n",
    "    bottom_density_states_table.cell(i, 0).text = str(row.us_state_enc)\n",
    "    bottom_density_states_table.cell(i, 1).text = f\"{row.immigrant_density:.4f}\"\n",
    "    bottom_density_states_table.cell(i, 2).text = f\"{row.los_capped:.4f}\"\n",
    "    bottom_density_states_table.cell(i, 3).text = str(row.num_observations)\n",
    "    bottom_density_states_table.cell(i, 4).text = f\"{row.import_from_slu_log:.4f}\"    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a table showing top 10 states by number of arrivals (observations), with their average LOS, immigrant density, and total imports from SLU\n",
    "top_arrival_states = df_clean_nb.groupby('us_state_enc').agg({\n",
    "    'los_capped': 'mean',\n",
    "    'immigrant_density': 'mean',\n",
    "    'import_from_slu_log': 'sum',\n",
    "    'us_state_enc': 'count'\n",
    "}).rename(columns={'us_state_enc': 'num_arrivals'}).reset_index()\n",
    "top_arrival_states = top_arrival_states.sort_values(by='num_arrivals', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 States by Number of Arrivals:\")\n",
    "print(top_arrival_states)\n",
    "\n",
    "# Add top arrival states table to document\n",
    "doc.add_paragraph('\\n')\n",
    "doc.add_heading('Top 10 States by Number of Arrivals', level=2)\n",
    "top_arrival_states_table = doc.add_table(rows=len(top_arrival_states)+1, cols=5)\n",
    "top_arrival_states_table.style = 'Table Grid'\n",
    "top_arrival_states_table.cell(0, 0).text = 'State'\n",
    "top_arrival_states_table.cell(0, 1).text = 'Number of Arrivals'\n",
    "top_arrival_states_table.cell(0, 2).text = 'Mean Length of Stay (Capped)'\n",
    "top_arrival_states_table.cell(0, 3).text = 'Immigrant Density (Mean)'\n",
    "top_arrival_states_table.cell(0, 4).text = 'Total Import from SLU (Log)'\n",
    "for i, row in enumerate(top_arrival_states.itertuples(), 1):\n",
    "    top_arrival_states_table.cell(i, 0).text = str(row.us_state_enc)\n",
    "    top_arrival_states_table.cell(i, 1).text = str(row.num_arrivals)\n",
    "    top_arrival_states_table.cell(i, 2).text = f\"{row.los_capped:.4f}\"\n",
    "    top_arrival_states_table.cell(i, 3).text = f\"{row.immigrant_density:.4f}\"\n",
    "    top_arrival_states_table.cell(i, 4).text = f\"{row.import_from_slu_log:.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the average length of stay (LOS) capped between the top 10 states and bottom 10 states\n",
    "top_avg_los = top_states['los_capped'].mean()\n",
    "bottom_avg_los = bottom_states['los_capped'].mean()\n",
    "print(f\"\\nAverage Length of Stay (Capped) - Top 10 States: {top_avg_los:.4f}\")\n",
    "print(f\"Average Length of Stay (Capped) - Bottom 10 States: {bottom_avg_los:.4f}\")\n",
    "\n",
    "#compare the average length of stay (LOS) capped between the top 10 states by density and bottom 10 states by density\n",
    "top_density_avg_los = top_density_states['los_capped'].mean()\n",
    "bottom_density_avg_los = bottom_density_states['los_capped'].mean()\n",
    "print(f\"\\nAverage Length of Stay (Capped) - Top 10 States by Density: {top_density_avg_los:.4f}\")\n",
    "print(f\"Average Length of Stay (Capped) - Bottom 10 States by Density: {bottom_density_avg_los:.4f}\")\n",
    "print(f\"\\nAverage Length of Stay (Capped) - Top 10 States by Arrivals: {top_arrival_states['los_capped'].mean():.4f}\")\n",
    "print(f\"Average Length of Stay (Capped) - Bottom 10 States by Arrivals: {bottom_states['los_capped'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prints all results into one word document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Word document\n",
    "print(\"\\nPlease select where to save the Word document...\")\n",
    "doc_path = select_file(\n",
    "    \"Save Analysis Report As\", \n",
    "    [(\"Word Document\", \"*.docx\"), (\"All files\", \"*.*\")],\n",
    "    save=True\n",
    ")\n",
    "\n",
    "if doc_path:\n",
    "    if not doc_path.endswith('.docx'):\n",
    "        doc_path += '.docx'\n",
    "    doc.save(doc_path)\n",
    "    print(f\"Analysis report saved to: {doc_path}\")\n",
    "else:\n",
    "    print(\"Document not saved as no location was selected.\")\n",
    "\n",
    "print(\"\\nAnalysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jlslu2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
